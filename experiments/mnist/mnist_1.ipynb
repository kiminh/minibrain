{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Experiments - Playing With Different Architectures\n",
    "\n",
    "Sometimes we need to come back to the basis, this is the place I choose for that.\n",
    "\n",
    "Here I'll experiment with different networks on the MNIST and MNIST variants datasets trying to find relations in which I can reduce the number of parameters in comparison with a Fully Connected (FC) network.\n",
    "\n",
    "Later on, I might try with other datasets that are small enough for my GTX1080.\n",
    "\n",
    "Yes, I know, the issue is already solved for Images with Convolutional Networks but what I want to see is not that. Instead I want to understand ways in which fully connected networks can be replaced by other types of connections to minimize the number of parameters in it. This is an exploratory work to get a deeper understanding on Neural Networks (NNs) that will at least give me some fun time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "#### Datasets to use:\n",
    "* MNIST\n",
    "* ... ??? (later might update this)\n",
    "\n",
    "#### Experiments I intend to run:\n",
    "- Fully Connected Network\n",
    "- Convolutional with late FC layer for clasification\n",
    "- Randomly Connected Network (Sparse)\n",
    "- Parallel Smaller Fully Connected Networks with a late merge for clasification (several \"columns\" instead of one big fully connected)\n",
    "    * Only continuous subsets of elements\n",
    "    * Playing with reordering the elements in the input\n",
    "Structurally Connected Networks (Different Structures will be used, is a follow up of the parallel)\n",
    "\n",
    "\n",
    "#### KPIs I intend to measure:\n",
    "- Number of Parameters\n",
    "- Performance ( Accuracy, Errors, ... )\n",
    "- Training Time\n",
    "- Evaluation Time\n",
    "\n",
    "Although training and evaluation time \n",
    "\n",
    "#### Things I would like to analyse later:\n",
    "* Kind of Errors\n",
    "* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network modules  to try\n",
    "from network_modules import *\n",
    "from net_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_mnist():\n",
    "    return transforms.Compose([\n",
    "#         transforms.Grayscale(),\n",
    "#         transforms.Resize((w, h)),  # this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor()\n",
    "#         transforms.Normalize(0.5, 0.25)\n",
    "    ])\n",
    "# Datasets:\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# something like this will be used to create the sparsity masks ... only that the sparsity distributions should be chosen before\n",
    "torch.bernoulli(torch.rand(10, 14)).expand(10, 14).clone() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0, lbl0 = mnist_trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl0.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f799e662978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f799e672668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img0.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, mname, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    num_epochs = 100\n",
    "    batch_size = 128\n",
    "#     learning_rate = 0.0001\n",
    "    learning_rate = 0.001\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = F.nll_loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    transformation = transform_mnist()\n",
    "    train_loader, test_loader = get_loaders(batch_size, transformation)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (img, labels) in enumerate(train_loader):\n",
    "#             print(\"shape: \", img.shape, labels.shape)\n",
    "            labels = labels.to(device)\n",
    "            img = img.to(device).view((-1,784))\n",
    "            \n",
    "#             print(\"shape2: \", img.shape)\n",
    "            # ===================forward=====================\n",
    "            #         print(\"encoding batch of  images\")\n",
    "            output = model(img)\n",
    "#             print(\"output shape: \", output.shape, labels.shape, labels[:10])\n",
    "            #         print(\"computing loss\")\n",
    "            loss = criterion(output, labels)\n",
    "            # ===================backward====================\n",
    "            #         print(\"Backward \")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        if epoch % 20 == 0:\n",
    "            print('epoch [{}/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "#         if epoch % 10 == 0:\n",
    "#             pic = to_img(output.cpu().data)\n",
    "#             in_pic = to_img(img.cpu().data)\n",
    "#             save_image(pic, './results/2x2-out_image_{}.png'.format(epoch))\n",
    "#             save_image(in_pic, './results/2x2-in_image_{}.png'.format(epoch))\n",
    "#         if loss.data[0] < 0.015: #arbitrary number because I saw that it works well enough\n",
    "#             print(\"loss < 0.015, breaking\")\n",
    "#             break\n",
    "#     model.save_model(mname, \"model\")\n",
    "\n",
    "    print('########################################################')\n",
    "    print('Final performance of model {} epoch [{}/{}], loss:{:.8f}'.format(mname, epoch+1, num_epochs, loss.data[0]))\n",
    "    print('--------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnets_layers = [\n",
    "        [784,500,10],\n",
    "        [784,1000,10],\n",
    "        [784,1500,10],\n",
    "        [784,500,500,10],\n",
    "        [784,1000,500,10],\n",
    "        [784,1000,1000,10],\n",
    "        [784,500,500,500,10],\n",
    "        [784,1000,500,500,10],\n",
    "        [784,1000,1000,500,10],\n",
    "        [784,1000,1000,1000,10],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FCNet(fcnets_layers[0], \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ FCNet(l, \"relu\") for l in fcnets_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCNet(\n",
       "  (fcnet): FCModule(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=784, out_features=500, bias=True)\n",
       "      (1): Linear(in_features=500, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [21/100], loss:0.0029\n",
      "epoch [41/100], loss:0.0005\n",
      "epoch [61/100], loss:0.0011\n",
      "epoch [81/100], loss:0.0116\n",
      "########################################################\n",
      "Final performance of model [784, 500, 10] epoch [100/100], loss:0.000900\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/ipykernel_launcher.py:52: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.2221\n",
      "epoch [21/100], loss:0.0040\n",
      "epoch [41/100], loss:0.0002\n",
      "epoch [61/100], loss:0.0007\n",
      "epoch [81/100], loss:0.0014\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 10] epoch [100/100], loss:0.001907\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.0883\n",
      "epoch [21/100], loss:0.0076\n",
      "epoch [41/100], loss:0.0019\n",
      "epoch [61/100], loss:0.0006\n",
      "epoch [81/100], loss:0.0001\n",
      "########################################################\n",
      "Final performance of model [784, 1500, 10] epoch [100/100], loss:0.000699\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.1183\n",
      "epoch [21/100], loss:0.0008\n",
      "epoch [41/100], loss:0.0000\n",
      "epoch [61/100], loss:0.0001\n",
      "epoch [81/100], loss:0.0001\n",
      "########################################################\n",
      "Final performance of model [784, 500, 500, 10] epoch [100/100], loss:0.000163\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.2173\n",
      "epoch [21/100], loss:0.0003\n",
      "epoch [41/100], loss:0.0095\n",
      "epoch [61/100], loss:0.0004\n",
      "epoch [81/100], loss:0.0001\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 500, 10] epoch [100/100], loss:0.000294\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.0644\n",
      "epoch [21/100], loss:0.0196\n",
      "epoch [41/100], loss:0.0074\n",
      "epoch [61/100], loss:0.0018\n",
      "epoch [81/100], loss:0.0001\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 1000, 10] epoch [100/100], loss:0.001587\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.1667\n",
      "epoch [21/100], loss:0.0001\n",
      "epoch [41/100], loss:0.0001\n",
      "epoch [61/100], loss:0.0116\n",
      "epoch [81/100], loss:0.0028\n",
      "########################################################\n",
      "Final performance of model [784, 500, 500, 500, 10] epoch [100/100], loss:0.000104\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.0676\n",
      "epoch [21/100], loss:0.0005\n",
      "epoch [41/100], loss:0.0031\n",
      "epoch [61/100], loss:0.0017\n",
      "epoch [81/100], loss:0.0056\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 500, 500, 10] epoch [100/100], loss:0.000010\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.1060\n",
      "epoch [21/100], loss:0.0023\n",
      "epoch [41/100], loss:0.0017\n",
      "epoch [61/100], loss:0.0030\n",
      "epoch [81/100], loss:0.0024\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 1000, 500, 10] epoch [100/100], loss:0.000013\n",
      "--------------------------------------------------------\n",
      "epoch [1/100], loss:0.1497\n",
      "epoch [21/100], loss:0.0028\n",
      "epoch [41/100], loss:0.0014\n",
      "epoch [61/100], loss:0.0017\n",
      "epoch [81/100], loss:0.0002\n",
      "########################################################\n",
      "Final performance of model [784, 1000, 1000, 1000, 10] epoch [100/100], loss:0.000493\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(fcnets_layers)):\n",
    "    mname = str(fcnets_layers[i])\n",
    "    model = models[i]\n",
    "    train(model, mname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
