{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Experiments - Playing With Different Architectures\n",
    "\n",
    "Sometimes we need to come back to the basis, this is the place I choose for that.\n",
    "\n",
    "Here I'll experiment with different networks on the MNIST and MNIST variants datasets trying to find relations in which I can reduce the number of parameters in comparison with a Fully Connected (FC) network.\n",
    "\n",
    "Later on, I might try with other datasets that are small enough for my GTX1080.\n",
    "\n",
    "Yes, I know, the issue is already solved for Images with Convolutional Networks but what I want to see is not that. Instead I want to understand ways in which fully connected networks can be replaced by other types of connections to minimize the number of parameters in it. This is an exploratory work to get a deeper understanding on Neural Networks (NNs) that will at least give me some fun time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "#### Datasets to use:\n",
    "* MNIST\n",
    "* ... ??? (later might update this)\n",
    "\n",
    "#### Experiments I intend to run:\n",
    "- Fully Connected Network\n",
    "- Convolutional with late FC layer for clasification\n",
    "- Randomly Connected Network (Sparse)\n",
    "- Parallel Smaller Fully Connected Networks with a late merge for clasification (several \"columns\" instead of one big fully connected)\n",
    "    * Only continuous subsets of elements\n",
    "    * Playing with reordering the elements in the input\n",
    "Structurally Connected Networks (Different Structures will be used, is a follow up of the parallel)\n",
    "\n",
    "\n",
    "#### KPIs I intend to measure:\n",
    "- Number of Parameters\n",
    "- Performance ( Accuracy, Errors, ... )\n",
    "- Training Time\n",
    "- Evaluation Time\n",
    "\n",
    "Although training and evaluation time \n",
    "\n",
    "#### Things I would like to analyse later:\n",
    "* Kind of Errors\n",
    "* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Datasets:\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers_sizes, activation=True):\n",
    "        \"\"\"\n",
    "        param: layers_sizes -> list containing the input, hidden layers and output size, minimum len = 2\n",
    "                                First element is Input size\n",
    "                                Last emenet is output size\n",
    "        param: activation -> Default True, if False no activation layer is used, else uses the ReLu one (TODO use other activation layers too)\n",
    "        \"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        assert(type(layer_sizes) is tuple or type(layer_sizes) is list)\n",
    "        n_layers = len(layers_sizes) - 1  # there is one layer less than the number of elements in the input\n",
    "        assert(n_layers >= 1)\n",
    "        self.layers = []\n",
    "        self.activation = activation\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append( nn.Linear(layers_sizes[i], layers_sizes[i+1]))\n",
    "        self.n_layers = len(self.layers)\n",
    "        assert(self.n_layers == n_layers)  # should always be the same or something went really wrong\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "            if self.activation and i<self.n_layers-2:\n",
    "                x = F.relu(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
