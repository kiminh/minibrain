{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Tuning Neurons\n",
    "\n",
    "This notebook is dedicated to study the effectof the tunning neurons in the MNIST task, the comparison should be done with other kind of ANNs already know in the industry.\n",
    "\n",
    "**The main points to compare are**:\n",
    " - Performance (errors, accuracy, other metrics)\n",
    " - Learning Speed (in cycles)\n",
    " - Compare results of BOTH types of networks at initialization moment with a final Linear decoder\n",
    " - Operation complexity (number of OPS for example for each to get to a meaningful representation)\n",
    " - Compare a  random set of Tuning Neuron Ensembles (TNEs) with a final linear decoder\n",
    " - Create several ideas on attention mechanisms for the TNEs to generate different Deep TNEs Networks (DTNENs) and compare results\n",
    "\n",
    "*NOTE*: The implementation of TNEs in TensorFlow might be needed for this task, as it might take a lot of time in the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue with TensorFlow, reference here: https://github.com/tensorflow/tensorflow/issues/6698\n",
    "#config needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and neuron creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tuning_neuron_2d(shape, min_y=0.5,\n",
    "#                                max_y=1.5, min_weight=0.0, max_weight=0.8,\n",
    "#                                min_x=-1.0, max_x=1.0, saturation=None,\n",
    "#                               ):\n",
    "#     height, width, input_channels, output_channels = shape #NHWC\n",
    "#     n_neurons = n_synapses = width * height\n",
    "#     # the first point is for y=0\n",
    "#     nshape = [height, width] #, output_channels]\n",
    "#     y1 = tf.zeros((n_neurons, ))\n",
    "#     x1 = tf.random_uniform((n_neurons, ), -1, 1)\n",
    "#     # the second point is for x = +-1\n",
    "#     y2 = tf.random_uniform((n_neurons, ),min_y, max_y)\n",
    "#     vec = np.random.choice([-1,1], (n_neurons, ))\n",
    "#     x2 = tf.convert_to_tensor(vec, tf.float32) #will define a's sign\n",
    "#     #a = tf.Variable( tf.divide(tf.subtract(y1,y2), tf.subtract(x1,x2)))\n",
    "#     a = tf.divide(tf.subtract(y1,y2), tf.subtract(x1,x2))\n",
    "#     A = a\n",
    "#     #A = tf.stack([a for i in range(output_channels)])#, axis=1)\n",
    "#     b = tf.subtract(y1, (a - x1))\n",
    "#     B = b\n",
    "#     #b = tf.Variable(tf.subtract(y1, (a - x1)))\n",
    "#     #B = tf.stack([b for i in range(output_channels)])#, axis=1)\n",
    "#     W = tf.Variable(tf.random_uniform([n_synapses,n_neurons]))\n",
    "#     if(saturation is None):\n",
    "#         saturation = [random.uniform(0.8, 1.0),1.0]\n",
    "#     sat = tf.random_uniform((n_neurons, ), *saturation)\n",
    "#     SAT = sat\n",
    "#     return (A, B, SAT, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_neuron_2d(shape, min_y=0.5,\n",
    "                               max_y=1.5, min_weight=0.0, max_weight=0.8,\n",
    "                               min_x=-1.0, max_x=1.0, saturation=None,\n",
    "                              ):\n",
    "    height, width, input_channels, output_channels = shape #NHWC\n",
    "    n_neurons = n_synapses = width * height\n",
    "    # the first point is for y=0\n",
    "    y1 = tf.zeros((n_neurons, ))\n",
    "    x1 = tf.random_uniform((n_neurons, ), -1, 1)\n",
    "    # the second point is for x = +-1\n",
    "    y2 = tf.random_uniform((n_neurons, ), min_y, max_y)\n",
    "    vec = np.random.choice([-1,1], (n_neurons, ))\n",
    "    x2 = tf.convert_to_tensor(vec, tf.float32) #will define a's sign\n",
    "    #a = tf.Variable( tf.divide(tf.subtract(y1,y2), tf.subtract(x1,x2)))\n",
    "    a = tf.divide(tf.subtract(y1,y2), tf.subtract(x1,x2))\n",
    "    A = a\n",
    "    #A = tf.stack([a for i in range(output_channels)])#, axis=1)\n",
    "    b = tf.subtract(y1, (a - x1))\n",
    "    #b = tf.Variable(tf.subtract(y1, (a - x1)))\n",
    "    #B = tf.stack([b for i in range(output_channels)])#, axis=1)\n",
    "    B = b\n",
    "    #W = tf.Variable(tf.random_uniform([n_synapses,n_neurons]))\n",
    "    W = tf.random_uniform([n_synapses,n_neurons])\n",
    "                   \n",
    "    if(saturation is None):\n",
    "        saturation = [random.uniform(0.8, 1.0),1.0]\n",
    "    sat = tf.random_uniform((n_neurons, ), *saturation)\n",
    "    SAT = sat\n",
    "    #SAT = tf.stack([sat for i in range(output_channels)])#, axis=1)\n",
    "    #outputs placeholder\n",
    "    return (A,B,SAT,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tuning_neuron(shape, A,B,SAT,W,x):\n",
    "    height, width, input_channels, output_channels = shape #NHWC\n",
    "    #regression logit model\n",
    "    # I will first play with reshaping as I don't understand TF yet too much\n",
    "    # it seems that reshaping is the ONLY available solution for tensorflow\n",
    "    #xresh = tf.transpose(tf.reshape(x, [batch_size, height*width, output_channels]))\n",
    "    #print(A.shape, B.shape, W.shape, SAT.shape, x.shape)\n",
    "    xshape = x.get_shape()\n",
    "    tf.Print(x, [x])\n",
    "    xresh = tf.transpose(tf.reshape(x, [-1, height*width, output_channels ]),perm=[0,2,1])\n",
    "    #try with another resizing and see what gives!!\n",
    "    xresh2 = tf.reshape(x, [-1, height*width])\n",
    "    xcurrent = tf.matmul(xresh2, W)\n",
    "    #yb = tf.maximum(tf.minimum(tf.multiply(xcurrent, A) + B, sat), 0 )\n",
    "    #print(A.shape, B.shape, W.shape, SAT.shape, x.shape, xcurrent.shape, xresh.shape)\n",
    "    yb = tf.maximum(tf.minimum(tf.multiply(xcurrent, A) + B, SAT), 0 )\n",
    "    y = tf.reshape(yb, [-1, height, width , output_channels])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "ty_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch [width, height, input channels, output channels]\n",
    "tW_conv1 = weight_variable([5, 5, 1, 32])\n",
    "tb_conv1 = bias_variable([32]) #number of output bias\n",
    "#4d tensor [ N# batch inputs, width, height, n#channels]\n",
    "tx_image = tf.reshape(tx, [-1, 28, 28, 1])\n",
    "\n",
    "thconv1 = conv2d(tx_image, tW_conv1) + tb_conv1\n",
    "#thconv1 = tf.Print(thconv1, [thconv1.shape])\n",
    "shape1 = [28, 28, 1, 32 ]\n",
    "A1,B1,SAT1,W1 = create_tuning_neuron_2d(shape1, saturation=[0.8,10])\n",
    "th_conv1 = evaluate_tuning_neuron(shape1, A1,B1,SAT1,W1, thconv1)\n",
    "# th_conv1 = tf.Print(th_conv1, [th_conv1.shape])\n",
    "th_pool1 = max_pool_2x2(th_conv1)\n",
    "\n",
    "# th_conv1 = tf.nn.relu(conv2d(tx_image, tW_conv1) + tb_conv1)\n",
    "# th_pool1 = max_pool_2x2(th_conv1)\n",
    "# th_pool1 = tf.Print(th_pool1, [th_pool1.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 14, 14, 32) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tW_conv2 = weight_variable([5, 5, 32, 64])\n",
    "tb_conv2 = bias_variable([64])\n",
    "\n",
    "# thconv2 = conv2d(th_pool1, tW_conv2) + tb_conv2\n",
    "# (a, b, sat, W, x, th_conv2, y_ )= create_tuning_neuron_2d([14, 14, 1, 64], thconv2,\n",
    "#                                                    saturation=[0.8,10])\n",
    "# th_pool2 = max_pool_2x2(th_conv2)\n",
    "\n",
    "# shape2 = [14, 14, 1, 64 ]\n",
    "# A2,B2,SAT2,W2 = create_tuning_neuron_2d(shape2, saturation=[0.8,10])\n",
    "# th_conv2 = evaluate_tuning_neuron(shape2, A2,B2,SAT2,W2, thconv2)\n",
    "# th_conv2 = tf.Print(th_conv1, [th_conv1.shape])\n",
    "# th_pool2 = max_pool_2x2(th_conv2)\n",
    "\n",
    "th_conv2 = tf.nn.relu(conv2d(th_pool1, tW_conv2) + tb_conv2)\n",
    "th_pool2 = max_pool_2x2(th_conv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_1:0' shape=(?, 7, 7, 64) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Densely connected layer\n",
    "tW_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "tb_fc1 = bias_variable([1024])\n",
    "\n",
    "th_pool2_flat = tf.reshape(th_pool2, [-1, 7*7*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is HERE that I have to replace by my Tuning Neurons\n",
    "\n",
    "# thconv1 = conv2d(tx_image, tW_conv1) + tb_conv1\n",
    "# (a, b, sat, W, x, th_conv1, y_ )= create_tuning_neuron_2d([28, 28, 1, 32 ], \n",
    "#                                                           thconv1, saturation=[0.8,10])\n",
    "# th_pool1 = max_pool_2x2(th_conv1)\n",
    "\n",
    "th_fc1 = tf.nn.relu(tf.matmul(th_pool2_flat, tW_fc1) + tb_fc1)\n",
    "\n",
    "#Dropout\n",
    "\n",
    "tkeep_prob = tf.placeholder(tf.float32)\n",
    "th_fc1_drop = tf.nn.dropout(th_fc1, tkeep_prob)\n",
    "\n",
    "#Readout Layer\n",
    "\n",
    "tW_fc2 = weight_variable([1024, 10])\n",
    "tb_fc2 = bias_variable([10])\n",
    "\n",
    "ty_conv = tf.matmul(th_fc1_drop, tW_fc2) + tb_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=ty_, logits=ty_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(ty_conv, 1), tf.argmax(ty_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ty_, logits=ty_conv))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for _ in range(1000):\n",
    "#   batch = mnist.train.next_batch(batch_size)\n",
    "#   train_step.run(feed_dict={tx: batch[0], ty_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training accuracy 0.04\n",
      "step 200, training accuracy 0.16\n",
      "step 300, training accuracy 0.08\n",
      "step 400, training accuracy 0.1\n",
      "step 500, training accuracy 0.1\n",
      "step 600, training accuracy 0.14\n",
      "step 700, training accuracy 0.1\n",
      "step 800, training accuracy 0.08\n",
      "step 900, training accuracy 0.12\n",
      "step 1000, training accuracy 0.1\n",
      "step 1100, training accuracy 0.14\n",
      "step 1200, training accuracy 0.12\n",
      "step 1300, training accuracy 0.08\n",
      "step 1400, training accuracy 0.08\n",
      "step 1500, training accuracy 0.2\n",
      "step 1600, training accuracy 0.12\n",
      "step 1700, training accuracy 0.14\n",
      "step 1800, training accuracy 0.12\n",
      "step 1900, training accuracy 0.12\n",
      "CPU times: user 9.98 s, sys: 1.55 s, total: 11.5 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i>0 and i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          tx: batch[0], ty_: batch[1], tkeep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "      if(train_accuracy > 0.999):\n",
    "        break\n",
    "    train_step.run(feed_dict={tx: batch[0], ty_: batch[1], tkeep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results seem completely random, as if it was creating a NEW encoding each time ???\n",
    "\n",
    "I really don't understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "x: mnist.test.images, y_: mnist.test.labels, tkeep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still with issues about the dimensions:\n",
    "\n",
    "    InvalidArgumentError: In[0].dim(0) and In[1].dim(0) must be the same: [10000,32,784] vs [1,784,784]\n",
    "         [[Node: MatMul_3 = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](transpose, Reshape_9)]]\n",
    "         [[Node: Mean_3/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_239_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
    "\n",
    "    During handling of the above exception, another exception occurred:\n",
    "    \n",
    "After this I must check on HOW to do the correct things for the trainings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = sess.run(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in zip([v.name for v in tf.trainable_variables()], vals):\n",
    "    print (\"var: \"+k+\" = \"+str(v.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
