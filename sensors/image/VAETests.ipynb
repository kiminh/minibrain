{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Autoencoders experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import skimage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code comes from github [SherlockLiao](https://github.com/SherlockLiao/pytorch-beginner/tree/master/08-AutoEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foveal and scaled Images Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Description\n",
    "\n",
    "This experiment has as a goal to train small autoencoders where the encoding (and decoding) parts will be lately used to create a more complex foveal based variational and autoencoder system\n",
    "\n",
    "The output vector from each of these small encoders *DOES NOT* meant to be a compressed (or decompressed) from the image, but means to be a vector that can be used to make later operations possible.\n",
    "\n",
    "On the contrary, I take an assumption leap in the knowledge and intelligence structure that the following holds:\n",
    "The first stages of encoding actually spread the code to a known vector dimension. This vector dimension (might) be bigger than the input domain, but with a more distributted and sparse encoding.\n",
    "\n",
    "This is the reason why the **fovea** is important, to maintain the vector dimension restrained and constant even with bigger images.\n",
    "\n",
    "Later stages and higher layers in the minibrain will instead do operations with these and vectors from other sensors to obtain a same-size vector, mainaining (or compressing) the space size but obtaining more complex embeddings that represent higher-level concepts and contexts.\n",
    "\n",
    "This means if is a dense vector it might (or might not) be small, if it is a sparse vector it might be of a similar to bigger dimensionality than the input patch, the point here is to make it possible to create from an image a more semantically interesting vector for semantic and syntactic operations later (See Vector Symbolic Architectures and Neural Engineering Framework for more ideas on this).\n",
    "\n",
    "Each resolution layer is dedicated to different tasks, that is why also the fovea contains most of the logic, while other scales are getting less and less dedicated logic. This applies specially the full image one, are simpler, they are used for locating important future focus points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Description\n",
    "\n",
    "Different scale Autoencoders are ment to be working on the same image but downsampled differently\n",
    "\n",
    "The experiment will attack the following scales:\n",
    "\n",
    " 1. Full Resolution\n",
    " 2. Downscale 1\n",
    " 3. Downscale 2\n",
    " 4. Downscale 3\n",
    " 5. Full Image Downscaled\n",
    "\n",
    " * The _full resolution_ patch will have 10x10 pixels\n",
    " * Each _Downscale_ will be the previous scale with a 3x3 gaussial filter with step 2 (this means, half the previous resolution)\n",
    " * Each _Downscale_ patch will also be about 10x10 pixels with a maximum of 15x15\n",
    " * The _full image_ downscaled will be no less than 50x50 pixels, the downscaling will  be done with torchvision.transforms.Resize(50)\n",
    " * Each Downscale will have it's own trained autoencoder\n",
    " * the trainig will be done in RGB\n",
    " * the images will be obtained from different sources. See Images sources below\n",
    " * the training images will be cropped such as to train the filters on the cropped part only\n",
    " * for the training, RandomCrop will be used\n",
    " * the encoded vector will consist of the output of the convolutional  filters concatenated with some image statistics on each color layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sources\n",
    "\n",
    " * [Deeplearning.net Datasets](http://deeplearning.net/datasets/)\n",
    " * [Openimages Dataset](https://github.com/openimages/dataset)\n",
    " * [Deeplearning4j Datasets](https://deeplearning4j.org/opendata)\n",
    " * [fast.ai datasets wiki](http://wiki.fast.ai/index.php/Image_Datasets)\n",
    " * [Wikipedia List of Datasets for ML Research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)\n",
    " * [Computer Vision ONLINE](https://computervisiononline.com/)\n",
    " * [Montreal Neurological Institute - MNI BITE](http://www.bic.mni.mcgill.ca/~laurence/data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography, source code and other resources for this experiment on Autoencoders\n",
    "\n",
    "- [VAE Pytorch commented and annotated](https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/)\n",
    "- [Adversarial Variational Bayes](https://arxiv.org/abs/1701.04722)\n",
    "- [Adversarial Autoencoders](https://arxiv.org/abs/1511.05644)\n",
    "- [Denoising Criterion for Variational Auto-Encoding Framework](https://arxiv.org/abs/1511.06406)\n",
    "- [Generative Models](https://github.com/wiseodd/generative-models)\n",
    "- [Another tutorial](https://wiseodd.github.io/techblog/2017/01/24/vae-pytorch/)\n",
    "- [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)\n",
    "- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "- [Pytorch Examples - VAE](https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "- [Variational Autoencoders Explained](https://arxiv.org/abs/1312.6114)\n",
    "- [VAE tutorial](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/variational_auto_encoder)\n",
    "- [github 1](https://github.com/ethanluoyc/pytorch-vae)\n",
    "- [github 2](https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/Variational_autoencoder.py)\n",
    "- [github 3](https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py)\n",
    "- [github 4](https://github.com/ethanluoyc/pytorch-vae/blob/master/vae.py)\n",
    "- []()\n",
    "- []()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "#test one:\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(50),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=50,h=50):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 10x10 patch \n",
    "full_resolution_crop = transforms.Compose([\n",
    "    transforms.RandomCrop(10),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=15):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n",
    "    \n",
    "#the first downsampled filter will have 3 times the spanned range of the fovea, \n",
    "# but will be downsampled to half the definition\n",
    "downsample_1_crop = downsampleTensor(30)\n",
    "#the second downsampled filter will have 6 times the spanned range of the fovea,\n",
    "# but will be downsampled to half of the previous downsampled the definition (or 1/4 resolution of the fovea)\n",
    "downsample_2_crop = downsampleTensor(60)\n",
    "#the first downsampled filter will have 12 times the spanned range of the fovea,\n",
    "# but will be downsampled to half of the previous downsampled the definition (or 1/8 resolution of the fovea)\n",
    "downsample_3_crop = downsampleTensor(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These classes are mostly taken and modified from the official examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/vae/main.py \n",
    "# and another here: https://github.com/ethanluoyc/pytorch-vae/blob/master/vae.py\n",
    "\n",
    "class VAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    As a Variational AutoEncoder with internal linear units\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        #hid_dim = out_dim*20\n",
    "        #in_dim = w * h * channels\n",
    "        self.fc1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.fc21 = nn.Linear(hid_dim, out_dim)\n",
    "        self.fc22 = nn.Linear(hid_dim, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        mu, logvar = self.fc21(h1), self.fc22(h1)\n",
    "        return self.reparameterize(mu, logvar), mu, logvar\n",
    "\n",
    "\n",
    "class VAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    As a Variational AutoEncoder with internal linear units\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_dim, hid_dim)\n",
    "        self.linear2 = torch.nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.sigmoid(self.linear2(x))\n",
    "\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, h_dim=400, code_dim=100):\n",
    "        super(VAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.img_dim = width*height*channels\n",
    "        self.encoder = VAEEncoder(self.img_dim, h_dim, code_dim)\n",
    "        self.decoder = VAEDecoder(code_dim, h_dim, self.img_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x.view(-1, self.img_dim))\n",
    "        return self.decoder(z), mu, logvar\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"vae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"vae_decoder_\"+name+\".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, loss_function, train_loader, epoch, batch_size, width, height, channels, log_interval=100, cuda=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, width, height, channels)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(name, model, test_loader, epoch, batch_size, width, height, channels, cuda=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar, width, height, channels).data[0]\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 100)\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(batch_size, channels, width, height)[:n]])\n",
    "            save_image(comparison.data.cpu(),'vae_results/' + name + '_reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_model(name, model, optimizer, loss_function, transformation, width, height, channels, epochs=10, batch_size=128, dataset = datasets.CIFAR100, cuda=True):\n",
    "    \n",
    "\n",
    "    train_loader, test_loader = get_loaders(batch_size, transformation)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, optimizer, loss_function, train_loader,epoch, batch_size, width, height, channels)\n",
    "        test(name, model, test_loader,epoch, batch_size, width, height, channels)\n",
    "\n",
    "    #torch.save(model, name+\".pth\")\n",
    "    model.save_model(name, \"VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, width, height, channels):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, width*height*channels), size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numbers are chosen so the image size allows for at least 2 convolution layers in a CNN (they'll be used for other parts)\n",
    "models = [\n",
    "          (\"fovea_12x12\", VAE(12,12,3,500,100),full_resolution_crop),\n",
    "          (\"downsample1_16x16-30x30\", VAE(16,16,3,500,100), downsampleTensor(30,16)),\n",
    "          # commented due to image size, it seems that some images fail the training, I'll need to find a good dataset\n",
    "          # (\"downsample2_16x16-60x60\", VAE(16,16,3,500,100), downsampleTensor(60,16)),\n",
    "          # (\"downsample3_16x16-120x120\", VAE(16,16,3,500,100), downsampleTensor(120,16)),\n",
    "          # (\"downsample4_20x20-240x240\", VAE(20,20,3,500,100), downsampleTensor(240,20)),\n",
    "          # (\"downsample5_20x20-480x480\", VAE(20,20,3,500,100), downsampleTensor(480,20)),\n",
    "          (\"fullimage_20\", VAE(20,20,3,500,100), fullimage_preprocess(20,20)),\n",
    "          (\"fullimage_48\", VAE(48,48,3,500,100), fullimage_preprocess(48,48)),\n",
    "          (\"fullimage_72\", VAE(72,72,3,500,100), fullimage_preprocess(72,72)),\n",
    "          (\"fullimage_100\", VAE(96,96,3,500,100), fullimage_preprocess(96,96)),\n",
    "         ]\n",
    "\n",
    "\n",
    "def train_all(models, epochs=10):\n",
    "    for n,m,t in models:\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"training model \"+ n)\n",
    "        m.cuda()\n",
    "        optimizer = optim.Adam(m.parameters(), lr=1e-3)\n",
    "        train_save_model(n,m, optimizer, loss_function, t, m.width, m.height, m.channels, dataset = datasets.CocoDetection, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n,m,t = models[1]\n",
    "# m.cuda()\n",
    "# optimizer = optim.Adam(m.parameters(), lr=1e-3)\n",
    "# train_save_model(n,m, optimizer, loss_function, t, m.width, m.height, m.channels, dataset = datasets.CocoDetection, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_all(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Testing and learning part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_URL = 'https://s3.amazonaws.com/outcome-blog/wp-content/uploads/2017/02/25192225/cat.jpg'\n",
    "response = requests.get(IMG_URL)\n",
    "img_pil = Image.open(io.BytesIO(response.content))#.convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = preprocess(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "i = to_pil(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil(img_tensor[:,10:30,5:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
