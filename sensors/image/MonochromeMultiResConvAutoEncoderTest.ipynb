{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from helper_modules import *\n",
    "from multi_res_cae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport helpers, helper_modules, multi_res_cae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "helper_modules helpers multi_res_cae\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frcae = MultiFullCAE((640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrcae = MultiResCAE([640,480])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 462 ms, total: 1.73 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultiFullCAE(in_img_shape=(32,32), full_image_resize=(24,24)).cuda()\n",
    "# model = MultiResCAE(in_img_shape=[32,32], channels=3, conv_layer_feat=[16, 32, 64],\n",
    "#                  res_px=[[24, 24], [16, 16], [12, 12]], crop_sizes=[[32, 32], [24,24], [12, 12]],\n",
    "#                  # conv_sizes = [(3,5,7), (3,5,7,11), (3,5,7,11)]  # this is too much I think\n",
    "#                  # conv_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5, 7]]  # test b\n",
    "#                  conv_sizes=[[5, 7, 11], [3, 5, 7, 9], [1, 3, 5]]  # test c\n",
    "#         ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 966 µs, sys: 0 ns, total: 966 µs\n",
      "Wall time: 972 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 32, 32)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CPU times: user 712 ms, sys: 254 ms, total: 966 ms\n",
      "Wall time: 976 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transformation = monochrome_preprocess(32,32)\n",
    "#transformation = fullimage_preprocess(32,32)\n",
    "#train_loader, test_loader = get_loaders(batch_size, transformation, dataset=datasets.CocoDetection)\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], loss:0.3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/ipykernel_launcher.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/20], loss:0.3606\n",
      "epoch [3/20], loss:0.2855\n",
      "epoch [4/20], loss:0.3345\n",
      "epoch [5/20], loss:0.2309\n",
      "epoch [6/20], loss:0.2653\n",
      "epoch [7/20], loss:0.2233\n",
      "epoch [8/20], loss:0.2906\n",
      "epoch [9/20], loss:0.2600\n",
      "epoch [10/20], loss:0.2384\n",
      "epoch [11/20], loss:0.2311\n",
      "epoch [12/20], loss:0.2851\n",
      "epoch [13/20], loss:0.2092\n",
      "epoch [14/20], loss:0.2814\n",
      "epoch [15/20], loss:0.2173\n",
      "epoch [16/20], loss:0.2584\n",
      "epoch [17/20], loss:0.2852\n",
      "epoch [18/20], loss:0.2499\n",
      "epoch [19/20], loss:0.2212\n",
      "epoch [20/20], loss:0.2242\n",
      "CPU times: user 11min 48s, sys: 2min 28s, total: 14min 17s\n",
      "Wall time: 14min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 4 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './fmrcae_results/c_MergingLayer_in-32x32_3-5-7-11_out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './fmrcae_results/c_MergingLayer_in-3-5-7-11_in_image_{}.png'.format(epoch))\n",
    "#     if loss.data[0] < 0.21: #arbitrary number because I saw that it works well enough\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(\"fmrcae_in-64x64_32x32_3-5-7-11.pth\", model)\n",
    "#torch.save(\"mrcae_in-32x32_.pth\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
