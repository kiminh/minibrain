{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly Taken from examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "# Other resources\n",
    "# https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "# https://github.com/kvfrans/variational-autoencoder\n",
    "# https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "# https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, w,h, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.first_feature_count = first_feature_count\n",
    "        \n",
    "        self.indices = []\n",
    "        \n",
    "        padding = math.floor(kernel_size/2)\n",
    "\n",
    "        self.l_features = [channels]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            l_features.append(first_feature_count * (2**(i)) )\n",
    "            \n",
    "        for i in range(levels):\n",
    "            \n",
    "            nfeat = self.l_features[i+1]\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(self.l_features[i], nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(nfeat, nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        #self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "        self.conv_dim = ((w*h)// ((2**levels)**2)) * self.l_features[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            layer = self.layers[i]\n",
    "            out, idx  = layer(out)\n",
    "            self.indices.append(idx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, width, height, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.l_features = [channels]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            l_features.append(first_feature_count * (2**(i)) )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        #self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "        self.conv_dim = ((w*h)// ((2**levels)**2)) * self.l_features[-1]\n",
    "\n",
    "        self.unpool_1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_1 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_2 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l1_feat, channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            rev_i = -(i+1)\n",
    "            out = F.max_unpool2d(out, self.encoder.indices[rev_i],2, stride=2)\n",
    "            out = self.layers[i](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, levels=2, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, levels, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, levels, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_factors(n):\n",
    "    i = 2\n",
    "    factors = []\n",
    "    while i * i <= n:\n",
    "        if n % i:\n",
    "            i += 1\n",
    "        else:\n",
    "            n //= i\n",
    "            factors.append(i)\n",
    "    if n > 1:\n",
    "        factors.append(n)\n",
    "    return sorted(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "def crop_fovea(size=12):\n",
    "    sample = transforms.Compose([\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    return sample\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n",
    "\n",
    "#TODO improve this to make it from Standard Deviation and kernel size\n",
    "def get_gaussian_kernel(kernlen=5, nsig=3):\n",
    "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
    "    interval = (2*nsig+1.)/(kernlen)\n",
    "    x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n",
    "    kern1d = np.diff(st.norm.cdf(x))\n",
    "    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n",
    "    kernel = kernel_raw/kernel_raw.sum()\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb53af59390>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb53b486c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(get_gaussian_kernel(5, 3), interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, width, height):\n",
    "        super().__init__()\n",
    "        self.resizer = transforms.Compose([\n",
    "                        torchvision.transforms.ToPILImage(),  #is this correct? will this be slow??\n",
    "                        torchvision.transforms.Resize(width, height),\n",
    "                        torchvision.transforms.ToTensor()\n",
    "                        ])\n",
    "    def forward(self, x):\n",
    "        return self.resizer(x)\n",
    "        #return self.conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiResCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Resolution group of Convolutional Autoencoders\n",
    "    This module intends to group several autoencoders that accompany different resolutions\n",
    "    The goal of this module is be able to train and maintain all the filters in one place\n",
    "    This model can be saved and loaded as a single element\n",
    "    This model outputs tensor of dimension 1x1xN that is the concatenation of the output of all the encoders ensemble\n",
    "    the FULL IMAGE is not considered in this \n",
    "    \"\"\"\n",
    "    def __init__(self, in_img_shape, channels=3, res_levels=3, conv_layer_feat=[32,16,16], \n",
    "                 res_px=[[12,12],[16,16],[20,20]], crop_sizes=[[12,12],[32,32],[64,64]],\n",
    "                 # conv_sizes = [(3,5,7), (3,5,7,11), (3,5,7,11)],  # this is too much I think\n",
    "                 conv_sizes = [(3,5,7), (3,5), (3,5)] ):\n",
    "        \"\"\"\n",
    "        @param in_imag_shape : [width, height]  # the input image shape, to be able to pre-compute the transform matrices\n",
    "        \"\"\"\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        self.res_levels = res_levels  #number of resolution levels (NOT including the full image)\n",
    "        self.conv_layer_feat = conv_layer_feat  # number of convolutional filters per CAE in the first level\n",
    "        self.res_px = res_px  # downsampled resolution in pixels for each resolution\n",
    "        self.conv_sizes = conv_sizes  # conv filter sizes per layer, one encoder per size per layer\n",
    "        ##\n",
    "        #compute the maximum number of levels that this resolution can handle, \n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.max_levels = [ prime_factors[min(i)].count(2) for i in res_px]\n",
    "        ##\n",
    "        # Pre-computing cropping matrices \n",
    "        self.crop_sizes = torch.IntTensor(crop_sizes)  # Ps - Patches sizes -  size of the patch to crop\n",
    "        self.half_crop_sizes = torch.IntTensor(crop_sizes)  # Ps/2 - Patches half sizes -  half size of the patch to crop, to compute positions\n",
    "        self.ref_patch =  torch.IntTensor(conv_sizes[:].append(in_img_shape)[::-1]) # RP - Reference Patches\n",
    "        #pre-compute Patch Dynamic Range (pixel wise)\n",
    "        self._pdr = self.ref_patch - self.crop_sizes\n",
    "        # saves the last patch centers\n",
    "        self._last_centers = None  #make a variable placeholder here\n",
    "        # saves the last patch ranges\n",
    "        self._last_px_mins = None\n",
    "        self._last_px_maxs = None\n",
    "        ##\n",
    "        # Actual work is done in the following modules\n",
    "        #\n",
    "        self.downsamplers = [DownsampleLayer(w,h) for w,h in res_px]\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        \n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_encoders()\n",
    "        self._create_decoders()\n",
    "        \n",
    "    def _create_encoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_encoders = []\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w,h = self.res_px[i]  # resolution of the image for the encoder\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEEncoder(w, h, c, levels, j, conv_features)\n",
    "                res_encoders.append(enc)\n",
    "            self.encoders.append(res_encoders)\n",
    "        \n",
    "    def _create_decoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_decoders = []\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w,h = self.res_px[i]  # resolution of the image for the encoder\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEDecoder(self.encoders[i][j], w, h, c, levels, j, conv_features)\n",
    "                res_decoders.append(enc)\n",
    "            self.decoders.append(res_encoders)\n",
    "    \n",
    "    def compute_patches(crop_centers):\n",
    "        \"\"\"\n",
    "        Computes the ranges that have to be cropped from the input image.\n",
    "        The computation follows the principles:\n",
    "         - The inner patches positions are relative to the outer ones, in the corresponding hierarchy\n",
    "         - The patches can not go off the container image\n",
    "        \n",
    "        @param crop_centers MUST be a HalfTensor or FloatTensor\n",
    "        @returns : (centers, min_points, max_points) two IntTensors with the center and ranges that each patch occupies\n",
    "        \n",
    "        The returned elements contain the patches from the bigger to the smaller one\n",
    "        \"\"\"\n",
    "        # formula follows this -> see how to implement it in vector operations to make it faster, or precompute it for a fixed image size\n",
    "        # x_min = 0 + patch_width/2\n",
    "        # x_max = full_img.width - patch_width/2\n",
    "        # x_pos = c * (x_max - x_min) == full_img.width - patch_width\n",
    "        # patch = img[x_pos - patch_width/2 : x_pos + patch_width/2] (warning on patch size)\n",
    "        \n",
    "        self._last_centers = crop_centers * self._pdr  # element wise multiplication\n",
    "        self._last_px_mins = self._last_centers - self.half_crop_sizes\n",
    "        self._last_px_maxs = self._last_centers + self.half_crop_sizes\n",
    "        \n",
    "        return self._last_centers, self._last_px_mins, self._last_px_maxs\n",
    "    \n",
    "    def forward(self, x, crop_centers=torch.HalfTensor([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])):\n",
    "        \"\"\"\n",
    "        x the input image\n",
    "        crop_centers, a list of centers c where  c in [0:1],\n",
    "        centers go from the larger one to the lower one\n",
    "        \"\"\"\n",
    "        full_img = x\n",
    "        ########################\n",
    "        # BEGIN Cropping\n",
    "        ###\n",
    "        # compute the patches positions\n",
    "        centers, min_px, max_px = compute_patches(crop_centers)\n",
    "        # make only one vector out of the needed ones\n",
    "        ranges = min_px.cat(max_px)  \n",
    "        # Get all the cropped layers\n",
    "        crops = []\n",
    "        # TODO find out how to do this without passing from GPU to CPU and vice versa\n",
    "        prev_crop = full_img\n",
    "        for pr in ranges:\n",
    "            #  pr == pixel_ranges = [x0,y0,x1,y1]\n",
    "            crop = prev_crop[pr[0]:pr[2],pr[1]:pr[3]]  # crop the input\n",
    "            crops.append(crop)\n",
    "            prev_crop = crop\n",
    "        \n",
    "        # Reverse the list to compute from the fovea to the other dimensions -> I'm not sure if the computation is done in place or not, so starting from the more detailed one\n",
    "        crops = crops[::-1]\n",
    "        # encoded outputs from each resolution layer\n",
    "        codes = []  \n",
    "        for i in range(len(crops)):\n",
    "            layer = crops(i)\n",
    "            encs = self.encoders[i]  # encoders for the current resolution\n",
    "            #downsample it as many times as needed (basically the )\n",
    "            downsampler = self.downsamplers[i]\n",
    "            layer = downsampler(layer)  \n",
    "            # apply all the encoders in the corresponding i'th layer\n",
    "            cds = []  # codes of the corresponding layer (at the same resolution)\n",
    "            for enc in encs:\n",
    "                c = enc(layer)\n",
    "                cds.append(c)\n",
    "            codes.append(cds)\n",
    "        # Create a simple embedding (maybe later work with a multinomial probability distribution)\n",
    "        # The embeddings contain also the crop_centers, the scaling (downsample), \n",
    "        #     the zoom (upsample) and the relative crop sizes to the complete image\n",
    "        \n",
    "        ###\n",
    "        # END Encoding\n",
    "        ########################\n",
    "        # BEGIN decoding\n",
    "        ###\n",
    "        # I'm in doubt here if I should do the reverse process of the encoding for each encoder and then use the outputs to generate the input, \n",
    "        # or I should create a single composite decoder that handles the entire reconstruction\n",
    "        # ... ???\n",
    "        \n",
    "        ###\n",
    "        # END Decoding\n",
    "        ########################\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiFullCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Group of Convolutional Autoencoders for a single input resolution\n",
    "    The image is treated as monochrome\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels=1, ds_full_image_cae=True, full_image_size=32, full_img_conv_feat=16, full_conv_sizes=(3,5,7)):\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.levels = prime_factors[full_image_size].count(2)\n",
    "        self.conv_sizes = conv_sizes  # filter sizes to create for each resolution\n",
    "        self.ds_full_img_cae = ds_full_image_cae  # indicate if create or not the full image downsample conv encoder\n",
    "        self.full_image_size = full_image_size  # image to which to redimension the entire input image (if previous is True)\n",
    "        self.full_img_conv_feat = full_img_conv_feat  # number of convolutional filters to use per layer \n",
    "        self.full_conv_sizes = full_conv_sizes  # sizes of the convolutional filters, one encoder per size\n",
    "        \n",
    "        self.full_encoders = []\n",
    "        self.full_decoders = []\n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_full_encoders()\n",
    "        self._create_full_decoders()\n",
    "        \n",
    "        \n",
    "    def _create_full_encoders(channels=1):\n",
    "        for cs in self.full_conv_sizes:\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEEncoder(width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_encoders.append(enc)\n",
    "        \n",
    "    def _create_full_decoders(channels=1):\n",
    "        for i in range(self.full_conv_sizes):\n",
    "            cs = full_conv_sizes[i]\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEDecoder(self.full_encoders[i], width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_decoders.append(enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input = downsampled full image converted to monochrome\n",
    "        ########################\n",
    "        # BEGIN Encoding\n",
    "        ###\n",
    "        \n",
    "        # for the moment this full image is computed each time, but in the future this will be \n",
    "        #     done ONLY if the input image changes\n",
    "        #     maybe what we want to work with is only the difference from previous frames -> future when working in dynamic environments\n",
    "        # encoder full downsampled image\n",
    "        # \n",
    "        # join  all encodings into a single vector\n",
    "        # END Encoding\n",
    "        ########################\n",
    "        #BEGIN decoding\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = transforms.Grayscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3,500,200,32).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-2xfc-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-2xfc-in_image_{}.png'.format(epoch))\n",
    "    if loss.data[0] < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break\n",
    "\n",
    "\n",
    "model.save_model(\"2x2-2xfc-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n",
    "- 2 layers with 2 convolutional stages each  <- **best result**\n",
    "- 2 layers with 2 convolutional stages each and 2 fully connected layers  <- bigger model and a bit slower to converge, but results are good too\n",
    "- 2 layers with 2 convolutional stages each with batch normalization\n",
    "- 2 layers with 4 convolutional stages each <- **worst result**\n",
    "\n",
    "\n",
    "2 layers with 4 conv stages each does not give the same results as 2 layers with 2 conv stages\n",
    "\n",
    "It not only converges MUCH faster and the models are smaller, but the actually the convergence is much better\n",
    "\n",
    "For bathc normalization happens the same, without batchnorm2d converges faster and model is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
