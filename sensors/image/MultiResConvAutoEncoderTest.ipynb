{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly Taken from examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "# Other resources\n",
    "# https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "# https://github.com/kvfrans/variational-autoencoder\n",
    "# https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "# https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, w,h, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.first_feature_count = first_feature_count\n",
    "        \n",
    "        self.indices = []\n",
    "        \n",
    "        padding = math.floor(kernel_size/2)\n",
    "\n",
    "        self.l_features = [channels]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            l_features.append(first_feature_count * (2**(i)) )\n",
    "            \n",
    "        for i in range(levels):\n",
    "            \n",
    "            nfeat = self.l_features[i+1]\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(self.l_features[i], nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(nfeat, nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            layer = self.layers[i]\n",
    "            out, idx  = layer(out)\n",
    "            self.indices.append(idx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, maxpool_indices, width, height, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.indices = maxpool_indices\n",
    "        \n",
    "        self.l_features = [channels]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            l_features.append(first_feature_count * (2**(i)) )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "\n",
    "        self.unpool_1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_1 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_2 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l1_feat, channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            rev_i = -(i+1)\n",
    "            out = F.max_unpool2d(out, self.indices[rev_i],2, stride=2)\n",
    "            out = self.layers[i](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, levels=2, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, levels, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, levels, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_factors(n):\n",
    "    i = 2\n",
    "    factors = []\n",
    "    while i * i <= n:\n",
    "        if n % i:\n",
    "            i += 1\n",
    "        else:\n",
    "            n //= i\n",
    "            factors.append(i)\n",
    "    if n > 1:\n",
    "        factors.append(n)\n",
    "    return sorted(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "def crop_fovea(size=12):\n",
    "    sample = transforms.Compose([\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    return sample\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiResCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Resolution group of Convolutional Autoencoders\n",
    "    This module intends to group several autoencoders that accompany different resolutions\n",
    "    The goal of this module is be able to train and maintain all the filters in one place\n",
    "    This model can be saved and loaded as a single element\n",
    "    This model outputs tensor of dimension 1x1xN that is the concatenation of the output of all the encoders ensemble\n",
    "    the FULL IMAGE is not considered in this \n",
    "    \"\"\"\n",
    "    def __init__(self, channels=3, res_levels=3, conv_layer_feat=[32,16,16], res_px=[12,16,20],\n",
    "                 crop_sizes=[12,32,64],\n",
    "                 # conv_sizes = [(3,5,7), (3,5,7,11), (3,5,7,11)],  # this is too much I think\n",
    "                 conv_sizes = [(3,5,7), (3,5), (3,5)], \n",
    "                 ds_full_image_cae=True, full_image_size=32, full_img_conv_feat=16, full_conv_sizes=(3,5,7)):\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        self.res_levels = res_levels  #number of resolution levels (NOT including the full image)\n",
    "        self.conv_layer_feat = conv_layer_feat  # number of convolutional filters per CAE in the first level\n",
    "        self.res_px = res_px  # downsampled resolution in pixels for each resolution\n",
    "        self.conv_sizes = conv_sizes  # conv filter sizes per layer, one encoder per size per layer\n",
    "        self.crop_sizes = crop_sizes  # size of the patch to crop\n",
    "        #compute the maximum number of levels that this resolution can handle, \n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.max_levels = [ prime_factors[i].count(2) for i in res_px]\n",
    "        self.conv_sizes = conv_sizes  # filter sizes to create for each resolution\n",
    "        \n",
    "        self.encoders = []\n",
    "        self.full_encoders = []\n",
    "        self.full_decoders = []\n",
    "        self.decoders = []\n",
    "        \n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_encoders()\n",
    "        self._create_decoders()\n",
    "        \n",
    "    def _create_encoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_encoders = []\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w = h = r_px = self.res_px[i]  # resolution of the image for the encoder\n",
    "            # crop_px = self.crop_sizes[i]  # pre-processing this is the crop size for the input image\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEEncoder(w, h, c, levels, j, conv_features)\n",
    "                res_encoders.append(enc)\n",
    "            self.encoders.append(res_encoders)\n",
    "        \n",
    "    def _create_decoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_decoders = []\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w = h = r_px = self.res_px[i]  # resolution of the image for the encoder\n",
    "            # crop_px = self.crop_sizes[i]  # pre-processing this is the crop size for the input image\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEDecoder(self.encoders[i][j], w, h, c, levels, j, conv_features)\n",
    "                res_decoders.append(enc)\n",
    "            self.decoders.append(res_encoders)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        in_img = x\n",
    "        ########################\n",
    "        # BEGIN Encoding\n",
    "        ###\n",
    "        # \n",
    "        # For the moment all crops are in the center of the given image\n",
    "        # fovea crop\n",
    "        # encode fovea\n",
    "        #\n",
    "        # next crops and downsamples\n",
    "        # encode downsamples\n",
    "        #\n",
    "        # reshape and concatenate all encodings -> create a simple embedding (maybe later work with a multinomial probability distribution)\n",
    "        ###\n",
    "        # END Encoding\n",
    "        ########################\n",
    "        #BEGIN decoding\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiFullCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Group of Convolutional Autoencoders for a single input resolution\n",
    "    The image is treated as monochrome\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels=1, ds_full_image_cae=True, full_image_size=32, full_img_conv_feat=16, full_conv_sizes=(3,5,7)):\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.levels = prime_factors[full_image_size].count(2)\n",
    "        self.conv_sizes = conv_sizes  # filter sizes to create for each resolution\n",
    "        self.ds_full_img_cae = ds_full_image_cae  # indicate if create or not the full image downsample conv encoder\n",
    "        self.full_image_size = full_image_size  # image to which to redimension the entire input image (if previous is True)\n",
    "        self.full_img_conv_feat = full_img_conv_feat  # number of convolutional filters to use per layer \n",
    "        self.full_conv_sizes = full_conv_sizes  # sizes of the convolutional filters, one encoder per size\n",
    "        \n",
    "        self.full_encoders = []\n",
    "        self.full_decoders = []\n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_full_encoders()\n",
    "        self._create_full_decoders()\n",
    "        \n",
    "        \n",
    "    def _create_full_encoders(channels=1):\n",
    "        for cs in self.full_conv_sizes:\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEEncoder(width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_encoders.append(enc)\n",
    "        \n",
    "    def _create_full_decoders(channels=1):\n",
    "        for i in range(self.full_conv_sizes):\n",
    "            cs = full_conv_sizes[i]\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEDecoder(self.full_encoders[i], width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_decoders.append(enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input = downsampled full image converted to monochrome\n",
    "        ########################\n",
    "        # BEGIN Encoding\n",
    "        ###\n",
    "        \n",
    "        # for the moment this full image is computed each time, but in the future this will be \n",
    "        #     done ONLY if the input image changes\n",
    "        #     maybe what we want to work with is only the difference from previous frames -> future when working in dynamic environments\n",
    "        # encoder full downsampled image\n",
    "        # \n",
    "        # join  all encodings into a single vector\n",
    "        # END Encoding\n",
    "        ########################\n",
    "        #BEGIN decoding\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = transforms.Grayscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mtg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           Grayscale\n",
       "\u001b[0;31mString form:\u001b[0m    <torchvision.transforms.transforms.Grayscale object at 0x7f9c6080ef60>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/DeepLearning/venv3/lib/python3.5/site-packages/torchvision/transforms/transforms.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convert image to grayscale.\n",
       "\n",
       "Args:\n",
       "    num_output_channels (int): (1 or 3) number of channels desired for output image\n",
       "\n",
       "Returns:\n",
       "    PIL Image: Grayscale version of the input.\n",
       "    - If num_output_channels == 1 : returned image is single channel\n",
       "    - If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Args:\n",
       "    img (PIL Image): Image to be converted to grayscale.\n",
       "\n",
       "Returns:\n",
       "    PIL Image: Randomly grayscaled image.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3,500,200,32).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.4138\n",
      "epoch [2/100], loss:0.2646\n",
      "epoch [3/100], loss:0.3089\n",
      "epoch [4/100], loss:0.2290\n",
      "epoch [5/100], loss:0.2640\n",
      "epoch [6/100], loss:0.3082\n",
      "epoch [7/100], loss:0.2789\n",
      "epoch [8/100], loss:0.2025\n",
      "epoch [9/100], loss:0.1942\n",
      "epoch [10/100], loss:0.2041\n",
      "epoch [11/100], loss:0.2508\n",
      "epoch [12/100], loss:0.2682\n",
      "epoch [13/100], loss:0.2567\n",
      "epoch [14/100], loss:0.2532\n",
      "epoch [15/100], loss:0.2498\n",
      "epoch [16/100], loss:0.2509\n",
      "epoch [17/100], loss:0.2485\n",
      "epoch [18/100], loss:0.2437\n",
      "epoch [19/100], loss:0.2815\n",
      "epoch [20/100], loss:0.1903\n",
      "epoch [21/100], loss:0.2225\n",
      "epoch [22/100], loss:0.2277\n",
      "epoch [23/100], loss:0.2364\n",
      "epoch [24/100], loss:0.2252\n",
      "epoch [25/100], loss:0.2694\n",
      "epoch [26/100], loss:0.1984\n",
      "epoch [27/100], loss:0.2271\n",
      "epoch [28/100], loss:0.3034\n",
      "epoch [29/100], loss:0.2541\n",
      "epoch [30/100], loss:0.2344\n",
      "epoch [31/100], loss:0.2076\n",
      "epoch [32/100], loss:0.2221\n",
      "epoch [33/100], loss:0.2719\n",
      "epoch [34/100], loss:0.1923\n",
      "epoch [35/100], loss:0.2095\n",
      "epoch [36/100], loss:0.2596\n",
      "epoch [37/100], loss:0.2550\n",
      "epoch [38/100], loss:0.2363\n",
      "epoch [39/100], loss:0.2445\n",
      "epoch [40/100], loss:0.2521\n",
      "epoch [41/100], loss:0.1987\n",
      "epoch [42/100], loss:0.1879\n",
      "epoch [43/100], loss:0.1972\n",
      "epoch [44/100], loss:0.2940\n",
      "epoch [45/100], loss:0.2636\n",
      "epoch [46/100], loss:0.1925\n",
      "epoch [47/100], loss:0.2465\n",
      "epoch [48/100], loss:0.2364\n",
      "epoch [49/100], loss:0.1989\n",
      "epoch [50/100], loss:0.2321\n",
      "epoch [51/100], loss:0.2317\n",
      "epoch [52/100], loss:0.2206\n",
      "epoch [53/100], loss:0.1844\n",
      "epoch [54/100], loss:0.2661\n",
      "epoch [55/100], loss:0.2204\n",
      "epoch [56/100], loss:0.2462\n",
      "epoch [57/100], loss:0.2152\n",
      "epoch [58/100], loss:0.1952\n",
      "epoch [59/100], loss:0.3290\n",
      "epoch [60/100], loss:0.2265\n",
      "epoch [61/100], loss:0.2401\n",
      "epoch [62/100], loss:0.2317\n",
      "epoch [63/100], loss:0.1886\n",
      "epoch [64/100], loss:0.1555\n",
      "epoch [65/100], loss:0.2122\n",
      "epoch [66/100], loss:0.2117\n",
      "epoch [67/100], loss:0.2657\n",
      "epoch [68/100], loss:0.2382\n",
      "epoch [69/100], loss:0.3080\n",
      "epoch [70/100], loss:0.1970\n",
      "epoch [71/100], loss:0.2335\n",
      "epoch [72/100], loss:0.2216\n",
      "epoch [73/100], loss:0.2435\n",
      "epoch [74/100], loss:0.2335\n",
      "epoch [75/100], loss:0.1922\n",
      "epoch [76/100], loss:0.1479\n",
      "CPU times: user 2min 20s, sys: 8.04 s, total: 2min 28s\n",
      "Wall time: 5min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type CAEEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type CAEDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-2xfc-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-2xfc-in_image_{}.png'.format(epoch))\n",
    "    if loss.data[0] < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break\n",
    "\n",
    "\n",
    "model.save_model(\"2x2-2xfc-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n",
    "- 2 layers with 2 convolutional stages each  <- **best result**\n",
    "- 2 layers with 2 convolutional stages each and 2 fully connected layers  <- bigger model and a bit slower to converge, but results are good too\n",
    "- 2 layers with 2 convolutional stages each with batch normalization\n",
    "- 2 layers with 4 convolutional stages each <- **worst result**\n",
    "\n",
    "\n",
    "2 layers with 4 conv stages each does not give the same results as 2 layers with 2 conv stages\n",
    "\n",
    "It not only converges MUCH faster and the models are smaller, but the actually the convergence is much better\n",
    "\n",
    "For bathc normalization happens the same, without batchnorm2d converges faster and model is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
