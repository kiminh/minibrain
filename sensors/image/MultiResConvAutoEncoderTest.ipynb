{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly Taken from examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "# Other resources\n",
    "# https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "# https://github.com/kvfrans/variational-autoencoder\n",
    "# https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "# https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, width, height, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.first_feature_count = first_feature_count\n",
    "        \n",
    "        self.indices = []\n",
    "        \n",
    "        padding = math.floor(kernel_size/2)\n",
    "\n",
    "        self.l_features = [channels]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            self.l_features.append(first_feature_count * (2**(i)) )\n",
    "            \n",
    "        for i in range(levels):\n",
    "            \n",
    "            nfeat = self.l_features[i+1]\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(self.l_features[i], nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(nfeat, nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        #self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "        self.conv_dim = ((width*height)// ((2**levels)**2)) * self.l_features[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            layer = self.layers[i]\n",
    "            out, idx  = layer(out)\n",
    "            self.indices.append(idx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = CAEEncoder(12,12).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAEEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(c_model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, width, height, channels=3, levels=2, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        self.width = width\n",
    "        self.heigth = height\n",
    "        self.channels = channels\n",
    "        self.levels = levels\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.l_features = [channels]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(levels + 1):\n",
    "            self.l_features.append(first_feature_count * (2**(i)) )\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        #self.conv_dim = int(((w*h)/ ((2**levels)**2)) * self.l_features[-1])\n",
    "        self.conv_dim = ((width*height)// ((2**levels)**2)) * self.l_features[-1]\n",
    "\n",
    "        for i in range(levels):\n",
    "            \n",
    "            nfeat = self.l_features[i+1]\n",
    "            \n",
    "            last_op = nn.ReLU() if i+1 < levels else nn.Tanh()\n",
    "            layer = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(nfeat, nfeat, kernel_size=kernel_size, padding=padding),\n",
    "                    nn.ReLU(),\n",
    "                    nn.ConvTranspose2d(nfeat, self.l_features[i], kernel_size=kernel_size, padding=padding),\n",
    "                    last_op\n",
    "\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.levels):\n",
    "            rev_i = -(i+1)\n",
    "            out = F.max_unpool2d(out, self.encoder.indices[rev_i],2, stride=2)\n",
    "            out = self.layers[i](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = CAEDecoder(c_model,12,12).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAEDecoder(\n",
       "  (encoder): CAEEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, levels=2, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, levels, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, levels, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAE(\n",
       "  (encoder): CAEEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): CAEDecoder(\n",
       "    (encoder): CAEEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_factors(n):\n",
    "    i = 2\n",
    "    factors = []\n",
    "    while i * i <= n:\n",
    "        if n % i:\n",
    "            i += 1\n",
    "        else:\n",
    "            n //= i\n",
    "            factors.append(i)\n",
    "    if n > 1:\n",
    "        factors.append(n)\n",
    "    return sorted(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "def crop_fovea(size=12):\n",
    "    sample = transforms.Compose([\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    return sample\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n",
    "\n",
    "#TODO improve this to make it from Standard Deviation and kernel size\n",
    "def get_gaussian_kernel(kernlen=5, nsig=3):\n",
    "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
    "    interval = (2*nsig+1.)/(kernlen)\n",
    "    x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n",
    "    kern1d = np.diff(st.norm.cdf(x))\n",
    "    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n",
    "    kernel = kernel_raw/kernel_raw.sum()\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc006877780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(get_gaussian_kernel(5, 3), interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, width, height):\n",
    "        super().__init__()\n",
    "        self.resizer = transforms.Compose([\n",
    "                        torchvision.transforms.ToPILImage(),  #is this correct? will this be slow??\n",
    "                        torchvision.transforms.Resize(width, height),\n",
    "                        torchvision.transforms.ToTensor()\n",
    "                        ])\n",
    "    def forward(self, x):\n",
    "        return self.resizer(x)\n",
    "        #return self.conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiResCAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Resolution group of Convolutional Autoencoders\n",
    "    This module intends to group several autoencoders that accompany different resolutions\n",
    "    The goal of this module is be able to train and maintain all the filters in one place\n",
    "    This model can be saved and loaded as a single element\n",
    "    This model outputs tensor of dimension 1x1xN that is the concatenation of the output of all the encoders ensemble\n",
    "    the FULL IMAGE is not considered in this \n",
    "    \"\"\"\n",
    "    def __init__(self, in_img_shape, channels=3, res_levels=3, conv_layer_feat=[32,16,16], \n",
    "                 res_px=[[12,12],[16,16],[20,20]], crop_sizes=[[12,12],[32,32],[64,64]],\n",
    "                 # conv_sizes = [(3,5,7), (3,5,7,11), (3,5,7,11)],  # this is too much I think\n",
    "                 conv_sizes = [(3,5,7), (3,5), (3,5)] ):\n",
    "        \"\"\"\n",
    "        @param in_imag_shape : [width, height]  # the input image shape, to be able to pre-compute the transform matrices\n",
    "        \"\"\"\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        self.res_levels = res_levels  #number of resolution levels (NOT including the full image)\n",
    "        self.conv_layer_feat = conv_layer_feat  # number of convolutional filters per CAE in the first level\n",
    "        self.res_px = res_px  # downsampled resolution in pixels for each resolution\n",
    "        self.conv_sizes = conv_sizes  # conv filter sizes per layer, one encoder per size per layer\n",
    "        ##\n",
    "        #compute the maximum number of levels that this resolution can handle, \n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.max_levels = [ prime_factors[min(i)].count(2) for i in res_px]\n",
    "        ##\n",
    "        # Pre-computing cropping matrices \n",
    "        self.crop_sizes = torch.IntTensor(crop_sizes)  # Ps - Patches sizes -  size of the patch to crop\n",
    "        self.half_crop_sizes = torch.IntTensor(crop_sizes)  # Ps/2 - Patches half sizes -  half size of the patch to crop, to compute positions\n",
    "        self.ref_patch =  torch.IntTensor(conv_sizes[:].append(in_img_shape)[::-1]) # RP - Reference Patches\n",
    "        #pre-compute Patch Dynamic Range (pixel wise)\n",
    "        self._pdr = self.ref_patch - self.crop_sizes\n",
    "        # saves the last patch centers\n",
    "        self._last_centers = None  #make a variable placeholder here\n",
    "        # saves the last patch ranges\n",
    "        self._last_px_mins = None\n",
    "        self._last_px_maxs = None\n",
    "        ##\n",
    "        # Actual work is done in the following modules\n",
    "        #\n",
    "        self.downsamplers = [DownsampleLayer(w,h) for w,h in res_px]\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_encoders()\n",
    "        self._create_decoders()\n",
    "        \n",
    "    def _create_encoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_encoders = nn.ModuleList()\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w,h = self.res_px[i]  # resolution of the image for the encoder\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEEncoder(w, h, c, levels, j, conv_features)\n",
    "                res_encoders.append(enc)\n",
    "            self.encoders.append(res_encoders)\n",
    "        \n",
    "    def _create_decoders():\n",
    "        for i in range(self.res_levels):\n",
    "            res_decoders = nn.ModuleList()\n",
    "            # Conv Encoder description\n",
    "            conv_features = self.conv_layer_feat[i]\n",
    "            l_conv_sizes = self.conv_sizes[i]\n",
    "            levels = self.max_levels[i]\n",
    "            # Image size\n",
    "            c = self.channels\n",
    "            w,h = self.res_px[i]  # resolution of the image for the encoder\n",
    "\n",
    "            for j in range(l_conv_sizes):\n",
    "                enc = CAEDecoder(self.encoders[i][j], w, h, c, levels, j, conv_features)\n",
    "                res_decoders.append(enc)\n",
    "            self.decoders.append(res_encoders)\n",
    "    \n",
    "    def compute_patches(crop_centers):\n",
    "        \"\"\"\n",
    "        Computes the ranges that have to be cropped from the input image.\n",
    "        The computation follows the principles:\n",
    "         - The inner patches positions are relative to the outer ones, in the corresponding hierarchy\n",
    "         - The patches can not go off the container image\n",
    "        \n",
    "        @param crop_centers MUST be a HalfTensor or FloatTensor\n",
    "        @returns : (centers, min_points, max_points) two IntTensors with the center and ranges that each patch occupies\n",
    "        \n",
    "        The returned elements contain the patches from the bigger to the smaller one\n",
    "        \"\"\"\n",
    "        # formula follows this -> see how to implement it in vector operations to make it faster, or precompute it for a fixed image size\n",
    "        # x_min = 0 + patch_width/2\n",
    "        # x_max = full_img.width - patch_width/2\n",
    "        # x_pos = c * (x_max - x_min) == full_img.width - patch_width\n",
    "        # patch = img[x_pos - patch_width/2 : x_pos + patch_width/2] (warning on patch size)\n",
    "        \n",
    "        self._last_centers = crop_centers * self._pdr  # element wise multiplication\n",
    "        self._last_px_mins = self._last_centers - self.half_crop_sizes\n",
    "        self._last_px_maxs = self._last_centers + self.half_crop_sizes\n",
    "        \n",
    "        return self._last_centers, self._last_px_mins, self._last_px_maxs\n",
    "    \n",
    "    def encode(self, x, crop_centers):\n",
    "        full_img = x\n",
    "        ########################\n",
    "        # BEGIN Cropping\n",
    "        ###\n",
    "        # compute the patches positions\n",
    "        centers, min_px, max_px = self.compute_patches(crop_centers)\n",
    "        # make only one vector out of the needed ones\n",
    "        ranges = min_px.cat(max_px)  \n",
    "        # Get all the cropped layers\n",
    "        crops = []\n",
    "        # TODO find out how to do this without passing from GPU to CPU and vice versa\n",
    "        prev_crop = full_img\n",
    "        for pr in ranges:\n",
    "            #  pr == pixel_ranges = [x0,y0,x1,y1]\n",
    "            crop = prev_crop[pr[0]:pr[2],pr[1]:pr[3]]  # crop the input\n",
    "            crops.append(crop)\n",
    "            prev_crop = crop\n",
    "        \n",
    "        # Reverse the list to compute from the fovea to the other dimensions -> I'm not sure if the computation is done in place or not, so starting from the more detailed one\n",
    "        crops = crops[::-1]\n",
    "        # encoded outputs from each resolution layer\n",
    "        codes = []\n",
    "        for i in range(len(crops)):\n",
    "            layer = crops(i)\n",
    "            encs = self.encoders[i]  # encoders for the current resolution\n",
    "            #downsample it as many times as needed (basically the )\n",
    "            downsampler = self.downsamplers[i]\n",
    "            layer = downsampler(layer)  \n",
    "            # apply all the encoders in the corresponding i'th layer\n",
    "            cds = []  # codes of the corresponding layer (at the same resolution)\n",
    "            for enc in encs:\n",
    "                c = enc(layer)\n",
    "                cds.append(c)\n",
    "            codes.append(cds)\n",
    "        return codes \n",
    "    \n",
    "    def decode(self, codes, crop_centers):\n",
    "        #first decode each of the centers\n",
    "        decoded_vec = []\n",
    "        for i in range(len(codes)):\n",
    "            cl = codes[i]\n",
    "            vec = []\n",
    "            for j in range(len(cl)):\n",
    "                c = cl[j]\n",
    "                dec = self.decoders[i][j]\n",
    "                decoded_vec.append(dec(c))\n",
    "        #first merge each resolution independently:\n",
    "        # I will do a simple mean merging instead of doing some learning, this might be good enough\n",
    "        declen = len(decoded_vec)\n",
    "        reslayers = []\n",
    "        for i in range(declen):\n",
    "            declayer = decoded_vec[i]\n",
    "            rl = declayer[0]\n",
    "            try:\n",
    "                for l in declayer[1:]:\n",
    "                    rl = rl + l\n",
    "                rl = rl / len(declayer)\n",
    "            except:\n",
    "                # nothing happened, there was only one image in this layer\n",
    "                pass\n",
    "            reslayers.append(rl)\n",
    "        \n",
    "        #and now recreate the image using also the location information\n",
    "        #this is the difficult part\n",
    "        # I might want here to work on the Overcomplete Partitioned Connected Layers before going on ... this seems hard here\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def forward(self, x, crop_centers=torch.HalfTensor([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])):\n",
    "        \"\"\"\n",
    "        x the input image\n",
    "        crop_centers, a list of centers c where  c in [0:1],\n",
    "        centers go from the larger one to the lower one\n",
    "        \"\"\"\n",
    "        codes = self.encode(x, crop_centers)\n",
    "        # TODO maybe ...\n",
    "        # Create a simple embedding (maybe later work with a multinomial probability distribution)\n",
    "        # The embeddings contain also the crop_centers, the scaling (downsample), \n",
    "        #     the zoom (upsample) and the relative crop sizes to the complete image\n",
    "        out = self.decode(codes, crop_centers)\n",
    "        ###\n",
    "        # I'm in doubt here if I should do the reverse process of the encoding for each encoder and then use the outputs to generate the input, \n",
    "        # or I should create a single composite decoder that handles the entire reconstruction\n",
    "        # First experiment -> single composite decoder ???\n",
    "        # ... ???\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all patches will be square\n",
    "# full size image will be resized to a square image, beacause it's easy\n",
    "\n",
    "class MultiFullCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Group of Convolutional Autoencoders for a single input resolution\n",
    "    The image is treated as monochrome\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels=1, ds_full_image_cae=True, full_image_size=32, full_img_conv_feat=16, full_conv_sizes=(3,5,7)):\n",
    "        super(CAE, self).__init__()\n",
    "        self.channels = channels  # number of channels in the input image\n",
    "        #this will be the parameter given to create the resolution encoder\n",
    "        self.levels = prime_factors[full_image_size].count(2)\n",
    "        self.conv_sizes = conv_sizes  # filter sizes to create for each resolution\n",
    "        self.ds_full_img_cae = ds_full_image_cae  # indicate if create or not the full image downsample conv encoder\n",
    "        self.full_image_size = full_image_size  # image to which to redimension the entire input image (if previous is True)\n",
    "        self.full_img_conv_feat = full_img_conv_feat  # number of convolutional filters to use per layer \n",
    "        self.full_conv_sizes = full_conv_sizes  # sizes of the convolutional filters, one encoder per size\n",
    "        \n",
    "        self.full_encoders = nn.ModuleList()\n",
    "        self.full_decoders = nn.ModuleList()\n",
    "        #separated as functions to be able to later LOAD the encoders instead of creating them each time\n",
    "        self._create_full_encoders()\n",
    "        self._create_full_decoders()\n",
    "        \n",
    "        \n",
    "    def _create_full_encoders(channels=1):\n",
    "        for cs in self.full_conv_sizes:\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEEncoder(width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_encoders.append(enc)\n",
    "        \n",
    "    def _create_full_decoders(channels=1):\n",
    "        for i in range(self.full_conv_sizes):\n",
    "            cs = full_conv_sizes[i]\n",
    "            width = height = self.full_image_size\n",
    "            channels = self.channels  # although I'm thinking in making this monochrome instead to save processing time\n",
    "            enc = CAEDecoder(self.full_encoders[i], width, height, channels, self.levels, cs, self.full_img_conv_feat)\n",
    "            self.full_decoders.append(enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input = downsampled full image converted to monochrome\n",
    "        ########################\n",
    "        # BEGIN Encoding\n",
    "        ###\n",
    "        \n",
    "        # for the moment this full image is computed each time, but in the future this will be \n",
    "        #     done ONLY if the input image changes\n",
    "        #     maybe what we want to work with is only the difference from previous frames -> future when working in dynamic environments\n",
    "        # encoder full downsampled image\n",
    "        # \n",
    "        # join  all encodings into a single vector\n",
    "        # END Encoding\n",
    "        ########################\n",
    "        #BEGIN decoding\n",
    "        return out\n",
    "        \n",
    "    def save_models(self, name, path):\n",
    "        raise NotImplementedError()\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = transforms.Grayscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mtg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           Grayscale\n",
       "\u001b[0;31mString form:\u001b[0m    <torchvision.transforms.transforms.Grayscale object at 0x7fc003022860>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/DeepLearning/venv3/lib/python3.5/site-packages/torchvision/transforms/transforms.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convert image to grayscale.\n",
       "\n",
       "Args:\n",
       "    num_output_channels (int): (1 or 3) number of channels desired for output image\n",
       "\n",
       "Returns:\n",
       "    PIL Image: Grayscale version of the input.\n",
       "    - If num_output_channels == 1 : returned image is single channel\n",
       "    - If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Args:\n",
       "    img (PIL Image): Image to be converted to grayscale.\n",
       "\n",
       "Returns:\n",
       "    PIL Image: Randomly grayscaled image.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResCAE(12,12,3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CAE(\n",
       "  (encoder): CAEEncoder(\n",
       "  )\n",
       "  (decoder): CAEDecoder(\n",
       "    (encoder): CAEEncoder(\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-cfa6d750aa62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#criterion = nn.CrossEntropyLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DeepLearning/venv3/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     35\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     36\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepLearning/venv3/lib/python3.5/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-2xfc-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-2xfc-in_image_{}.png'.format(epoch))\n",
    "    if loss.data[0] < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break\n",
    "\n",
    "\n",
    "model.save_model(\"2x2-2xfc-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
