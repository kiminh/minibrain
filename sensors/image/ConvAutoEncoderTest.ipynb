{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import skimage \n",
    "import math\n",
    "# import io\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "* [Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly Taken from examples here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "# https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16_deconv.py\n",
    "# Other resources\n",
    "# https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/conv_deconv.py\n",
    "# https://github.com/kvfrans/variational-autoencoder\n",
    "# https://github.com/SherlockLiao/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py\n",
    "# https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder = Q(z|X) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, w,h, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEEncoder, self).__init__()\n",
    "        self.indices = []\n",
    "        padding = math.floor(kernel_size/2)\n",
    "        l1_feat = first_feature_count\n",
    "        l2_feat = l1_feat * 2\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l1_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l1_feat, l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l1_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(l1_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "# #             nn.BatchNorm2d(l2_feat),\n",
    "#             nn.ReLU(),\n",
    "            nn.Conv2d(l2_feat, l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.BatchNorm2d(l2_feat),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "        self.conv_dim = int(((w*h)/16) * l2_feat)\n",
    "        #self.conv_dim = int( channels * (w/4) * l2_feat)\n",
    "        self.fc1 = nn.Linear(self.conv_dim, hid_dim)\n",
    "        self.fc2 = nn.Linear(hid_dim, code_dim)\n",
    "#         self.fc1 = nn.Linear(576, hid_dim)\n",
    "\n",
    "    def get_conv_layer_indices(self):\n",
    "        return [0, 2, 5, 7, 10]  # without BatchNorm2d\n",
    "        #return [0, 3, 7, 10, 14]  # with BatchNorm2d\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.indices = []\n",
    "#         print(\"encoding conv l1\")\n",
    "        out, idx  = self.layer1(x)\n",
    "        self.indices.append(idx)\n",
    "#         print(\"encoding conv l2\")\n",
    "        out, idx = self.layer2(out)\n",
    "        self.indices.append(idx)\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"view for  FC l1\")\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         print(out.size())\n",
    "#         print(\"encoding FC1 \")\n",
    "#         out = self.fc1(out)\n",
    "#         print(\"encoding FC2 \")\n",
    "#         out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAEDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder = P(X|z) for the Network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, width, height, channels=3, hid_dim=500, code_dim=200, kernel_size=3, first_feature_count=16):\n",
    "        super(CAEDecoder, self).__init__()\n",
    "        padding = math.floor(kernel_size/2)\n",
    "#         self. width = width\n",
    "#         self.height = height\n",
    "#         self.channels = channels\n",
    "        self.encoder = encoder\n",
    "        self.w_conv_dim = int(width/4)\n",
    "        self.h_conv_dim = int(height/4)\n",
    "        self.l1_feat = first_feature_count\n",
    "        self.l2_feat = self.l1_feat * 2\n",
    "        self.conv_dim = int(((width*height)/16) * self.l2_feat)\n",
    "        #self.conv_dim = int(channels * (width/4) * self.l2_feat)\n",
    "        self.layer1 = torch.nn.Linear(code_dim, hid_dim)\n",
    "        self.layer2 = torch.nn.Linear(hid_dim, self.conv_dim)\n",
    "        self.unpool_1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_1 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l2_feat, self.l2_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l2_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.unpool_2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_layer_2 = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(self.l1_feat, self.l1_feat, kernel_size=kernel_size, padding=padding),\n",
    "#             nn.ReLU(),\n",
    "            nn.ConvTranspose2d(self.l1_feat, channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "#         print(\"decoding l1\")\n",
    "#         out = F.relu(self.layer1(x))\n",
    "#         print(\"decoding l2\")\n",
    "#         out = F.relu(self.layer2(out))\n",
    "#         print(out.size(), self.conv_dim)\n",
    "#         print(\"changing tensor shape to be an image\")\n",
    "#         out = out.view(out.size(0), self.l2_feat, self.w_conv_dim, self.h_conv_dim)\n",
    "        out = self.unpool_1(out, self.encoder.indices[-1])\n",
    "#         print(out.size())\n",
    "#         print(\"decoding c1\")\n",
    "        out = self.deconv_layer_1(out) # FIXME fails here with TypeError: forward() missing 1 required positional argument: 'indices'\n",
    "#         print(\"decoding c2\")\n",
    "        out = self.unpool_2(out, self.encoder.indices[-2])\n",
    "        out = self.deconv_layer_2(out)\n",
    "#         print(\"returning decoder response\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, width, height, channels, hid_dim=500, code_dim=200, conv_layer_feat=16):\n",
    "        super(CAE, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.encoder = CAEEncoder(width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "        self.decoder = CAEDecoder(self.encoder, width, height, channels, hid_dim, code_dim, 3, conv_layer_feat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "        \n",
    "    def save_model(self, name, path):\n",
    "        torch.save(self.encoder, os.path.join(path, \"cae_encoder_\"+name+\".pth\"))\n",
    "        torch.save(self.decoder, os.path.join(path, \"cae_decoder_\"+name+\".pth\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of the operations for the full image autoencoder\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406], # from example here https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "   std=[0.229, 0.224, 0.225]\n",
    "#   mean=[0.5, 0.5, 0.5], # from example here http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "#    std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "#the whole image gets resized to a small image that can be quickly analyzed to get important points\n",
    "def fullimage_preprocess(w=48,h=48):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((w,h)), #this should be used ONLY if the image is bigger than this size\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "#the full resolution fovea just is a small 12x12 patch \n",
    "full_resolution_crop = transforms.Compose([\n",
    "    transforms.RandomCrop(12),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "def downsampleTensor(crop_size, final_size=16):\n",
    "    sample = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.Resize(final_size), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transformation, dataset = datasets.CIFAR100, cuda=True):\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=True, download=True,\n",
    "                       transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset('../data', train=False, transform=transformation),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# num_epochs = 5\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE(12,12,3,500,200,32).cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 12, 12)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = full_resolution_crop\n",
    "train_loader, test_loader = get_loaders(batch_size, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.4539\n",
      "epoch [2/100], loss:0.4631\n",
      "epoch [3/100], loss:0.3747\n",
      "epoch [4/100], loss:0.3498\n",
      "epoch [5/100], loss:0.3100\n",
      "epoch [6/100], loss:0.2568\n",
      "epoch [7/100], loss:0.2502\n",
      "epoch [8/100], loss:0.3316\n",
      "epoch [9/100], loss:0.3610\n",
      "epoch [10/100], loss:0.2967\n",
      "epoch [11/100], loss:0.2269\n",
      "epoch [12/100], loss:0.2684\n",
      "epoch [13/100], loss:0.3130\n",
      "epoch [14/100], loss:0.3207\n",
      "epoch [15/100], loss:0.2146\n",
      "epoch [16/100], loss:0.2892\n",
      "epoch [17/100], loss:0.2699\n",
      "epoch [18/100], loss:0.3031\n",
      "epoch [19/100], loss:0.2612\n",
      "epoch [20/100], loss:0.2221\n",
      "epoch [21/100], loss:0.2603\n",
      "epoch [22/100], loss:0.2754\n",
      "epoch [23/100], loss:0.2542\n",
      "epoch [24/100], loss:0.2577\n",
      "epoch [25/100], loss:0.2154\n",
      "epoch [26/100], loss:0.2441\n",
      "epoch [27/100], loss:0.2863\n",
      "epoch [28/100], loss:0.2250\n",
      "epoch [29/100], loss:0.2591\n",
      "epoch [30/100], loss:0.2663\n",
      "epoch [31/100], loss:0.2410\n",
      "epoch [32/100], loss:0.2465\n",
      "epoch [33/100], loss:0.2696\n",
      "epoch [34/100], loss:0.2555\n",
      "epoch [35/100], loss:0.2792\n",
      "epoch [36/100], loss:0.2057\n",
      "epoch [37/100], loss:0.2775\n",
      "epoch [38/100], loss:0.2958\n",
      "epoch [39/100], loss:0.2641\n",
      "epoch [40/100], loss:0.2446\n",
      "epoch [41/100], loss:0.2386\n",
      "epoch [42/100], loss:0.2634\n",
      "epoch [43/100], loss:0.2733\n",
      "epoch [44/100], loss:0.2900\n",
      "epoch [45/100], loss:0.2530\n",
      "epoch [46/100], loss:0.2101\n",
      "epoch [47/100], loss:0.2416\n",
      "epoch [48/100], loss:0.2906\n",
      "epoch [49/100], loss:0.2754\n",
      "epoch [50/100], loss:0.1793\n",
      "epoch [51/100], loss:0.2391\n",
      "epoch [52/100], loss:0.2573\n",
      "epoch [53/100], loss:0.2087\n",
      "epoch [54/100], loss:0.2294\n",
      "epoch [55/100], loss:0.2772\n",
      "epoch [56/100], loss:0.2257\n",
      "epoch [57/100], loss:0.2330\n",
      "epoch [58/100], loss:0.2378\n",
      "epoch [59/100], loss:0.2567\n",
      "epoch [60/100], loss:0.2430\n",
      "epoch [61/100], loss:0.2412\n",
      "epoch [62/100], loss:0.2903\n",
      "epoch [63/100], loss:0.2047\n",
      "epoch [64/100], loss:0.2507\n",
      "epoch [65/100], loss:0.1983\n",
      "epoch [66/100], loss:0.2549\n",
      "epoch [67/100], loss:0.3053\n",
      "epoch [68/100], loss:0.1809\n",
      "epoch [69/100], loss:0.2997\n",
      "epoch [70/100], loss:0.2102\n",
      "epoch [71/100], loss:0.2040\n",
      "epoch [72/100], loss:0.2288\n",
      "epoch [73/100], loss:0.2720\n",
      "epoch [74/100], loss:0.2650\n",
      "epoch [75/100], loss:0.2545\n",
      "epoch [76/100], loss:0.1715\n",
      "epoch [77/100], loss:0.2577\n",
      "epoch [78/100], loss:0.2608\n",
      "epoch [79/100], loss:0.2321\n",
      "epoch [80/100], loss:0.2491\n",
      "epoch [81/100], loss:0.2729\n",
      "epoch [82/100], loss:0.2644\n",
      "epoch [83/100], loss:0.1553\n",
      "epoch [84/100], loss:0.2080\n",
      "epoch [85/100], loss:0.2886\n",
      "epoch [86/100], loss:0.1798\n",
      "epoch [87/100], loss:0.1990\n",
      "epoch [88/100], loss:0.1914\n",
      "epoch [89/100], loss:0.2138\n",
      "epoch [90/100], loss:0.2598\n",
      "epoch [91/100], loss:0.2524\n",
      "epoch [92/100], loss:0.2505\n",
      "epoch [93/100], loss:0.1890\n",
      "epoch [94/100], loss:0.2159\n",
      "epoch [95/100], loss:0.2712\n",
      "epoch [96/100], loss:0.2005\n",
      "epoch [97/100], loss:0.2234\n",
      "epoch [98/100], loss:0.2300\n",
      "epoch [99/100], loss:0.2058\n",
      "epoch [100/100], loss:0.2056\n",
      "CPU times: user 3min 41s, sys: 17 s, total: 3min 58s\n",
      "Wall time: 6min 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type CAEEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/leo/DeepLearning/venv3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type CAEDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "#         print(\"encoding batch of  images\")\n",
    "        output = model(img)\n",
    "#         print(\"computing loss\")\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "#         print(\"Backward \")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        in_pic = to_img(img.cpu().data)\n",
    "        save_image(pic, './cae_results/2x2-out_image_{}.png'.format(epoch))\n",
    "        save_image(in_pic, './cae_results/2x2-in_image_{}.png'.format(epoch))\n",
    "    if loss.data[0] < 0.15: #arbitrary number because I saw that it works well enough\n",
    "        break\n",
    "\n",
    "\n",
    "model.save_model(\"2x2-layer\", \"CAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results \n",
    "\n",
    "Experiments with the following configurations:\n",
    "- 2 layers with 2 convolutional stages each  <- **best result**\n",
    "- 2 layers with 2 convolutional stages each with batch normalization\n",
    "- 2 layers with 4 convolutional stages each <- **worst result**\n",
    "\n",
    "\n",
    "2 layers with 4 conv stages each does not give the same results as 2 layers with 2 conv stages\n",
    "\n",
    "It not only converges MUCH faster and the models are smaller, but the actually the convergence is much better\n",
    "\n",
    "For bathc normalization happens the same, without batchnorm2d converges faster and model is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
