{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Correcting Codes Encoding Study\n",
    "\n",
    "The goal of this study is to understand options to the popular one-hot encoding. There are many sides to each story (no, not only two), on those sides are: \n",
    "\n",
    "- I never liked one-hot encoding (and is been more than a decade since I first used it, so the disgust might never go out);\n",
    "- I don't like how neural networks are treated and should always be end to end learning (no they should not, they should be more complex architectures, many already in research literature)\n",
    "- There are priors \n",
    "- Each type of input should  (and HAS in nature) its own priors, which are adapted to *facilitate* the learning, no we should not do everyhting inside a NN, we should give as input something that has priors that facilitate learning (and might or might not later save processing power during operations)\n",
    "\n",
    "\n",
    "On the priors, many have already shown good results, the most remarcable prior are: Convolutional Neural Networks, MAC Networks, LSTMs, others are more subtle, like (remember citation here ...) adding a coordinate system to the input image as an (or many) extra channel(s). There are many more that I think are worth exploring and adding to the literature, even if they don't give good results. \n",
    "On those priors there are many that we not only know, but also we have specialized hardware that is perfectly adapted\n",
    "* time and space -> this we can encode and add it as extra channels\n",
    "* Different transforms (Fourier, Laplace, Wavelets, ...)\n",
    "* spikes (borders in images)\n",
    "* ....\n",
    "\n",
    "\n",
    "The idea  behind this is that I don't agree with one-hot encoding, not because it does not work, but because it imposes a few limits that I don't want to deal with at first\n",
    "\n",
    "* We know the actual number of values to encode (with words this is not necessary true)\n",
    "* We have a sample data to train the encoding\n",
    "\n",
    "This limits us in several ways; for example, for training on a domain, the encoder will depend on that domain only. If there are under-represented values (such as words that don't appear or are new later, or changing domain) this limits the encoding possibliities. A better idea will be to be able to encode everything even if the internal representations have not yet learned to use those simbols.\n",
    "\n",
    "I want to be able to do a separation ebtween the *possibility*  of representing a value, and the learning of that concept.\n",
    "\n",
    "The first and biggest limitation of one-hot encoding is that does not allow to represent values that are not accepted.\n",
    "\n",
    "As some other parts of this study have already focused on integer value representations, arbitrary function representation (although with limitted success on the fourier inspired encodings) this study is more focused on being able to represent correctly all the values of utf-8, basically doing a first binary representation that will be given as input to an OVERFITTED encoder. \n",
    "\n",
    "The reasoning behind this is:\n",
    "\n",
    "\n",
    "* The origin domain is all text\n",
    "* UTF-8 can represent all text in all languages including some extra elements\n",
    "* let's use UTF-8 as the origin domain\n",
    "* Create an encoder that can deal with ANY and ALL input in the origin domain\n",
    "* the encoded values can later be used\n",
    "\n",
    "As text should be always correctly reconstructed in the presence of noise, I want to imagine now a Neural Network like a REALLY NOISY channel. For this using (Forward) ECCs is one way of thinking in this medium\n",
    "Then the tests that I intend to do is:\n",
    "\n",
    "* Create an autoencoder that OVERFITS to all the data\n",
    "\n",
    "\n",
    "One idea that I have been dealing with my head for the past 3-4 years is that we are thinking overfitting the wrong way, and we can actually use it well, but we have to learn how.\n",
    "\n",
    "I think that here is the first time I actually find a way of doing it in a useful way\n",
    "\n",
    "The idea is to overfit so it generates an smaller encoding vector than the one in the input. Autoencoders are good to do this test.\n",
    "\n",
    "The other idea is that if the autoencoder can NOT do this, then the encoding ideas that I will try are BAAAD and I should feel BAAAD. In this case ... just go to the drrawing table and think of other things.\n",
    "\n",
    "On the other side, if this works, this means that FINALLY I can go on the next stage, that is building the predictors first basic ones (LSTMs, HMMs, Time Convolutions), then with meta-learning and later with my still too fresh idea on neural databases. \n",
    "\n",
    "One interesting thing I want to find out about Error Correcting Codes (ECCs) is if they are actually useful in the output decoding, as they should be adding *explicit* redundancy to the input and also to the output.\n",
    "\n",
    "The other thing about ECCs is that we can pile them up, for example, one (or many codes) to representa a symbol (for example the value *'â‚¬'* ) and then use convolutional or turbo codes for the *temporal* encoding/decoding part, this means that we not only add priors to the intantaneous input, but also to the temporal dimension, which is something I really want to explore (and this should facilitate fixing and correcting \"channel errors\")\n",
    "\n",
    "I don't deal here with *erasure* error types, but that is a possibility later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
