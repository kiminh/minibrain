{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Correcting Codes Encoding Study\n",
    "\n",
    "The goal of this study is to understand options to the popular one-hot encoding. There are many sides to each story (no, not only two), on those sides are: \n",
    "\n",
    "- I never liked one-hot encoding (and is been more than a decade since I first used it, so the disgust might never go out);\n",
    "- I don't like how neural networks are treated and should always be end to end learning (no they should not, they should be more complex architectures, many already in research literature)\n",
    "- There are priors \n",
    "- Each type of input should  (and HAS in nature) its own priors, which are adapted to *facilitate* the learning, no we should not do everyhting inside a NN, we should give as input something that has priors that facilitate learning (and might or might not later save processing power during operations)\n",
    "\n",
    "\n",
    "On the priors, many have already shown good results, the most remarcable prior are: Convolutional Neural Networks, MAC Networks, LSTMs, others are more subtle, like (remember citation here ...) adding a coordinate system to the input image as an (or many) extra channel(s). There are many more that I think are worth exploring and adding to the literature, even if they don't give good results. \n",
    "On those priors there are many that we not only know, but also we have specialized hardware that is perfectly adapted\n",
    "* time and space -> this we can encode and add it as extra channels\n",
    "* Different transforms (Fourier, Laplace, Wavelets, ...)\n",
    "* spikes (borders in images)\n",
    "* ....\n",
    "\n",
    "\n",
    "The idea  behind this is that I don't agree with one-hot encoding, not because it does not work, but because it imposes a few limits that I don't want to deal with at first\n",
    "\n",
    "* We know the actual number of values to encode (with words this is not necessary true)\n",
    "* We have a sample data to train the encoding\n",
    "\n",
    "This limits us in several ways; for example, for training on a domain, the encoder will depend on that domain only. If there are under-represented values (such as words that don't appear or are new later, or changing domain) this limits the encoding possibliities. A better idea will be to be able to encode everything even if the internal representations have not yet learned to use those simbols.\n",
    "\n",
    "I want to be able to do a separation ebtween the *possibility*  of representing a value, and the learning of that concept.\n",
    "\n",
    "The first and biggest limitation of one-hot encoding is that does not allow to represent values that are not accepted.\n",
    "\n",
    "As some other parts of this study have already focused on integer value representations, arbitrary function representation (although with limitted success on the fourier inspired encodings) this study is more focused on being able to represent correctly all the values of utf-8, basically doing a first binary representation that will be given as input to an OVERFITTED encoder. \n",
    "\n",
    "The reasoning behind this is:\n",
    "\n",
    "\n",
    "* The origin domain is all text\n",
    "* UTF-8 can represent all text in all languages including some extra elements\n",
    "* let's use UTF-8 as the origin domain\n",
    "* Create an encoder that can deal with ANY and ALL input in the origin domain\n",
    "* the encoded values can later be used\n",
    "\n",
    "As text should be always correctly reconstructed in the presence of noise, I want to imagine now a Neural Network like a REALLY NOISY channel. For this using (Forward) ECCs is one way of thinking in this medium\n",
    "Then the tests that I intend to do is:\n",
    "\n",
    "* Create an autoencoder that OVERFITS to all the data\n",
    "\n",
    "\n",
    "One idea that I have been dealing with my head for the past 3-4 years is that we are thinking overfitting the wrong way, and we can actually use it well, but we have to learn how.\n",
    "\n",
    "I think that here is the first time I actually find a way of doing it in a useful way\n",
    "\n",
    "The idea is to overfit so it generates an smaller encoding vector than the one in the input. Autoencoders are good to do this test.\n",
    "\n",
    "The other idea is that if the autoencoder can NOT do this, then the encoding ideas that I will try are BAAAD and I should feel BAAAD. In this case ... just go to the drawing table and think of other things.\n",
    "\n",
    "On the other side, if this works, this means that FINALLY I can go on the next stage, that is building the predictors first basic ones (LSTMs, HMMs, Time Convolutions), then with meta-learning and later with my still too fresh idea on neural databases. \n",
    "\n",
    "One interesting thing I want to find out about Error Correcting Codes (ECCs) is if they are actually useful in the output decoding, as they should be adding *explicit* redundancy to the input and also to the output.\n",
    "\n",
    "The other thing about ECCs is that we can pile them up, for example, one (or many codes) to representa a symbol (for example the value *'€'* ) and then use convolutional or turbo codes for the *temporal* encoding/decoding part, this means that we not only add priors to the intantaneous input, but also to the temporal dimension, which is something I really want to explore (and this should facilitate fixing and correcting \"channel errors\")\n",
    "\n",
    "I don't deal here with *erasure* error types, but that is a possibility later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import commpy\n",
    "# import bitarray as ba\n",
    "# import struct\n",
    "import sys\n",
    "# import binascii\n",
    "from bitstring import BitArray, BitStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.byteorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '€'.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = BitArray(b'\\x00\\x00\\x00\\x00')\n",
    "b = BitArray(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitArray('0xe282ac')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14844588"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(c, byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 - b.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422294"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(c, byteorder='big') >> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range ((32 - b.len)//8):\n",
    "    b.prepend(b'\\x00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitArray('0x00e282ac')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 - b.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried to do some things about the first part of the code, turning bytes to a numpy array, but seems that the most efficient way would be a table (numpy 2d array that has as index the int value of the input and there the array in that position is the binary code, this can already include the first pass to make a one hot of every N bits (maybe every 4, so there are not so many initial values ), this matrix could already have pre-computed the ECC ...\n",
    "\n",
    "For the ECC, I stil don't decide if making it by chunks of input bits, or by all the values, I guess that by all should do, but maybe is easier to compute them reshaping the input arrays to the code in use (example for Golay [24,12,8] will do for every 12 input bits) \n",
    "\n",
    "The idea is to not completely get rid of one-hot encoding, but to limit it to parts of the input vector code restricting the size of the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters for a one-hot by chunks encoding:\n",
    "chunk_sizes = [4, 5, 6, 8, 12]\n",
    "n_params = []\n",
    "for c in chunk_sizes:\n",
    "    n_params.append((c, (32 // c) * 2**c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 128), (5, 192), (6, 320), (8, 1024), (12, 8192)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe for my tests up to chunks of size 6 should be acceptable (I still need to add the next ECC)\n",
    "\n",
    "The next code can be:\n",
    "- Repetition (x3)\n",
    "- Hamming\n",
    "- Golay\n",
    "- Reed Solomon\n",
    "- Latin Square \n",
    "- AN Correcting\n",
    "\n",
    "Here some thoughts about the codes:\n",
    "\n",
    "Repetition: thishas the disadvantage of giving a redundancy that is quite obvious, besides the low power of reconstruction and catastrofic errors, it is obvious that just repeating does not necessarilly adds to a neural network another perspective at the input. Might be worth trying, but for the moment I'm not interested in it.\n",
    "\n",
    "Hamming: it can correct one error (Hamming 7,4), adding 3 out of 4 bits. With an extra bit it can detect up to 2 errors with an extra parity bit.\n",
    "\n",
    "Golay: might serve well enough for my first tests as it ads not too much overhead (duplicates the number of elements) for an interesting error correction (up to 3 bits from each 12, so one fourth).\n",
    "\n",
    "\n",
    "There is one difference in the focus from this analysis and telecomunications (or any other domain with noisy channel), here I'm interested not in the code Rate (ammount of information sent vs ammount of actual bits sent) but in giving as input to the NN some form of non necessary evident redundancy that it could use, and having more ways to correct the output if a single mistake is made during the output extrapolation, I want ot check this part.\n",
    "\n",
    "Thinking a bit more about auto-encoders, it might not be the best idea to start there as it might not give any useful information ... I still have to try some things, might give it a try if it is quick enough to build it once I have the input code.\n",
    "\n",
    "\n",
    "For efficiency what I will do is build from the beginning the encoding table, for the decoding, I stil need to think it thorugh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
