{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHot Sparse Encoding\n",
    "\n",
    "This tests are based on the previous Error Correction Codes. For those I still don't find a way of easy deconding from a neural network, but there should be something there.\n",
    "\n",
    "In this study I make an encoding for UTF-8 that can manage all the possible (valid) codes. \n",
    "\n",
    "The idea is to reduce the input space, being able to encode any existing text with a finite set that should be smaller than current one-hot encoder approaches. This should provide a universal first layer for text encoding.\n",
    "\n",
    "The next layers I have an idea on how to work with them, but for the moment let's make a first one that actually works.\n",
    "\n",
    "\n",
    "The first step is to build an **Overfitted** autoencoder, this is just to validate that the codes are feasible.\n",
    "\n",
    "A couple of sources:\n",
    "\n",
    "- [UTF-8 complete table](https://www.utf8-chartable.de/unicode-utf8-table.pl)\n",
    "- [UTF-8 Wikipedia](https://en.wikipedia.org/wiki/UTF-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from utf8_encoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire utf-8 univers is NOT the entire $2^{32}$ domain, but there are limitations explained in [the utf-8 description](https://en.wikipedia.org/wiki/UTF-8)\n",
    "\n",
    "| Number of bytes | Bits for code point | First code point | Last code point | Byte 1   | Byte 2   | Byte 3   | Byte 4   |\n",
    "|----------------|--------------------|-----------------|----------------|----------|----------|----------|----------|\n",
    "| 1              | 7                  | U+0000          | U+007F         | 0xxxxxxx |          |          |          |\n",
    "| 2              | 11                 | U+0080          | U+07FF         | 110xxxxx | 10xxxxxx |          |          |\n",
    "| 3              | 16                 | U+0800          | U+FFFF         | 1110xxxx | 10xxxxxx | 10xxxxxx |          |\n",
    "| 4              | 21                 | U+10000         | U+10FFFF       | 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx |\n",
    "\n",
    "I'll then compute different table parts and do an append when needed\n",
    "\n",
    "The thing is that the number of elements in the table should be at most $2^{21}$, I need to create a sort of index that can handle the 4 cases.\n",
    "It seems I'll have to create 4 different conversion tables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact ... it seems that I can just chunk the utf-8 value in chunks and do one-hot per different parts:\n",
    "- there are only 4 segment ranges, that can be coded in one-hot also add there either hamming or other ECC\n",
    "- the largest value is for 7 bits -> 128 values\n",
    "- the others contain 6 bits -> 64 values\n",
    "The prefix of each can be taken away and replaced by the initial one-hot\n",
    "\n",
    "So a complete code would be:  $ 4 + 128 + 64 + 64 + 64 = 324 $\n",
    "\n",
    "Instead of having dimension 1,112,064 to encode any utf-8 value.\n",
    "\n",
    "The encoder is  much simpler than I thought for this case, later I can add ECC for each, knowing that there is only one active bit in each row, this makes the task easier.\n",
    "\n",
    "This embedding can stil be reduced but should be sparse enough already to make a good input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + 128 + 64 + 64 + 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters for a one-hot by chunks encoding:\n",
    "chunk_sizes = [4, 5, 6, 8, 12]\n",
    "n_params = []\n",
    "for c in chunk_sizes:\n",
    "    n_params.append((c, (32 // c) * 2**c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 128), (5, 192), (6, 320), (8, 1024), (12, 8192)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "emd = torch.nn.Embedding(2**10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, emd.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/7\n",
    "# counting the number of (trainable) parameters of a pytorch model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is a fully connected layer ... this means a LOT of parameters\n",
    "\n",
    "To be able to do an effective one-hot of all utf-8 would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 55603200\n",
      "100 111206400\n",
      "200 222412800\n",
      "300 333619200\n"
     ]
    }
   ],
   "source": [
    "for i in [50,100,200,300]:\n",
    "    print(i, 1112064 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means I don't want to train that layer ... it would not even fit in my GPU\n",
    "\n",
    "There is another thing, the embedding layer learns from the sample input, this means that it will ignore all values that don't appear or are underrepresented (a know issue). My goal is to deal with this with meta-learning techniques, but always being able to keep adding new inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of codes =  59328\n",
      "number of code_exceptions =  4224\n"
     ]
    }
   ],
   "source": [
    "tables = create_tables(segments=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = create_tables()  # 4 segments by default\n",
    "# if the previous line is executed gives:\n",
    "# number of codes =  1107904\n",
    "# number of code_exceptions =  790656\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"utf8_code_matrix_3seg\", tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(tables[1], \"txt2code_3seg\")\n",
    "save_obj(tables[2], \"code2txt_3seg\")\n",
    "save_obj(tables[3], \"txt2num_3seg\")\n",
    "save_obj(tables[4], \"num2txt_3seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2c = tables[1]\n",
    "c2t = tables[2]\n",
    "n2t = tables[4]\n",
    "t2n = tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59328, 388), 59328, 59328, 59328, 59328)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that all the tables have the right number of codes\n",
    "tables[0].shape, len(t2n.keys()), len(n2t.keys()), len(tables[1].keys()), len(tables[2].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Wikipedia says:\n",
    "\n",
    "'''\n",
    "  UTF-8 is a variable width character encoding capable of encoding all 1,112,064 valid code points in Unicode using one to four 8-bit bytes.\n",
    "'''\n",
    "\n",
    "We have managed to encode only 1107904 codes, so we are missing somehow 4160 codes that python can't encode from bytes. I won't deal with this for the moment, I'll just believe python that it knows how to encode utf-8 (or I should start creating tests and find if python has a bug and create the ticket ... I must stay strong and follow my goal without diverging as I almost have no time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1112064 - 1107904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 + (2**5 * 2**6)+ (2**4 * (2**6)**2) + (2**3 * (2**6)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164864"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**21 + 2**16 + 2**11 + 2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices for the segments:  0 128 2176 65664 2097280\n"
     ]
    }
   ],
   "source": [
    "print(\"indices for the segments: \", 0, 128, (128 + 2**5 * 2**6), (128 + 2**4 * (2**6)**2), (128 + 2**3 * (2**6)**3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from:  https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '\\x00'),\n",
       " (1, '\\x01'),\n",
       " (2, '\\x02'),\n",
       " (3, '\\x03'),\n",
       " (4, '\\x04'),\n",
       " (5, '\\x05'),\n",
       " (6, '\\x06'),\n",
       " (7, '\\x07'),\n",
       " (8, '\\x08'),\n",
       " (9, '\\t'),\n",
       " (10, '\\n'),\n",
       " (11, '\\x0b'),\n",
       " (12, '\\x0c'),\n",
       " (13, '\\r'),\n",
       " (14, '\\x0e'),\n",
       " (15, '\\x0f'),\n",
       " (16, '\\x10'),\n",
       " (17, '\\x11'),\n",
       " (18, '\\x12'),\n",
       " (19, '\\x13'),\n",
       " (20, '\\x14'),\n",
       " (21, '\\x15'),\n",
       " (22, '\\x16'),\n",
       " (23, '\\x17'),\n",
       " (24, '\\x18'),\n",
       " (25, '\\x19'),\n",
       " (26, '\\x1a'),\n",
       " (27, '\\x1b'),\n",
       " (28, '\\x1c'),\n",
       " (29, '\\x1d'),\n",
       " (30, '\\x1e'),\n",
       " (31, '\\x1f'),\n",
       " (32, ' '),\n",
       " (33, '!'),\n",
       " (34, '\"'),\n",
       " (35, '#'),\n",
       " (36, '$'),\n",
       " (37, '%'),\n",
       " (38, '&'),\n",
       " (39, \"'\"),\n",
       " (40, '('),\n",
       " (41, ')'),\n",
       " (42, '*'),\n",
       " (43, '+'),\n",
       " (44, ','),\n",
       " (45, '-'),\n",
       " (46, '.'),\n",
       " (47, '/'),\n",
       " (48, '0'),\n",
       " (49, '1'),\n",
       " (50, '2'),\n",
       " (51, '3'),\n",
       " (52, '4'),\n",
       " (53, '5'),\n",
       " (54, '6'),\n",
       " (55, '7'),\n",
       " (56, '8'),\n",
       " (57, '9'),\n",
       " (58, ':'),\n",
       " (59, ';'),\n",
       " (60, '<'),\n",
       " (61, '='),\n",
       " (62, '>'),\n",
       " (63, '?'),\n",
       " (64, '@'),\n",
       " (65, 'A'),\n",
       " (66, 'B'),\n",
       " (67, 'C'),\n",
       " (68, 'D'),\n",
       " (69, 'E'),\n",
       " (70, 'F'),\n",
       " (71, 'G'),\n",
       " (72, 'H'),\n",
       " (73, 'I'),\n",
       " (74, 'J'),\n",
       " (75, 'K'),\n",
       " (76, 'L'),\n",
       " (77, 'M'),\n",
       " (78, 'N'),\n",
       " (79, 'O'),\n",
       " (80, 'P'),\n",
       " (81, 'Q'),\n",
       " (82, 'R'),\n",
       " (83, 'S'),\n",
       " (84, 'T'),\n",
       " (85, 'U'),\n",
       " (86, 'V'),\n",
       " (87, 'W'),\n",
       " (88, 'X'),\n",
       " (89, 'Y'),\n",
       " (90, 'Z'),\n",
       " (91, '['),\n",
       " (92, '\\\\'),\n",
       " (93, ']'),\n",
       " (94, '^'),\n",
       " (95, '_'),\n",
       " (96, '`'),\n",
       " (97, 'a'),\n",
       " (98, 'b'),\n",
       " (99, 'c')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(100, n2t.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2n['\\x09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(take(10, t2c.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = torch.from_numpy(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-to-convert-a-dense-matrix-to-a-sparse-one/7809\n",
    "\n",
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scodes = to_sparse(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scodes.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.sparse.DoubleTensor"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch sparse can't be saved yet ... not implemented for the moment (I should do it myself and send the patch)\n",
    "# torch.save(scodes, \"utf8-codes.pt\")\n",
    "# save_obj(scodes, \"utf8-codes.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spcodes = sp.sparse.coo_matrix(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(spcodes, \"utf8-codes-scipy-sparse_3seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the moment we have the posibility to encode all utf-8 characters, but is still a bit big in size when having the complete. But I'll try to cut the use of memory because 6.8GB for the \"dense\" matrix reprsentation is too much. In Sparse mode matrix is only 83MB for the entire dataset. Nevertheless there are many characters that I will not be using for the first tests, so having it use only a part will (should) be enough.\n",
    "\n",
    "So I'll see how big the encoder is without the 4 segments, but only using the first 3 (this should be enough for most applications) so we can encode \n",
    "\n",
    "number of codes =  59328\n",
    "\n",
    "number of code_exceptions =  4224\n",
    "\n",
    "the entire code is now 206MB on a file on disk in non sparse mode and 3.6MB on sparse mode on disk for codes with 4 segments (this mode is scalable to later add the rest of the code without the need of redoing the architecture)\n",
    "\n",
    "But also reducing the number of bytes on the code (using only 3 bytes max instead of 4) by not taking the last one that anyways we are not using for this application we can reduce this to 177MB of the complete \"dense\" code on disk and 3.6MB on sparse mode.\n",
    "\n",
    "I would not recomend doing this all the time as it restricts the power of the input network to only known elements (and we want to do with all the possible codes) but for my tests this reduces the usage of memory, the amount of parameters and the processing time.\n",
    "\n",
    "When using 4 segments there are 452 elements per code, when using 3 there are 388\n",
    "\n",
    "So I can start playing with it without worrying about memory ;)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584070796460177"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
