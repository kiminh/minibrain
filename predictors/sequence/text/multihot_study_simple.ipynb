{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHot Sparse Encoding\n",
    "\n",
    "This tests are based on the previous Error Correction Codes. For those I still don't find a way of easy deconding from a neural network, but there should be something there.\n",
    "\n",
    "In this study I make an encoding for UTF-8 that can manage all the possible (valid) codes. \n",
    "\n",
    "The idea is to reduce the input space, being able to encode any existing text with a finite set that should be smaller than current one-hot encoder approaches. This should provide a universal first layer for text encoding.\n",
    "\n",
    "The next layers I have an idea on how to work with them, but for the moment let's make a first one that actually works.\n",
    "\n",
    "\n",
    "The first step is to build an **Overfitted** autoencoder, this is just to validate that the codes are feasible.\n",
    "\n",
    "A couple of sources:\n",
    "\n",
    "- [UTF-8 complete table](https://www.utf8-chartable.de/unicode-utf8-table.pl)\n",
    "- [UTF-8 Wikipedia](https://en.wikipedia.org/wiki/UTF-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from utf8_encoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire utf-8 univers is NOT the entire $2^{32}$ domain, but there are limitations explained in [the utf-8 description](https://en.wikipedia.org/wiki/UTF-8)\n",
    "\n",
    "| Number of bytes | Bits for code point | First code point | Last code point | Byte 1   | Byte 2   | Byte 3   | Byte 4   |\n",
    "|----------------|--------------------|-----------------|----------------|----------|----------|----------|----------|\n",
    "| 1              | 7                  | U+0000          | U+007F         | 0xxxxxxx |          |          |          |\n",
    "| 2              | 11                 | U+0080          | U+07FF         | 110xxxxx | 10xxxxxx |          |          |\n",
    "| 3              | 16                 | U+0800          | U+FFFF         | 1110xxxx | 10xxxxxx | 10xxxxxx |          |\n",
    "| 4              | 21                 | U+10000         | U+10FFFF       | 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx |\n",
    "\n",
    "I'll then compute different table parts and do an append when needed\n",
    "\n",
    "The thing is that the number of elements in the table should be at most $2^{21}$, I need to create a sort of index that can handle the 4 cases.\n",
    "It seems I'll have to create 4 different conversion tables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact ... it seems that I can just chunk the utf-8 value in chunks and do one-hot per different parts:\n",
    "- there are only 4 segment ranges, that can be coded in one-hot also add there either hamming or other ECC\n",
    "- the largest value is for 7 bits -> 128 values\n",
    "- the others contain 6 bits -> 64 values\n",
    "The prefix of each can be taken away and replaced by the initial one-hot\n",
    "\n",
    "So a complete code would be:  $ 4 + 128 + 64 + 64 + 64 = 324 $\n",
    "\n",
    "Instead of having dimension 1,112,064 to encode any utf-8 value.\n",
    "\n",
    "The encoder is  much simpler than I thought for this case, later I can add ECC for each, knowing that there is only one active bit in each row, this makes the task easier.\n",
    "\n",
    "This embedding can stil be reduced but should be sparse enough already to make a good input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + 128 + 64 + 64 + 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters for a one-hot by chunks encoding:\n",
    "chunk_sizes = [4, 5, 6, 8, 12]\n",
    "n_params = []\n",
    "for c in chunk_sizes:\n",
    "    n_params.append((c, (32 // c) * 2**c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 128), (5, 192), (6, 320), (8, 1024), (12, 8192)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "emd = torch.nn.Embedding(2**10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, emd.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/7\n",
    "# counting the number of (trainable) parameters of a pytorch model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is a fully connected layer ... this means a LOT of parameters\n",
    "\n",
    "To be able to do an effective one-hot of all utf-8 would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 55603200\n",
      "100 111206400\n",
      "200 222412800\n",
      "300 333619200\n"
     ]
    }
   ],
   "source": [
    "for i in [50,100,200,300]:\n",
    "    print(i, 1112064 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means I don't want to train that layer ... it would not even fit in my GPU\n",
    "\n",
    "There is another thing, the embedding layer learns from the sample input, this means that it will ignore all values that don't appear or are underrepresented (a know issue). My goal is to deal with this with meta-learning techniques, but always being able to keep adding new inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 192 128\n",
      "0 1 192 129\n",
      "0 2 192 130\n",
      "0 3 192 131\n",
      "0 4 192 132\n",
      "0 5 192 133\n",
      "0 6 192 134\n",
      "0 7 192 135\n",
      "0 8 192 136\n",
      "0 9 192 137\n",
      "0 10 192 138\n",
      "0 11 192 139\n",
      "0 12 192 140\n",
      "0 13 192 141\n",
      "0 14 192 142\n",
      "0 15 192 143\n",
      "0 16 192 144\n",
      "0 17 192 145\n",
      "0 18 192 146\n",
      "0 19 192 147\n",
      "0 20 192 148\n",
      "0 21 192 149\n",
      "0 22 192 150\n",
      "0 23 192 151\n",
      "0 24 192 152\n",
      "0 25 192 153\n",
      "0 26 192 154\n",
      "0 27 192 155\n",
      "0 28 192 156\n",
      "0 29 192 157\n",
      "0 30 192 158\n",
      "0 31 192 159\n",
      "0 32 192 160\n",
      "0 33 192 161\n",
      "0 34 192 162\n",
      "0 35 192 163\n",
      "0 36 192 164\n",
      "0 37 192 165\n",
      "0 38 192 166\n",
      "0 39 192 167\n",
      "0 40 192 168\n",
      "0 41 192 169\n",
      "0 42 192 170\n",
      "0 43 192 171\n",
      "0 44 192 172\n",
      "0 45 192 173\n",
      "0 46 192 174\n",
      "0 47 192 175\n",
      "0 48 192 176\n",
      "0 49 192 177\n",
      "0 50 192 178\n",
      "0 51 192 179\n",
      "0 52 192 180\n",
      "0 53 192 181\n",
      "0 54 192 182\n",
      "0 55 192 183\n",
      "0 56 192 184\n",
      "0 57 192 185\n",
      "0 58 192 186\n",
      "0 59 192 187\n",
      "0 60 192 188\n",
      "0 61 192 189\n",
      "0 62 192 190\n",
      "0 63 192 191\n",
      "1 0 193 128\n",
      "1 1 193 129\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc1 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ab5d8808d6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mctables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/minibrain/predictors/sequence/text/utf8_encoder.py\u001b[0m in \u001b[0;36mcreate_tables\u001b[0;34m(segments)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mexcept_count\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/minibrain/predictors/sequence/text/utf8_encoder.py\u001b[0m in \u001b[0;36mcreate_tables\u001b[0;34m(segments)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# index =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mor_mask2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mor_mask2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[0;31m# index = index_offset_2 + i*j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mappend_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "tables = create_tables(segments=3)\n",
    "# ctables = add_mappings(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = create_tables()  # 4 segments by default\n",
    "# if the previous line is executed gives:\n",
    "# number of codes =  1107904\n",
    "# number of code_exceptions =  790656\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"utf8_code_matrix_3seg\", tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(tables[1], \"txt2code_3seg\")\n",
    "save_obj(tables[2], \"code2txt_3seg\")\n",
    "save_obj(tables[3], \"txt2num_3seg\")\n",
    "save_obj(tables[4], \"num2txt_3seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2c = tables[1]\n",
    "c2t = tables[2]\n",
    "n2t = tables[4]\n",
    "t2n = tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take(50, c2t.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all the tables have the right number of codes\n",
    "tables[0].shape, len(t2n.keys()), len(n2t.keys()), len(t2c.keys()), len(c2t.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the text2code and text to num are bigger than the backward reference, this is because of the added mappings. \n",
    "the added mappings are for the special codes I count on using the first 20 elements of the utf-8 0000 -> 0020 which are special indicators as NUL, SOH, STX, ETX, EOT, ACK, ....\n",
    "\n",
    "Mappings:\n",
    "- *\\<start\\>* -> STX = 0002 Start of Transaction\n",
    "- *\\<end\\>* -> ETX = 0003 End of Transaction\n",
    "- *\\<unknown\\>* = *\\<unk\\>* -> NAK = 0015\n",
    "\n",
    "The reverse mappings go back only to the new name of the elements so that is why the difference is (and must be) the number of special mappings added = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Wikipedia says:\n",
    "\n",
    "'''\n",
    "  UTF-8 is a variable width character encoding capable of encoding all 1,112,064 valid code points in Unicode using one to four 8-bit bytes.\n",
    "'''\n",
    "\n",
    "We have managed to encode only 1107904 codes, so we are missing somehow 4160 codes that python can't encode from bytes. I won't deal with this for the moment, I'll just believe python that it knows how to encode utf-8 (or I should start creating tests and find if python has a bug and create the ticket ... I must stay strong and follow my goal without diverging as I almost have no time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1112064 - 1107904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 + (2**5 * 2**6)+ (2**4 * (2**6)**2) + (2**3 * (2**6)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**21 + 2**16 + 2**11 + 2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"indices for the segments: \", 0, 128, (128 + 2**5 * 2**6), (128 + 2**4 * (2**6)**2), (128 + 2**3 * (2**6)**3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from:  https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take(50, n2t.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2n['\\x09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(take(10, t2c.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = torch.from_numpy(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-to-convert-a-dense-matrix-to-a-sparse-one/7809\n",
    "\n",
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scodes = to_sparse(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scodes.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch sparse can't be saved yet ... not implemented for the moment (I should do it myself and send the patch)\n",
    "# torch.save(scodes, \"utf8-codes.pt\")\n",
    "# save_obj(scodes, \"utf8-codes.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spcodes = sp.sparse.coo_matrix(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(spcodes, \"utf8-codes-scipy-sparse_3seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the moment we have the posibility to encode all utf-8 characters, but is still a bit big in size when having the complete. But I'll try to cut the use of memory because 6.8GB for the \"dense\" matrix reprsentation is too much. In Sparse mode matrix is only 83MB for the entire dataset. Nevertheless there are many characters that I will not be using for the first tests, so having it use only a part will (should) be enough.\n",
    "\n",
    "So I'll see how big the encoder is without the 4 segments, but only using the first 3 (this should be enough for most applications) so we can encode \n",
    "\n",
    "number of codes =  59328\n",
    "\n",
    "number of code_exceptions =  4224\n",
    "\n",
    "the entire code is now 206MB on a file on disk in non sparse mode and 3.6MB on sparse mode on disk for codes with 4 segments (this mode is scalable to later add the rest of the code without the need of redoing the architecture)\n",
    "\n",
    "But also reducing the number of bytes on the code (using only 3 bytes max instead of 4) by not taking the last one that anyways we are not using for this application we can reduce this to 177MB of the complete \"dense\" code on disk and 3.6MB on sparse mode.\n",
    "\n",
    "I would not recomend doing this all the time as it restricts the power of the input network to only known elements (and we want to do with all the possible codes) but for my tests this reduces the usage of memory, the amount of parameters and the processing time.\n",
    "\n",
    "When using 4 segments there are 452 elements per code, when using 3 there are 388\n",
    "\n",
    "So I can start playing with it without worrying about memory ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "I have now cut the size of the embedding by 128, this is taking out redundancy from the first segment. \n",
    "\n",
    "Also, for the special codes I count on using the first 20 elements of the utf-8 0000 -> 0020 which are special indicators as NUL, SOH, STX, ETX, EOT, ACK, ....\n",
    "\n",
    "The mapping will be:\n",
    "- *\\<start\\>* -> STX = 0002 Start of Transaction\n",
    "- *\\<end\\>* -> ETX = 0003 End of Transaction\n",
    "- *\\<unknown\\>* = *\\<unk\\>* -> NAK = 0015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"utf8_code_matrix_3seg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.from_numpy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudad = d.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the autoencoder now, this is just to see if it works before going on with a more complex setup\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from utf8_encoder import *\n",
    "from utf8vae import *\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, vector_size, channels=1):\n",
    "#     print(\"x shape = \", x.shape, recon_x.shape)\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.nll_loss(recon_x, x)\n",
    "    BCE = F.mse_loss(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_overfit():\n",
    "    # generate dataset inputs (basically the same as the encoding)\n",
    "    # We are going to overfit\n",
    "    epochs = 100\n",
    "    segments = 3  # I do with 3 as it will be much MUCH faster and smaller for my resources than 4 segments\n",
    "    in_size = 388-128  # 3 segments\n",
    "    hidd_size = 100\n",
    "    code_size = 50\n",
    "    device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vector_size = code_size\n",
    "    channels = 1\n",
    "    batch_size = 10\n",
    "    datafile = \"utf8_code_matrix_3seg.npy\"\n",
    "    log_interval = 10\n",
    "    \n",
    "    model = UTF8VAE(in_size, hidd_size, code_size, segments=segments)\n",
    "    # loader = DataLoader(UTF8Dataset(\"utf8_code_matrix_3seg.npy\"), batch_size=batch_size)\n",
    "\n",
    "    name = \"utf8-vae-3segments-overfit\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    n_batches = 10\n",
    "    n_epocs = 100\n",
    "    # train_loader, test_loader = get_loaders(batch_size, transformation)\n",
    "    # we are overfitting, so train and test is the same thing.\n",
    "\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         train(model, optimizer, loss_function, loader,epoch, vector_size, channels)\n",
    "#         test(model, loader, epoch, vector_size, channels)\n",
    "    data = torch.from_numpy(np.load(datafile)).float()\n",
    "    data = data#.to(device)\n",
    "    model = model#.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epocs):\n",
    "        train_loss = 0\n",
    "        for batch_idx in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar, vector_size, channels)\n",
    "            loss.backward()\n",
    "            train_loss += loss.data.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(data),\n",
    "                    100. * batch_idx / len(data),\n",
    "                    loss.data.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(data)))\n",
    "\n",
    "\n",
    "    model.save_model(name, \"saved_models\")\n",
    "    # train_all(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_overfit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
