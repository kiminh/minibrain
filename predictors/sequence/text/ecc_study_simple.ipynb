{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Correcting Codes Encoding Study\n",
    "\n",
    "The goal of this study is to understand options to the popular one-hot encoding. There are many sides to each story (no, not only two), on those sides are: \n",
    "\n",
    "- I never liked one-hot encoding (and is been more than a decade since I first used it, so the disgust might never go out);\n",
    "- I don't like how neural networks are treated and should always be end to end learning (no they should not, they should be more complex architectures, many already in research literature)\n",
    "- There are priors \n",
    "- Each type of input should  (and HAS in nature) its own priors, which are adapted to *facilitate* the learning, no we should not do everyhting inside a NN, we should give as input something that has priors that facilitate learning (and might or might not later save processing power during operations)\n",
    "\n",
    "\n",
    "On the priors, many have already shown good results, the most remarcable prior are: Convolutional Neural Networks, MAC Networks, LSTMs, others are more subtle, like (remember citation here ...) adding a coordinate system to the input image as an (or many) extra channel(s). There are many more that I think are worth exploring and adding to the literature, even if they don't give good results. \n",
    "On those priors there are many that we not only know, but also we have specialized hardware that is perfectly adapted\n",
    "* time and space -> this we can encode and add it as extra channels\n",
    "* Different transforms (Fourier, Laplace, Wavelets, ...)\n",
    "* spikes (borders in images)\n",
    "* ....\n",
    "\n",
    "There is another idea behind the ECCs, is that we usually can 'feel' that something is not right or missing, what about giving NNs a way of having an extra element that would allow for this 'feeling'? \n",
    "\n",
    "The idea behind this is that I don't agree with one-hot encoding the way is used today, not because it does not work, but because it imposes a few limits that I don't want to deal with at first\n",
    "\n",
    "* We know the actual number of values to encode (with words this is not necessary true)\n",
    "* We have a sample data to train the encoding\n",
    "\n",
    "This limits us in several ways; for example, for training on a domain, the encoder will depend on that domain only. If there are under-represented values (such as words that don't appear or are new later, or changing domain) this limits the encoding possibliities. A better idea will be to be able to encode everything even if the internal representations have not yet learned to use those simbols.\n",
    "\n",
    "I want to be able to do a separation ebtween the *possibility*  of representing a value, and the learning of that concept.\n",
    "\n",
    "The first and biggest limitation of one-hot encoding is that does not allow to represent values that are not accepted.\n",
    "\n",
    "As some other parts of this study have already focused on integer value representations, arbitrary function representation (although with limitted success on the fourier inspired encodings) this study is more focused on being able to represent correctly all the values of utf-8, basically doing a first binary representation that will be given as input to an OVERFITTED encoder. \n",
    "\n",
    "The reasoning behind this is:\n",
    "\n",
    "\n",
    "* The origin domain is all text\n",
    "* UTF-8 can represent all text in all languages including some extra elements\n",
    "* let's use UTF-8 as the origin domain\n",
    "* Create an encoder that can deal with ANY and ALL input in the origin domain\n",
    "* the encoded values can later be used\n",
    "\n",
    "As text should be always correctly reconstructed in the presence of noise, I want to imagine now a Neural Network like a REALLY NOISY channel. For this using (Forward) ECCs is one way of thinking in this medium\n",
    "Then the tests that I intend to do is:\n",
    "\n",
    "* Create an autoencoder that OVERFITS to all the data\n",
    "\n",
    "\n",
    "One idea that I have been dealing with my head for the past 3-4 years is that we are thinking overfitting the wrong way, and we can actually use it well, but we have to learn how.\n",
    "\n",
    "I think that here is the first time I actually find a way of doing it in a useful way\n",
    "\n",
    "The idea is to overfit so it generates an smaller encoding vector than the one in the input. Autoencoders are good to do this test.\n",
    "\n",
    "The other idea is that if the autoencoder can NOT do this, then the encoding ideas that I will try are BAAAD and I should feel BAAAD. In this case ... just go to the drawing table and think of other things.\n",
    "\n",
    "On the other side, if this works, this means that FINALLY I can go on the next stage, that is building the predictors first basic ones (LSTMs, HMMs, Time Convolutions), then with meta-learning and later with my still too fresh idea on neural databases. \n",
    "\n",
    "One interesting thing I want to find out about Error Correcting Codes (ECCs) is if they are actually useful in the output decoding, as they should be adding *explicit* redundancy to the input and also to the output.\n",
    "\n",
    "The other thing about ECCs is that we can pile them up, for example, one (or many codes) to representa a symbol (for example the value *'€'* ) and then use convolutional or turbo codes for the *temporal* encoding/decoding part, this means that we not only add priors to the intantaneous input, but also to the temporal dimension, which is something I really want to explore (and this should facilitate fixing and correcting \"channel errors\")\n",
    "\n",
    "I don't deal here with *erasure* error types, but that is a possibility later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import commpy\n",
    "import bitarray as ba\n",
    "import struct\n",
    "import sys\n",
    "import pickle\n",
    "# import binascii\n",
    "from bitstring import BitArray, BitStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.byteorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '€'.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a'.encode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bytearray('a'.encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = BitArray(b'\\x00\\x00\\x00\\x00')\n",
    "b = BitArray(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int.from_bytes(c, byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32 - b.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int.from_bytes(c, byteorder='big') >> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range ((32 - b.len)//8):\n",
    "    b.prepend(b'\\x00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32 - b.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 256\n",
    "a.bit_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'€'.encode()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit of a whacky hack and for sure not the most efficient one, but it just works for what I want\n",
    "\n",
    "def prepend_zeros(s, n):\n",
    "    return '0'* (n - len(s))+s\n",
    "\n",
    "def get_strbintable(n):\n",
    "    bl = n.bit_length() - 1  # because n is actually never used\n",
    "    lines = [ ' '.join(i for i in prepend_zeros(\"{0:b}\".format(l), bl)) for l in range(n)]\n",
    "    return lines\n",
    "\n",
    "def get_npbintable(n):\n",
    "    bins = np.fromstring('\\n'.join(get_strbintable(n)), dtype='int32', sep=' ')\n",
    "    bins = bins.reshape([n, -1])\n",
    "    return bins\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire utf-8 univers is NOT the entire $2^{32}$ domain, but there are limitations explained in [the utf-8 description](https://en.wikipedia.org/wiki/UTF-8)\n",
    "\n",
    "| Number of bytes | Bits for code point | First code point | Last code point | Byte 1   | Byte 2   | Byte 3   | Byte 4   |\n",
    "|----------------|--------------------|-----------------|----------------|----------|----------|----------|----------|\n",
    "| 1              | 7                  | U+0000          | U+007F         | 0xxxxxxx |          |          |          |\n",
    "| 2              | 11                 | U+0080          | U+07FF         | 110xxxxx | 10xxxxxx |          |          |\n",
    "| 3              | 16                 | U+0800          | U+FFFF         | 1110xxxx | 10xxxxxx | 10xxxxxx |          |\n",
    "| 4              | 21                 | U+10000         | U+10FFFF       | 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx |\n",
    "\n",
    "I'll then compute different table parts and do an append when needed\n",
    "\n",
    "The thing is that the number of elements in the table should be at most $2^{21}$, I need to create a sort of index that can handle the 4 cases.\n",
    "It seems I'll have to create 4 different conversion tables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part makes sure to encode as bin\n",
    "eye4 = np.eye(4)\n",
    "eye64 = np.eye(64)\n",
    "eye256 = np.eye(256)\n",
    "\n",
    "# code for 7 bits, Byte 1 of utf-8\n",
    "code_b7 = np.append(np.zeros([2**7, 1]), get_npbintable(2**7), axis=1)\n",
    "\n",
    "# code for 6 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b6 = np.append(np.append(np.ones([2**6, 1]), np.zeros([2**6, 1]), axis=1), \n",
    "                    get_npbintable(2**6), axis=1)\n",
    "\n",
    "# code for 5 bits, Byte 1 of \n",
    "code_b5 = np.append(np.append(np.ones([2**5, 2]), np.zeros([2**5, 1]), axis=1), \n",
    "                    get_npbintable(2**5), axis=1)\n",
    "\n",
    "# code for 4 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b4 = np.append(np.append(np.ones([2**4, 3]), np.zeros([2**4, 1]), axis=1), \n",
    "                    get_npbintable(2**4), axis=1)\n",
    "\n",
    "# code for 3 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b3 = np.append(np.append(np.ones([2**3, 4]), np.zeros([2**3, 1]), axis=1),\n",
    "                    get_npbintable(2**3), axis=1)\n",
    "\n",
    "def encode_utf8(l):\n",
    "    el = l.encode()\n",
    "    code = np.zeros(36)  # 32 is the size of the input code + 4 of the extra redundancy\n",
    "    nbytes = len(el)\n",
    "    # assert( 0<nbytes && nbytes<=4)\n",
    "    assert(nbytes<=4)\n",
    "    bin4 = eye4[nbytes-1]  # this adds redundant knowledge about the  part\n",
    "    # this is ugly but explicit, for the moment is good enough and I can see what is\n",
    "    code[:4] = bin4\n",
    "    if nbytes == 1:\n",
    "        code[4:12] = code_b7[el[0]& 0b01111111 ]\n",
    "    elif nbytes == 2:\n",
    "        code[4:12] = code_b5[el[0] & 0b00011111 ]\n",
    "        code[12:20] = code_b6[el[1] & 0b00111111]\n",
    "    elif nbytes == 3:\n",
    "        code[4:12] = code_b4[el[0] & 0b00001111]\n",
    "        code[12:20] = code_b6[el[1] & 0b00111111]\n",
    "        code[20:28] = code_b6[el[2] & 0b00111111]\n",
    "    elif nbytes == 4:\n",
    "        code[4:12] = code_b3[el[0] & 0b00000111]\n",
    "        code[12:20] = code_b6[el[1] & 0b00111111]\n",
    "        code[20:28] = code_b6[el[2] & 0b00111111]\n",
    "        code[28:36] = code_b6[el[3] & 0b00111111]\n",
    "    else:\n",
    "        raise Exception(\"Bad input, input has to have 1 to 4 input bytes\")\n",
    "    return code\n",
    "\n",
    "\n",
    "# TODO I need to find a more efficient way of doing this that could make this as vector or matrix operations instead\n",
    "def encode_utf8_multihot(c):\n",
    "    e_c = list(c.encode())\n",
    "#     code = np.zeros(36)  # 32 is the size of the input code + 4 of the extra redundancy\n",
    "    nbytes = len(e_c)\n",
    "    # assert( 0<nbytes && nbytes<=4)\n",
    "    assert(nbytes<=4)\n",
    "    bin4 = eye4[nbytes-1]  # this adds redundant knowledge about the  part\n",
    "    # this is ugly but explicit, for the moment is good enough and I can see what is\n",
    "#     code[:4] = bin4\n",
    "    # max size of each part of the code\n",
    "    # I will treat the first byte as always 8 bits, this will make it easier to decode later and adds aditional information\n",
    "    # this has an extra benefit, when a code is there only certain regions will become 1 giving an extra hint to the network\n",
    "    # maxsizes = [2**8, 2**6, 2**6, 2**6]\n",
    "    code = np.zeros(4 + (2**8) + 3*(2**6))\n",
    "    masks = [0xff, 0x3f, 0x3f, 0x3f]\n",
    "    indices = [256+4, 64+256+4, 2*64 + 256+4, 3*64 + 256+4]\n",
    "    maxsizes = [eye256, eye64, eye64, eye64]\n",
    "    code[:4] = bin4\n",
    "    prev_i = 4\n",
    "    for i,n,e,m in zip(indices[:nbytes], e_c, maxsizes[:nbytes], masks[:nbytes]):\n",
    "        code[prev_i:i] = e[n & m]  #masking\n",
    "        prev_i = i\n",
    "    return code\n",
    "\n",
    "def encode_utf8_ecc(l):\n",
    "    # TODO ...\n",
    "    el = l.encode()\n",
    "    code = np.zeros(36)  # 32 is the size of the input code + 4 of the extra redundancy\n",
    "    nbytes = len(el)\n",
    "    # assert( 0<nbytes && nbytes<=4)\n",
    "    assert(nbytes<=4)\n",
    "    bin4 = eye4[nbytes-1]  # this adds redundant knowledge about the  part\n",
    "    # this is ugly but explicit, for the moment is good enough and I can see what is\n",
    "    raise NotImplementedError(\"not implemented yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([1,2,3,4], (1,2,3,4), (1,2,3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = '€'.encode()\n",
    "'{0:b}  {1:b}  {2:b}'.format(el[0], el[1], el[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_utf8('€')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = 'á'.encode()\n",
    "'{0:b}  {0:b}'.format(el[0], el[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_utf8('á')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_utf8_multihot('€').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [97,0,0,0]\n",
    "l2 = [98,0,0,0]\n",
    "l3 = [99,0,0,0]\n",
    "l4 = [254,200,210,210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(bytes([0x20]),'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {bytes([1,2,3,4]): 'lala '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**21 # this should be enough to make the entire utf-8 encoding ... and much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tt10 = get_npbintable(2**21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt10[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense binary input codes\n",
    "\n",
    "# code for 7 bits, Byte 1 of utf-8\n",
    "code_b7 = get_npbintable(2**7)\n",
    "t_zeros = np.zeros([2**7, 1])\n",
    "code_b7 = np.append(t_zeros, code_b7, axis=1)\n",
    "\n",
    "# code for 6 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b6 = get_npbintable(2**6)\n",
    "t_b6 = np.append(np.ones([2**6, 1]), np.zeros([2**6, 1]), axis=1)\n",
    "code_b6 = np.append(t_b6, code_b6, axis=1)\n",
    "\n",
    "# code for 5 bits, Byte 1 of \n",
    "code_b5 = get_npbintable(2**5)\n",
    "t_b5 = np.append(np.ones([2**5, 2]), np.zeros([2**5, 1]), axis=1)\n",
    "code_b5 = np.append(t_b5, code_b5, axis=1)\n",
    "\n",
    "# code for 6 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b4 = get_npbintable(2**4)\n",
    "t_b4 = np.append(np.ones([2**4, 3]), np.zeros([2**4, 1]), axis=1)\n",
    "code_b4 = np.append(t_b4, code_b4, axis=1)\n",
    "\n",
    "# code for 6 bits, Byte 2 to 4 of utf-8 -> this is going to be used later for all the other values\n",
    "code_b3 = get_npbintable(2**3)\n",
    "t_b3 = np.append(np.ones([2**3, 4]), np.zeros([2**3, 1]), axis=1)\n",
    "code_b3 = np.append(t_b3, code_b3, axis=1)\n",
    "\n",
    "# 4 bits\n",
    "b4 = get_npbintable(2**4)\n",
    "eye4 = np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact ... it seems that I can just chunk the utf-8 value in chunks and do one-hot per different parts:\n",
    "- there are only 4 segment ranges, that can be coded in one-hot also add there either hamming or other ECC\n",
    "- the largest value is for 7 bits -> 128 values\n",
    "- the others contain 6 bits -> 64 values\n",
    "The prefix of each can be taken away and replaced by the initial one-hot\n",
    "\n",
    "So a complete code would be\n",
    "$ 4 + 128 + 64 + 64 + 64 = 324 $\n",
    "\n",
    "plus the ECC parity bits\n",
    "\n",
    "Instead of having dimension 1,112,064 to encode any utf-8 value.\n",
    "\n",
    "The encoder is  much simpler than I thought for this case, later I can add ECC for each, knowing that there is only one active bit in each row, this makes the task easier.\n",
    "\n",
    "This embedding can stil be reduced but should be sparse enough already to make a good input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + 128 + 64 + 64 + 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 + 128 + 64 + 64 + 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromstring('0 0 1 0 1 0 0 1', dtype=bool, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromstring('00101001', dtype=bool)  # there seems to be an issue here on numpy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromstring('0 0 1 0 1 0 0 1', dtype=int, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.fromstring('\\n'.join(get_strbintable(16)), dtype='<H', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.reshape([16,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(get_strbintable(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried to do some things about the first part of the code, turning bytes to a numpy array, but seems that the most efficient way would be a table (numpy 2d array that has as index the int value of the input and there the array in that position is the binary code, this can already include the first pass to make a one hot of every N bits (maybe every 4, so there are not so many initial values ), this matrix could already have pre-computed the ECC ...\n",
    "\n",
    "For the ECC, I stil don't decide if making it by chunks of input bits, or by all the values, I guess that by all should do, but maybe is easier to compute them reshaping the input arrays to the code in use (example for Golay [24,12,8] will do for every 12 input bits) \n",
    "\n",
    "The idea is to not completely get rid of one-hot encoding, but to limit it to parts of the input vector code restricting the size of the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters for a one-hot by chunks encoding:\n",
    "chunk_sizes = [4, 5, 6, 8, 12]\n",
    "n_params = []\n",
    "for c in chunk_sizes:\n",
    "    n_params.append((c, (32 // c) * 2**c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe for my tests up to chunks of size 6 should be acceptable (I still need to add the next ECC)\n",
    "\n",
    "The next code can be:\n",
    "- Repetition (x3)\n",
    "- Hamming\n",
    "- Golay\n",
    "- Reed Solomon\n",
    "- Latin Square \n",
    "- AN Correcting\n",
    "\n",
    "Here some thoughts about the codes:\n",
    "\n",
    "Repetition: this has the disadvantage of giving a redundancy that is quite obvious besides the low power of reconstruction of catastrofic errors. Just repeating does not necessarilly adds to a neural network another perspective at the input. Might be worth trying, but for the moment I'm not interested in it.\n",
    "\n",
    "Hamming: it can correct one error (Hamming 7,4), adding 3 out of 4 bits. With an extra bit it can detect up to 2 errors with an extra parity bit.\n",
    "\n",
    "Golay: might serve well enough for my first tests as it ads not too much overhead (duplicates the number of elements) for an interesting error correction (up to 3 bits from each 12, so one fourth).\n",
    "\n",
    "\n",
    "There is one difference in the focus from this analysis and telecomunications (or any other domain with noisy channel), here I'm interested not in the code Rate (ammount of information sent vs ammount of actual bits sent) but in giving as input to the NN some form of non necessary evident redundancy that it could use, and having more ways to correct the output if a single mistake is made during the output extrapolation, I want ot check this part.\n",
    "\n",
    "Thinking a bit more about auto-encoders, it might not be the best idea to start there as it might not give any useful information ... I still have to try some things, might give it a try if it is quick enough to build it once I have the input code.\n",
    "\n",
    "\n",
    "For efficiency what I will do is build from the beginning the encoding table, for the decoding, I still need to think it thorugh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "emd = torch.nn.Embedding(2**10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, emd.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/7\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is a fully connected layer ... this means a LOT of parameters\n",
    "\n",
    "To be able to do an effective one-hot of all utf-8 would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50,100,200,300]:\n",
    "    print(i, 1112064 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means I don't want to train that layer ... it would not even fit in my GPU\n",
    "\n",
    "There is another thing, the embedding layer learns from the sample input, this means that it will ignore all values that don't appear or are underrepresented (a know issue). My goal is to deal with this with meta-learning techniques, but always being able to keep adding new inputs.\n",
    "\n",
    "So I want a few encoders to try:\n",
    "\n",
    "- chunked one-hot + hamming error correction of the first element\n",
    "- binary like encoding only (will be done per byte to avoid making a table that is too big)\n",
    "- binary like encoding with ECCs\n",
    "- binary like encoding but added one-hot by each 4 bits (total 16 * 8 -> 128 bits)\n",
    "- binary like encoding but added one-hot by each (4|12) bits plus ECC (total (16 * 8) + overload), hamming=224, golay=256\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128  + 32*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, for the moment I think that the thing that I need can be done with multihot encoding, this is easier to decode with multiple log softmax.\n",
    "\n",
    "For the most complex case of ECC, there is much more work to do for the decoding and even though I do have an idea of the encoding part I don't know yet how to do the decoding in a NN yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding utf8 encoded\n",
    "\n",
    "nums = [0x01, 0x02, 0x03, 0x04]\n",
    "masks = [0xf0, 0xe0, 0xd0, 0xc0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(nums) | np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([127])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utf8_encoder import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of codes =  59328\n",
      "number of code_exceptions =  4224\n"
     ]
    }
   ],
   "source": [
    "tables = create_tables(segments=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of codes =  1107904\n",
      "number of code_exceptions =  790656\n"
     ]
    }
   ],
   "source": [
    "tables = create_tables()  # 4 segments by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"utf8_code_matrix_3seg\", tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(tables[1], \"txt2code_3seg\")\n",
    "save_obj(tables[2], \"code2txt_3seg\")\n",
    "save_obj(tables[3], \"txt2num_3seg\")\n",
    "save_obj(tables[4], \"num2txt_3seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2c = tables[1]\n",
    "c2t = tables[2]\n",
    "n2t = tables[4]\n",
    "t2n = tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59328, 59328, 59328, 59328)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2n.keys()), len(n2t.keys()), len(tables[1].keys()), len(tables[2].keys()),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Wikipedia says:\n",
    "\n",
    "  UTF-8 is a variable width character encoding capable of encoding all 1,112,064[1] valid code points in Unicode using one to four 8-bit bytes.\n",
    "  \n",
    "We have managed to encode only 1107904 codes, so we are missing somehow 4160 codes that python can't encode from bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1112064 - 1107904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164864"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 + (2**5 * 2**6)+ (2**4 * (2**6)**2) + (2**3 * (2**6)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164864"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**21 + 2**16 + 2**11 + 2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices:  128 2176 65664 2097280\n"
     ]
    }
   ],
   "source": [
    "print(\"indices: \", 128, (128 + 2**5 * 2**6), (128 + 2**4 * (2**6)**2), (128 + 2**3 * (2**6)**3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from:  https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '\\x00'),\n",
       " (1, '\\x01'),\n",
       " (2, '\\x02'),\n",
       " (3, '\\x03'),\n",
       " (4, '\\x04'),\n",
       " (5, '\\x05'),\n",
       " (6, '\\x06'),\n",
       " (7, '\\x07'),\n",
       " (8, '\\x08'),\n",
       " (9, '\\t'),\n",
       " (10, '\\n'),\n",
       " (11, '\\x0b'),\n",
       " (12, '\\x0c'),\n",
       " (13, '\\r'),\n",
       " (14, '\\x0e'),\n",
       " (15, '\\x0f'),\n",
       " (16, '\\x10'),\n",
       " (17, '\\x11'),\n",
       " (18, '\\x12'),\n",
       " (19, '\\x13'),\n",
       " (20, '\\x14'),\n",
       " (21, '\\x15'),\n",
       " (22, '\\x16'),\n",
       " (23, '\\x17'),\n",
       " (24, '\\x18'),\n",
       " (25, '\\x19'),\n",
       " (26, '\\x1a'),\n",
       " (27, '\\x1b'),\n",
       " (28, '\\x1c'),\n",
       " (29, '\\x1d'),\n",
       " (30, '\\x1e'),\n",
       " (31, '\\x1f'),\n",
       " (32, ' '),\n",
       " (33, '!'),\n",
       " (34, '\"'),\n",
       " (35, '#'),\n",
       " (36, '$'),\n",
       " (37, '%'),\n",
       " (38, '&'),\n",
       " (39, \"'\"),\n",
       " (40, '('),\n",
       " (41, ')'),\n",
       " (42, '*'),\n",
       " (43, '+'),\n",
       " (44, ','),\n",
       " (45, '-'),\n",
       " (46, '.'),\n",
       " (47, '/'),\n",
       " (48, '0'),\n",
       " (49, '1'),\n",
       " (50, '2'),\n",
       " (51, '3'),\n",
       " (52, '4'),\n",
       " (53, '5'),\n",
       " (54, '6'),\n",
       " (55, '7'),\n",
       " (56, '8'),\n",
       " (57, '9'),\n",
       " (58, ':'),\n",
       " (59, ';'),\n",
       " (60, '<'),\n",
       " (61, '='),\n",
       " (62, '>'),\n",
       " (63, '?'),\n",
       " (64, '@'),\n",
       " (65, 'A'),\n",
       " (66, 'B'),\n",
       " (67, 'C'),\n",
       " (68, 'D'),\n",
       " (69, 'E'),\n",
       " (70, 'F'),\n",
       " (71, 'G'),\n",
       " (72, 'H'),\n",
       " (73, 'I'),\n",
       " (74, 'J'),\n",
       " (75, 'K'),\n",
       " (76, 'L'),\n",
       " (77, 'M'),\n",
       " (78, 'N'),\n",
       " (79, 'O'),\n",
       " (80, 'P'),\n",
       " (81, 'Q'),\n",
       " (82, 'R'),\n",
       " (83, 'S'),\n",
       " (84, 'T'),\n",
       " (85, 'U'),\n",
       " (86, 'V'),\n",
       " (87, 'W'),\n",
       " (88, 'X'),\n",
       " (89, 'Y'),\n",
       " (90, 'Z'),\n",
       " (91, '['),\n",
       " (92, '\\\\'),\n",
       " (93, ']'),\n",
       " (94, '^'),\n",
       " (95, '_'),\n",
       " (96, '`'),\n",
       " (97, 'a'),\n",
       " (98, 'b'),\n",
       " (99, 'c')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(100, n2t.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2n['\\x09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(take(10, t2c.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = torch.from_numpy(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://discuss.pytorch.org/t/how-to-convert-a-dense-matrix-to-a-sparse-one/7809\n",
    "\n",
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scodes = to_sparse(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scodes.is_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.sparse.DoubleTensor"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch sparse can't be saved yet ... not implemented for the moment (I should do it myself and send the patch)\n",
    "# torch.save(scodes, \"utf8-codes.pt\")\n",
    "# save_obj(scodes, \"utf8-codes.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spcodes = sp.sparse.coo_matrix(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(spcodes, \"utf8-codes-scipy-sparse_3seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the moment we have the posibility to encode all utf-8 characters, but is still a bit big in size when having the complete. But I'll try to cut the use of memory because 6.8GB for the \"dense\" matrix reprsentation is too much. In Sparse mode matrix is only 83MB for the entire dataset. Nevertheless there are many characters that I will not be using for the first tests, so having it use only a part will (should) be enough.\n",
    "\n",
    "So I'll see how big the encoder is without the 4 segments, but only using the first 3 (this should be enough for most applications) so we can encode \n",
    "\n",
    "number of codes =  59328\n",
    "\n",
    "number of code_exceptions =  4224\n",
    "\n",
    "the entire code is now 206MB on a file on disk in non sparse mode and 3.6MB on sparse mode on disk. \n",
    "\n",
    "But also reducing the number of bytes on the code (using only 3 bytes max instead of 4) by not taking the last one that anyways we are not using for this application we can reduce this to 177MB of the complete \"dense\" code on disk and 3.6MB on sparse mode.\n",
    "\n",
    "I would not recomend doing this all the time as it restricts the power of the input network to only known elements (and we want to do with all the possible codes) but for my tests this reduces the usage of memory, the amount of parameters and the processing time.\n",
    "\n",
    "So I can start playing with it without worrying about memory ;)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
