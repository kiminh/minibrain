{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tests for Part Of Speech tagging\n",
    "\n",
    "This notebook is dedicated to start working with the PoS dataset already pre-processed and the column networks that I'm creating.\n",
    "\n",
    "The network will be constructed from small parts, each will be trained on top of the previous one, adding a new column and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Loading faiss with AVX2 support.\n",
      "Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from langmodels.models import *\n",
    "import langmodels.utf8codec as utf8codec\n",
    "from langmodels.utils.tools import *\n",
    "from langmodels.utils.helpers import *\n",
    "from langmodels.utils.preprocess_conllu import *\n",
    "from langmodels.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f69ada1db10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "# torch.manual_seed(random.randint(0,1000))\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# main_conv1dcolnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weird thing that the training curves jump up at a certain moment, with one try it went up at epoch 11,12 and the other at epochs 6,7,8; 72,73,74,75 after those it goes down. I will have to see that.\n",
    "\n",
    "Also the observation that upos loss is less than the deprel loss, this makes sense as deprel task is much more difficult (many more number of classes) than upos and we are using the same network for both and giving the same importance in the loss to both tasks. This serves the purpose for pre-training (starting) the networks, but nothing else. A better loss must be created in order to improve the accuracy (which I'm not yet measuring)\n",
    "\n",
    "\n",
    "Note, the TensorBoard names of these 2 training samples are:\n",
    "*Dec04_11-52-54_labestia* and *Dec04_13-56-43_labestia* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 GeForce RTX 2080 Ti\n",
      "1 GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "# torch.cuda.set_device(1)  # device 0 is gpu 1 and vice-versa so: RTX2080ti is GPU:1 but cuda:0\n",
    "# print(torch.cuda.current_device())\n",
    "print(0, torch.cuda.get_device_name(0))\n",
    "print(1, torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll do a first try with the pretrained networks that jump, then I'll try again with a newly trained conv1dcolnet and then without convolutional pre-training and training all from scratch.\n",
    "\n",
    "This will take a lot of time unless my new rtx2080ti arrives ...so it'll take less time there but still, a lot of time -> ARRIVED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model with column type ConvAttNetCol and pretrained Conv1dColNet\n",
      "Parameter model details: \n",
      "conv1d_encoder parameters 2173824 from which 2173824 are trainable \n",
      "ConvAttColNet parameters 13016064 from which 13016064 are trainable \n",
      "decoder parameters 378832 from which 378832 are trainable \n",
      "Total model parameters 14037712 from which 14037712 are trainable \n",
      "====> Timestamp 2019-12-06 14:42:07.453593 Epoch: 1 Average loss: 0.04023628\n",
      "epoch: 1====> Test set loss: 1.70771577\n",
      "====> Timestamp 2019-12-06 14:43:35.835037 Epoch: 2 Average loss: 0.03531876\n",
      "epoch: 2====> Test set loss: 1.71208881\n",
      "====> Timestamp 2019-12-06 14:45:04.130045 Epoch: 3 Average loss: 0.03149185\n",
      "epoch: 3====> Test set loss: 1.58856511\n",
      "====> Timestamp 2019-12-06 14:46:32.542815 Epoch: 4 Average loss: 0.02931030\n",
      "epoch: 4====> Test set loss: 1.96122572\n",
      "====> Timestamp 2019-12-06 14:48:00.516503 Epoch: 5 Average loss: 0.02930105\n",
      "epoch: 5====> Test set loss: 1.61341013\n",
      "====> Timestamp 2019-12-06 14:49:27.289465 Epoch: 6 Average loss: nan\n",
      "epoch: 6====> Test set loss: nan\n",
      "====> Timestamp 2019-12-06 14:50:52.188218 Epoch: 7 Average loss: nan\n",
      "epoch: 7====> Test set loss: nan\n",
      "====> Timestamp 2019-12-06 14:52:17.962145 Epoch: 8 Average loss: nan\n",
      "epoch: 8====> Test set loss: nan\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_convattnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utf8codes = np.load(utf8codematrix)\n",
    "# # utf8codes = utf8codes.reshape(1987, 324)\n",
    "# # the convolutional encoder must NOT be retrained (that is what I'm trying to test)\n",
    "# # with torch.no_grad():\n",
    "# #     conv1d_encoder = Conv1DColNet(transpose_output=False)  # use default parameters\n",
    "# #     conv1d_decoder = LinearUposDeprelDecoder(transpose_input=False)\n",
    "# #     conv1d_model = NetContainer(utf8codes, conv1d_encoder, conv1d_decoder)\n",
    "# #     # load pre-trained conv1dcolnet\n",
    "# #     # conv1d_model.load_checkpoint(conv1d_pretrain_file)\n",
    "# #     # cleanup things that we'll not use, we just need the encoder\n",
    "# #     del conv1d_model\n",
    "# #     del conv1d_decoder\n",
    "# #     torch.cuda.empty_cache()\n",
    "# conv1d_encoder = Conv1DColNet(transpose_output=False)  # use default parameters\n",
    "# encoder = ConvAttColNet(conv1d_encoder, transpose_output=False)\n",
    "# decoder = LinearUposDeprelDecoder(transpose_input=False)\n",
    "# model = NetContainer(utf8codes, encoder, decoder)\n",
    "# print(\"Starting training for model with column type ConvAttNetCol and pretrained Conv1dColNet\")\n",
    "# print(\"Parameter model details: \")\n",
    "# print(\"conv1d_encoder parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(conv1d_encoder), count_parameters(conv1d_encoder)))\n",
    "# print(\"ConvAttColNet parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(encoder), count_parameters(encoder)))\n",
    "# print(\"decoder parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(decoder), count_parameters(decoder)))\n",
    "# print(\"Total model parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(model), count_parameters(model)))\n",
    "# path = \"./trained_models/ConvAttNet\"\n",
    "# base_name = \"ConvAttNet_nll-loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_test(model, path, base_name, max_seq_len=384, max_data=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training now goes smoothly with the new  card rtx2080ti, takes about 2 hours for the entire network from scratch. \n",
    "\n",
    "There is an issue with the training where there are a couple of spikes in the training (and testing) results (epochs 33, 34 and 63), although it comes back later better, this behaviour was shown already in some other tests on the Conv1dColNet training today in the GTX1080 card.\n",
    "\n",
    "Results from Conv1dColNet and ConvAttColNet are not directly comparable due to the shape of the vectors (the later outputs only a part of the result, the las 384 elements). What might be interesting is to use this pretrained network to do some fine-tuninig for a full lenght (1024) Attention output, this might be faster if only I train the last (big) layer and then fine tuning the previous ones (fast.ai results on ULMFit for example, although the method needs to be tweaked for the network architecture here that is a bit more complex)\n",
    "\n",
    "\n",
    "Next stage is actually measuring the accuracy (need to create the measurement) \n",
    "\n",
    "Later need to work on the:\n",
    "\n",
    "* Do a training on supervized tasks where the input is noisy (something like for the denoiser autoencoders for Language Modeling)\n",
    "* Language Model\n",
    "* language encoding (having the list of languages and making it as a new input to the network for the output language selection)\n",
    "* input language detection\n",
    "* being able to add more context\n",
    "* ... Many Many more things...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training seems unstable .... loss oscilates on an (bad, really bad) asymptote, one case the test set even seems to stabilize in a loss that is even bigger than the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
