{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tests for Part Of Speech tagging\n",
    "\n",
    "This notebook is dedicated to start working with the PoS dataset already pre-processed and the column networks that I'm creating.\n",
    "\n",
    "The network will be constructed from small parts, each will be trained on top of the previous one, adding a new column and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Loading faiss with AVX2 support.\n",
      "Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from langmodels.models import *\n",
    "import langmodels.utf8codec as utf8codec\n",
    "from langmodels.utils.tools import *\n",
    "from langmodels.utils.helpers import *\n",
    "from langmodels.utils.preprocess_conllu import *\n",
    "from langmodels.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f5085a9b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "# torch.manual_seed(random.randint(0,1000))\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# main_conv1dcolnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weird thing that the training curves jump up at a certain moment, with one try it went up at epoch 11,12 and the other at epochs 6,7,8; 72,73,74,75 after those it goes down. I will have to see that.\n",
    "\n",
    "Also the observation that upos loss is less than the deprel loss, this makes sense as deprel task is much more difficult (many more number of classes) than upos and we are using the same network for both and giving the same importance in the loss to both tasks. This serves the purpose for pre-training (starting) the networks, but nothing else. A better loss must be created in order to improve the accuracy (which I'm not yet measuring)\n",
    "\n",
    "\n",
    "Note, the TensorBoard names of these 2 training samples are:\n",
    "*Dec04_11-52-54_labestia* and *Dec04_13-56-43_labestia* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 GeForce RTX 2080 Ti\n",
      "1 GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "# torch.cuda.set_device(1)  # device 0 is gpu 1 and vice-versa so: RTX2080ti is GPU:1 but cuda:0\n",
    "# print(torch.cuda.current_device())\n",
    "print(0, torch.cuda.get_device_name(0))\n",
    "print(1, torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll do a first try with the pretrained networks that jump, then I'll try again with a newly trained conv1dcolnet and then without convolutional pre-training and training all from scratch.\n",
    "\n",
    "This will take a lot of time unless my new rtx2080ti arrives ...so it'll take less time there but still, a lot of time -> ARRIVED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model with column type ConvAttNetCol and pretrained Conv1dColNet\n",
      "Parameter model details: \n",
      "conv1d_encoder parameters 2173824 from which 2173824 are trainable \n",
      "ConvAttColNet parameters 13016064 from which 13016064 are trainable \n",
      "decoder parameters 378832 from which 378832 are trainable \n",
      "Total model parameters 14037712 from which 14037712 are trainable \n",
      "====> Timestamp 2020-01-29 20:01:17.213411 Epoch: 1 Average loss: 0.03913716\n",
      "epoch: 1====> Test set loss: 1.69911756\n",
      "====> Timestamp 2020-01-29 20:02:43.753063 Epoch: 2 Average loss: 0.03320252\n",
      "epoch: 2====> Test set loss: 1.86538986\n",
      "====> Timestamp 2020-01-29 20:04:10.116869 Epoch: 3 Average loss: 0.03010302\n",
      "epoch: 3====> Test set loss: 2.14886160\n",
      "====> Timestamp 2020-01-29 20:05:36.377858 Epoch: 4 Average loss: 0.02987164\n",
      "epoch: 4====> Test set loss: 2.15790308\n",
      "====> Timestamp 2020-01-29 20:07:03.034005 Epoch: 5 Average loss: 0.02899821\n",
      "epoch: 5====> Test set loss: 1.70517449\n",
      "====> Timestamp 2020-01-29 20:08:30.065170 Epoch: 6 Average loss: 0.02902412\n",
      "epoch: 6====> Test set loss: 1.71743888\n",
      "====> Timestamp 2020-01-29 20:09:56.564232 Epoch: 7 Average loss: 0.02820571\n",
      "epoch: 7====> Test set loss: 1.63998588\n",
      "====> Timestamp 2020-01-29 20:11:23.443761 Epoch: 8 Average loss: 0.02818096\n",
      "epoch: 8====> Test set loss: 1.56782799\n",
      "====> Timestamp 2020-01-29 20:12:49.936290 Epoch: 9 Average loss: 0.02782679\n",
      "epoch: 9====> Test set loss: 1.47654458\n",
      "====> Timestamp 2020-01-29 20:14:16.559588 Epoch: 10 Average loss: 0.02799470\n",
      "epoch: 10====> Test set loss: 1.58977425\n",
      "====> Timestamp 2020-01-29 20:15:43.457795 Epoch: 11 Average loss: 0.02802050\n",
      "epoch: 11====> Test set loss: 1.50940032\n",
      "====> Timestamp 2020-01-29 20:17:10.057946 Epoch: 12 Average loss: 0.02811584\n",
      "epoch: 12====> Test set loss: 1.38264805\n",
      "====> Timestamp 2020-01-29 20:18:36.412733 Epoch: 13 Average loss: 0.02737848\n",
      "epoch: 13====> Test set loss: 1.36943028\n",
      "====> Timestamp 2020-01-29 20:20:02.684766 Epoch: 14 Average loss: 0.02718559\n",
      "epoch: 14====> Test set loss: 1.36549227\n",
      "====> Timestamp 2020-01-29 20:21:29.099010 Epoch: 15 Average loss: 0.02690037\n",
      "epoch: 15====> Test set loss: 1.38343205\n",
      "====> Timestamp 2020-01-29 20:22:55.759403 Epoch: 16 Average loss: 0.02721870\n",
      "epoch: 16====> Test set loss: 1.38260849\n",
      "====> Timestamp 2020-01-29 20:24:22.023615 Epoch: 17 Average loss: 0.02665234\n",
      "epoch: 17====> Test set loss: 1.33070121\n",
      "====> Timestamp 2020-01-29 20:25:48.967347 Epoch: 18 Average loss: 0.02681949\n",
      "epoch: 18====> Test set loss: 1.32227725\n",
      "====> Timestamp 2020-01-29 20:27:15.450179 Epoch: 19 Average loss: 0.02626099\n",
      "epoch: 19====> Test set loss: 1.32927644\n",
      "====> Timestamp 2020-01-29 20:28:41.858841 Epoch: 20 Average loss: 0.02646714\n",
      "epoch: 20====> Test set loss: 1.32142399\n",
      "====> Timestamp 2020-01-29 20:30:08.218394 Epoch: 21 Average loss: 0.02618863\n",
      "epoch: 21====> Test set loss: 1.30425533\n",
      "====> Timestamp 2020-01-29 20:31:34.818476 Epoch: 22 Average loss: 0.02650379\n",
      "epoch: 22====> Test set loss: 1.30379756\n",
      "====> Timestamp 2020-01-29 20:33:01.868566 Epoch: 23 Average loss: 0.02605559\n",
      "epoch: 23====> Test set loss: 1.30305690\n",
      "====> Timestamp 2020-01-29 20:34:28.525803 Epoch: 24 Average loss: 0.02630483\n",
      "epoch: 24====> Test set loss: 1.29816660\n",
      "====> Timestamp 2020-01-29 20:35:55.234730 Epoch: 25 Average loss: 0.02665920\n",
      "epoch: 25====> Test set loss: 1.30789903\n",
      "====> Timestamp 2020-01-29 20:37:21.937610 Epoch: 26 Average loss: 0.02637282\n",
      "epoch: 26====> Test set loss: 1.30108502\n",
      "====> Timestamp 2020-01-29 20:38:49.205082 Epoch: 27 Average loss: 0.02617938\n",
      "epoch: 27====> Test set loss: 1.29569381\n",
      "====> Timestamp 2020-01-29 20:40:15.807413 Epoch: 28 Average loss: 0.02613540\n",
      "epoch: 28====> Test set loss: 1.29139842\n",
      "====> Timestamp 2020-01-29 20:41:42.529796 Epoch: 29 Average loss: 0.02597581\n",
      "epoch: 29====> Test set loss: 1.29252040\n",
      "====> Timestamp 2020-01-29 20:43:09.156161 Epoch: 30 Average loss: 0.02594701\n",
      "epoch: 30====> Test set loss: 1.31170711\n",
      "====> Timestamp 2020-01-29 20:44:36.684065 Epoch: 31 Average loss: 0.02658750\n",
      "epoch: 31====> Test set loss: 1.29270043\n",
      "====> Timestamp 2020-01-29 20:46:04.173617 Epoch: 32 Average loss: 0.02623483\n",
      "epoch: 32====> Test set loss: 1.28205069\n",
      "====> Timestamp 2020-01-29 20:47:31.796767 Epoch: 33 Average loss: 0.02605039\n",
      "epoch: 33====> Test set loss: 1.27885185\n",
      "====> Timestamp 2020-01-29 20:48:59.454463 Epoch: 34 Average loss: 0.02627183\n",
      "epoch: 34====> Test set loss: 1.27773106\n",
      "====> Timestamp 2020-01-29 20:50:26.571290 Epoch: 35 Average loss: 0.02616787\n",
      "epoch: 35====> Test set loss: 1.29188329\n",
      "====> Timestamp 2020-01-29 20:51:54.464566 Epoch: 36 Average loss: 0.02623307\n",
      "epoch: 36====> Test set loss: 1.28212669\n",
      "====> Timestamp 2020-01-29 20:53:21.951890 Epoch: 37 Average loss: 0.02631728\n",
      "epoch: 37====> Test set loss: 1.28271435\n",
      "====> Timestamp 2020-01-29 20:54:48.991961 Epoch: 38 Average loss: 0.02615642\n",
      "epoch: 38====> Test set loss: 1.27519894\n",
      "====> Timestamp 2020-01-29 20:56:15.804922 Epoch: 39 Average loss: 0.02616441\n",
      "epoch: 39====> Test set loss: 1.27962240\n",
      "====> Timestamp 2020-01-29 20:57:42.751323 Epoch: 40 Average loss: 0.02594081\n",
      "epoch: 40====> Test set loss: 1.28533323\n",
      "====> Timestamp 2020-01-29 20:59:09.614533 Epoch: 41 Average loss: 0.02933589\n",
      "epoch: 41====> Test set loss: 1.53022345\n",
      "====> Timestamp 2020-01-29 21:00:36.609486 Epoch: 42 Average loss: 0.02905274\n",
      "epoch: 42====> Test set loss: 1.42023005\n",
      "====> Timestamp 2020-01-29 21:02:03.426085 Epoch: 43 Average loss: 0.02798944\n",
      "epoch: 43====> Test set loss: 1.37728393\n",
      "====> Timestamp 2020-01-29 21:03:30.584025 Epoch: 44 Average loss: 0.02755605\n",
      "epoch: 44====> Test set loss: 1.35900888\n",
      "====> Timestamp 2020-01-29 21:04:57.291543 Epoch: 45 Average loss: 0.02741297\n",
      "epoch: 45====> Test set loss: 1.37692163\n",
      "====> Timestamp 2020-01-29 21:06:24.526120 Epoch: 46 Average loss: 0.02707901\n",
      "epoch: 46====> Test set loss: 1.36073690\n",
      "====> Timestamp 2020-01-29 21:07:51.491514 Epoch: 47 Average loss: 0.02718582\n",
      "epoch: 47====> Test set loss: 1.34648270\n",
      "====> Timestamp 2020-01-29 21:09:18.220152 Epoch: 48 Average loss: 0.02696505\n",
      "epoch: 48====> Test set loss: 1.34975165\n",
      "====> Timestamp 2020-01-29 21:10:45.157252 Epoch: 49 Average loss: 0.02625847\n",
      "epoch: 49====> Test set loss: 1.30327694\n",
      "====> Timestamp 2020-01-29 21:12:12.225669 Epoch: 50 Average loss: 0.02654040\n",
      "epoch: 50====> Test set loss: 1.29158942\n",
      "====> Timestamp 2020-01-29 21:13:39.117804 Epoch: 51 Average loss: 0.02586604\n",
      "epoch: 51====> Test set loss: 1.29209176\n",
      "====> Timestamp 2020-01-29 21:15:06.188914 Epoch: 52 Average loss: 0.02620949\n",
      "epoch: 52====> Test set loss: 1.28773402\n",
      "====> Timestamp 2020-01-29 21:16:33.094910 Epoch: 53 Average loss: 0.02605093\n",
      "epoch: 53====> Test set loss: 1.32178382\n",
      "====> Timestamp 2020-01-29 21:18:00.103952 Epoch: 54 Average loss: 0.02633582\n",
      "epoch: 54====> Test set loss: 1.31151096\n",
      "====> Timestamp 2020-01-29 21:19:26.922225 Epoch: 55 Average loss: 0.02645421\n",
      "epoch: 55====> Test set loss: 1.33570925\n",
      "====> Timestamp 2020-01-29 21:20:53.905815 Epoch: 56 Average loss: 0.02634488\n",
      "epoch: 56====> Test set loss: 1.29567006\n",
      "====> Timestamp 2020-01-29 21:22:20.786117 Epoch: 57 Average loss: 0.02591495\n",
      "epoch: 57====> Test set loss: 1.28807005\n",
      "====> Timestamp 2020-01-29 21:23:47.565798 Epoch: 58 Average loss: 0.02618377\n",
      "epoch: 58====> Test set loss: 1.30442179\n",
      "====> Timestamp 2020-01-29 21:25:14.187811 Epoch: 59 Average loss: 0.02588734\n",
      "epoch: 59====> Test set loss: 1.29423863\n",
      "====> Timestamp 2020-01-29 21:26:40.950578 Epoch: 60 Average loss: 0.02598189\n",
      "epoch: 60====> Test set loss: 1.29698848\n",
      "====> Timestamp 2020-01-29 21:28:07.796143 Epoch: 61 Average loss: 0.02618239\n",
      "epoch: 61====> Test set loss: 1.29327163\n",
      "====> Timestamp 2020-01-29 21:29:35.314372 Epoch: 62 Average loss: 0.02632434\n",
      "epoch: 62====> Test set loss: 1.28896136\n",
      "====> Timestamp 2020-01-29 21:31:01.986641 Epoch: 63 Average loss: 0.02582777\n",
      "epoch: 63====> Test set loss: 1.29452427\n",
      "====> Timestamp 2020-01-29 21:32:29.038015 Epoch: 64 Average loss: 0.02598150\n",
      "epoch: 64====> Test set loss: 1.29466714\n",
      "====> Timestamp 2020-01-29 21:33:56.240163 Epoch: 65 Average loss: 0.02575414\n",
      "epoch: 65====> Test set loss: 1.29624214\n",
      "====> Timestamp 2020-01-29 21:35:23.320824 Epoch: 66 Average loss: 0.02585142\n",
      "epoch: 66====> Test set loss: 1.28773578\n",
      "====> Timestamp 2020-01-29 21:36:49.974361 Epoch: 67 Average loss: 0.02563951\n",
      "epoch: 67====> Test set loss: 1.29811505\n",
      "====> Timestamp 2020-01-29 21:38:16.731314 Epoch: 68 Average loss: 0.02585215\n",
      "epoch: 68====> Test set loss: 1.28299054\n",
      "====> Timestamp 2020-01-29 21:39:43.247860 Epoch: 69 Average loss: 0.02593340\n",
      "epoch: 69====> Test set loss: 1.29422860\n",
      "====> Timestamp 2020-01-29 21:41:10.192211 Epoch: 70 Average loss: 0.02559174\n",
      "epoch: 70====> Test set loss: 1.28042221\n",
      "====> Timestamp 2020-01-29 21:42:36.714319 Epoch: 71 Average loss: 0.02598028\n",
      "epoch: 71====> Test set loss: 1.27452290\n",
      "====> Timestamp 2020-01-29 21:44:03.584448 Epoch: 72 Average loss: 0.02609152\n",
      "epoch: 72====> Test set loss: 1.30143422\n",
      "====> Timestamp 2020-01-29 21:45:31.222834 Epoch: 73 Average loss: 0.02573548\n",
      "epoch: 73====> Test set loss: 1.28235198\n",
      "====> Timestamp 2020-01-29 21:46:58.135074 Epoch: 74 Average loss: 0.02689149\n",
      "epoch: 74====> Test set loss: 1.58707386\n",
      "====> Timestamp 2020-01-29 21:48:25.036574 Epoch: 75 Average loss: 0.02847620\n",
      "epoch: 75====> Test set loss: 1.35645074\n",
      "====> Timestamp 2020-01-29 21:49:52.099775 Epoch: 76 Average loss: 0.02749886\n",
      "epoch: 76====> Test set loss: 1.36013183\n",
      "====> Timestamp 2020-01-29 21:51:19.041343 Epoch: 77 Average loss: 0.02702543\n",
      "epoch: 77====> Test set loss: 1.37247990\n",
      "====> Timestamp 2020-01-29 21:52:45.811241 Epoch: 78 Average loss: 0.02744976\n",
      "epoch: 78====> Test set loss: 1.39477088\n",
      "====> Timestamp 2020-01-29 21:54:12.908780 Epoch: 79 Average loss: 0.02697854\n",
      "epoch: 79====> Test set loss: 1.35014591\n",
      "====> Timestamp 2020-01-29 21:55:39.821060 Epoch: 80 Average loss: 0.02702060\n",
      "epoch: 80====> Test set loss: 1.35228283\n",
      "====> Timestamp 2020-01-29 21:57:06.642418 Epoch: 81 Average loss: 0.02680398\n",
      "epoch: 81====> Test set loss: 1.38129618\n",
      "====> Timestamp 2020-01-29 21:58:33.810507 Epoch: 82 Average loss: 0.02682009\n",
      "epoch: 82====> Test set loss: 1.35053554\n",
      "====> Timestamp 2020-01-29 22:00:00.601845 Epoch: 83 Average loss: 0.02656614\n",
      "epoch: 83====> Test set loss: 1.33918507\n",
      "====> Timestamp 2020-01-29 22:01:27.555796 Epoch: 84 Average loss: 0.02666721\n",
      "epoch: 84====> Test set loss: 1.38331080\n",
      "====> Timestamp 2020-01-29 22:02:54.197453 Epoch: 85 Average loss: 0.02684729\n",
      "epoch: 85====> Test set loss: 1.37481837\n",
      "====> Timestamp 2020-01-29 22:04:20.904899 Epoch: 86 Average loss: 0.02671011\n",
      "epoch: 86====> Test set loss: 1.38744248\n",
      "====> Timestamp 2020-01-29 22:05:47.967848 Epoch: 87 Average loss: 0.02689529\n",
      "epoch: 87====> Test set loss: 1.32802105\n",
      "====> Timestamp 2020-01-29 22:07:14.925710 Epoch: 88 Average loss: 0.02676750\n",
      "epoch: 88====> Test set loss: 1.33380357\n",
      "====> Timestamp 2020-01-29 22:08:42.094308 Epoch: 89 Average loss: 0.02675879\n",
      "epoch: 89====> Test set loss: 1.34473817\n",
      "====> Timestamp 2020-01-29 22:10:08.748957 Epoch: 90 Average loss: 0.02655174\n",
      "epoch: 90====> Test set loss: 1.32859660\n",
      "====> Timestamp 2020-01-29 22:11:35.705323 Epoch: 91 Average loss: 0.02670682\n",
      "epoch: 91====> Test set loss: 1.33529006\n",
      "====> Timestamp 2020-01-29 22:13:03.011231 Epoch: 92 Average loss: 0.02658609\n",
      "epoch: 92====> Test set loss: 1.33404647\n",
      "====> Timestamp 2020-01-29 22:14:29.814307 Epoch: 93 Average loss: 0.02674952\n",
      "epoch: 93====> Test set loss: 1.33981439\n",
      "====> Timestamp 2020-01-29 22:15:56.581078 Epoch: 94 Average loss: 0.02656792\n",
      "epoch: 94====> Test set loss: 1.32084670\n",
      "====> Timestamp 2020-01-29 22:17:23.343900 Epoch: 95 Average loss: 0.02680621\n",
      "epoch: 95====> Test set loss: 1.32016254\n",
      "====> Timestamp 2020-01-29 22:18:49.991996 Epoch: 96 Average loss: 0.02668037\n",
      "epoch: 96====> Test set loss: 1.34870898\n",
      "====> Timestamp 2020-01-29 22:20:16.739935 Epoch: 97 Average loss: 0.02635277\n",
      "epoch: 97====> Test set loss: 1.31706852\n",
      "====> Timestamp 2020-01-29 22:21:43.556744 Epoch: 98 Average loss: 0.02669792\n",
      "epoch: 98====> Test set loss: 1.32559929\n",
      "====> Timestamp 2020-01-29 22:23:10.501627 Epoch: 99 Average loss: 0.02636692\n",
      "epoch: 99====> Test set loss: 1.35416725\n",
      "====> Timestamp 2020-01-29 22:24:37.193346 Epoch: 100 Average loss: 0.02914837\n",
      "epoch: 100====> Test set loss: 1.41355811\n",
      "====> Timestamp 2020-01-29 22:26:04.565859 Epoch: 101 Average loss: 0.02627879\n",
      "epoch: 101====> Test set loss: 1.47856002\n",
      "====> Timestamp 2020-01-29 22:26:29.777840 Epoch: 102 Average loss: 0.00200793\n",
      "epoch: 102====> Test set loss: 1.51060348\n",
      "CPU times: user 1h 32min 17s, sys: 54min, total: 2h 26min 18s\n",
      "Wall time: 2h 26min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_convattnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utf8codes = np.load(utf8codematrix)\n",
    "# # utf8codes = utf8codes.reshape(1987, 324)\n",
    "# # the convolutional encoder must NOT be retrained (that is what I'm trying to test)\n",
    "# # with torch.no_grad():\n",
    "# #     conv1d_encoder = Conv1DColNet(transpose_output=False)  # use default parameters\n",
    "# #     conv1d_decoder = LinearUposDeprelDecoder(transpose_input=False)\n",
    "# #     conv1d_model = NetContainer(utf8codes, conv1d_encoder, conv1d_decoder)\n",
    "# #     # load pre-trained conv1dcolnet\n",
    "# #     # conv1d_model.load_checkpoint(conv1d_pretrain_file)\n",
    "# #     # cleanup things that we'll not use, we just need the encoder\n",
    "# #     del conv1d_model\n",
    "# #     del conv1d_decoder\n",
    "# #     torch.cuda.empty_cache()\n",
    "# conv1d_encoder = Conv1DColNet(transpose_output=False)  # use default parameters\n",
    "# encoder = ConvAttColNet(conv1d_encoder, transpose_output=False)\n",
    "# decoder = LinearUposDeprelDecoder(transpose_input=False)\n",
    "# model = NetContainer(utf8codes, encoder, decoder)\n",
    "# print(\"Starting training for model with column type ConvAttNetCol and pretrained Conv1dColNet\")\n",
    "# print(\"Parameter model details: \")\n",
    "# print(\"conv1d_encoder parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(conv1d_encoder), count_parameters(conv1d_encoder)))\n",
    "# print(\"ConvAttColNet parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(encoder), count_parameters(encoder)))\n",
    "# print(\"decoder parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(decoder), count_parameters(decoder)))\n",
    "# print(\"Total model parameters {} from which {} are trainable \".\n",
    "#       format(count_parameters(model), count_parameters(model)))\n",
    "# path = \"./trained_models/ConvAttNet\"\n",
    "# base_name = \"ConvAttNet_nll-loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_test(model, path, base_name, max_seq_len=384, max_data=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training now goes smoothly with the new  card rtx2080ti, takes about 2 hours for the entire network from scratch. \n",
    "\n",
    "    Starting training for model with column type ConvAttNetCol and pretrained Conv1dColNet\n",
    "    Parameter model details: \n",
    "    conv1d_encoder parameters 2173824 from which 2173824 are trainable \n",
    "    ConvAttColNet parameters 13016064 from which 13016064 are trainable \n",
    "    decoder parameters 378832 from which 378832 are trainable \n",
    "    Total model parameters 14037712 from which 14037712 are trainable \n",
    "    ====> Timestamp 2020-01-29 20:01:17.213411 Epoch: 1 Average loss: 0.03913716\n",
    "    epoch: 1====> Test set loss: 1.69911756\n",
    "    ====> Timestamp 2020-01-29 20:02:43.753063 Epoch: 2 Average loss: 0.03320252\n",
    "    epoch: 2====> Test set loss: 1.86538986\n",
    "    ====> Timestamp 2020-01-29 20:04:10.116869 Epoch: 3 Average loss: 0.03010302\n",
    "    epoch: 3====> Test set loss: 2.14886160\n",
    "    ====> Timestamp 2020-01-29 20:05:36.377858 Epoch: 4 Average loss: 0.02987164\n",
    "    epoch: 4====> Test set loss: 2.15790308\n",
    "    ====> Timestamp 2020-01-29 20:07:03.034005 Epoch: 5 Average loss: 0.02899821\n",
    "    epoch: 5====> Test set loss: 1.70517449\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    epoch: 98====> Test set loss: 1.32559929\n",
    "    ====> Timestamp 2020-01-29 22:23:10.501627 Epoch: 99 Average loss: 0.02636692\n",
    "    epoch: 99====> Test set loss: 1.35416725\n",
    "    ====> Timestamp 2020-01-29 22:24:37.193346 Epoch: 100 Average loss: 0.02914837\n",
    "    epoch: 100====> Test set loss: 1.41355811\n",
    "    ====> Timestamp 2020-01-29 22:26:04.565859 Epoch: 101 Average loss: 0.02627879\n",
    "    epoch: 101====> Test set loss: 1.47856002\n",
    "    ====> Timestamp 2020-01-29 22:26:29.777840 Epoch: 102 Average loss: 0.00200793\n",
    "    epoch: 102====> Test set loss: 1.51060348\n",
    "    CPU times: user 1h 32min 17s, sys: 54min, total: 2h 26min 18s\n",
    "    Wall time: 2h 26min 42s\n",
    "\n",
    "There is an issue with the training where there are a couple of spikes in the training (and testing) results (epochs 33, 34 and 63), although it comes back later better, this behaviour was shown already in some other tests on the Conv1dColNet training today in the GTX1080 card.\n",
    "\n",
    "Results from Conv1dColNet and ConvAttColNet are not directly comparable due to the shape of the vectors (the later outputs only a part of the result, the last 384 elements). What might be interesting is to use this pretrained network to do some fine-tuninig for a full lenght (1024) Attention output, this might be faster if only I train the last (big) layer and then fine tuning the previous ones (fast.ai results on ULMFit for example, although the method needs to be tweaked for the network architecture here that is a bit more complex)\n",
    "\n",
    "\n",
    "Next stage is actually measuring the accuracy (need to create the measurement) \n",
    "\n",
    "Later need to work on the:\n",
    "\n",
    "* Do a training on supervized tasks where the input is noisy (something like for the denoiser autoencoders for Language Modeling)\n",
    "* Language Model\n",
    "* language encoding (having the list of languages and making it as a new input to the network for the output language selection)\n",
    "* input language detection\n",
    "* being able to add more context\n",
    "* ... Many Many more things...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training seems unstable .... loss oscilates on an (bad, really bad) asymptote, one case the test set even seems to stabilize in a loss that is even bigger than the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
