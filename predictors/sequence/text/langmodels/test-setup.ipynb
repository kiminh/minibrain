{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import itertools\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from column_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "filename = 'text8.zip'\n",
    "train_size = 99000000\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    print('Downloading text8 dataset...')\n",
    "\n",
    "    with urllib.request.urlopen(url) as response, \\\n",
    "        open(filename, 'wb') as outfile:\n",
    "        shutil.copyfileobj(response, outfile)\n",
    "\n",
    "rawdata = zipfile.ZipFile(filename).read('text8').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split = rawdata[:train_size]\n",
    "# valid_split = rawdata[train_size:]\n",
    "\n",
    "# vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for c in train_split:\n",
    "#     vocab[c] += 1\n",
    "\n",
    "# vocab_cut = {k: v for k, v in vocab.items() if v > 10}\n",
    "# vocab_sorted = sorted(vocab_cut.items(), key=lambda x: x[1], reverse=True)\n",
    "# wordmap = {k: id + 1 for id, (k, _) in enumerate(vocab_sorted)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(wordmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "# embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "# lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "# hello_embed = embeds(lookup_tensor)\n",
    "# print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embeds(torch.tensor([0,0,0,1], dtype=torch.long)))\n",
    "# print(embeds(torch.tensor(1, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is not THE most efficient thing, is a step to learn ... alter will deal with memory issues\n",
    "# to change things for the better the input text should be read by chunks\n",
    "\n",
    "# splicing through the text such as to create the input vectors\n",
    "def get_next_batch_txt(in_txt, index=0, char_len=1024, step=128, batch_size=128):\n",
    "    \"\"\"\n",
    "    Returns a generator of batches for the given input.\n",
    "    \n",
    "    :param in_txt: text used to generate the tensor data\n",
    "    :param index: index where to begin the next batch\n",
    "    :param char_len: number of characters lenght of the tensor\n",
    "    :param step: number of characters to jump for the next step (we don't want to repeat too many things ... )\n",
    "    :param batch_size: number of samples in each batch\n",
    "    \n",
    "    :returns A generator of batches for the given input with dimension [seq_len,batch_size,1]\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for b in range(batch_size):\n",
    "        # get the new part\n",
    "#         print(\"get next \", index)\n",
    "        ret.append(in_txt[index:min(index+char_len, len(in_txt)-1)])\n",
    "        # go to next\n",
    "        index += step\n",
    "        if index >= len(in_txt) - step:\n",
    "            index = -1\n",
    "            break\n",
    "        \n",
    "    # tensorize\n",
    "    \n",
    "    # TODO now make sure all the parts are equal, else complete with\n",
    "    return ret, index\n",
    "    \n",
    "# simple tokenizer, should do something to make it fast instead of char by char ... ?\n",
    "def tokenize_txt(txt, tokendict, dim):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = np.empty(dim)\n",
    "    for i in range(len(txt)):\n",
    "        c = txt[i]\n",
    "        if c in tokendict:\n",
    "            ret[i] = tokendict[c]\n",
    "        else:\n",
    "            ret[i] = 0\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def tokenize_batch(batch, tokendict, dim):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for b in batch:\n",
    "        ret.append(tokenize_txt(b, tokendict,dim))\n",
    "    return np.stack(ret)    \n",
    "    \n",
    "    \n",
    "def tensorize_batch(batch):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # shape should be ?? (batch_size, one_hot, sequence_width) <<- the first one and NOT: (seq_len, batch_size, input_size)\n",
    "    # so as an example could be: (1024, 128, 1) where there are 1024 for each of the 128 batches and each char is dim 1\n",
    "    # this is BEFORE the embedding layer\n",
    "    #TODO\n",
    "#     ret = torch.tensor(batch.transpose(0,1)).unsqueeze(1).long()\n",
    "#     ret = torch.tensor(batch.transpose(0,1)).unsqueeze(0).long()\n",
    "#     ret = torch.tensor(batch.transpose(0,1)).long()\n",
    "    ret = torch.tensor(batch).long()  # .transpose(1,2)\n",
    "    return ret\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_batches(txt, token_dict, char_len=1024, step=128, batch_size=128):\n",
    "    index = 0\n",
    "#     batches = []\n",
    "    while index >= 0:\n",
    "#         print(index)\n",
    "        batch, index = get_next_batch_txt(txt, index, char_len, step, batch_size)\n",
    "        tok = tokenize_batch(batch, token_dict, char_len)\n",
    "        tens = tensorize_batch(tok)\n",
    "#         batches.append(batch)  # yield batch\n",
    "        yield batch, tok, tens\n",
    "#     return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list(get_all_batches(rawdata[:100], txt2num_2seg, 20, 5, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gen = get_all_batches(rawdata[:100], txt2num_2seg, 1024, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_txt = rawdata\n",
    "index=0\n",
    "char_len=1024\n",
    "step=128\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtbatch = get_next_batch_txt(in_txt, index=0, char_len=1024, step=128, batch_size=128)\n",
    "# tok_batch = tokenize_batch(txtbatch, txt2num_2seg, char_len)\n",
    "# ten_batch = tensorize_batch(tok_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utf8codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = nn.Embedding(*(utf8codes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds.weight.data.copy_(torch.from_numpy(utf8codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embeds(torch.tensor([0,0,0,1], dtype=torch.long)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code2txt_2seg.pkl.pkl  txt2code_2seg.pkl.pkl  utf8_code_matrix_2seg.pkl.npy\n",
      "code2txt_3seg.pkl.pkl  txt2code_3seg.pkl.pkl  utf8_code_matrix_3seg.pkl.npy\n",
      "num2txt_2seg.pkl.pkl   txt2num_2seg.pkl.pkl\n",
      "num2txt_3seg.pkl.pkl   txt2num_3seg.pkl.pkl\n"
     ]
    }
   ],
   "source": [
    "ls ../utf8-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the UTF-8 codes \n",
    "utf8codes = np.load(\"../utf8-codes/utf8_code_matrix_2seg.pkl.npy\")\n",
    "txt2code_2seg = load_obj(\"../utf8-codes/txt2code_2seg.pkl\")\n",
    "code2txt_2seg = load_obj(\"../utf8-codes/code2txt_2seg.pkl\")\n",
    "txt2num_2seg = load_obj(\"../utf8-codes/txt2num_2seg.pkl\")\n",
    "num2txt_2seg = load_obj(\"../utf8-codes/num2txt_2seg.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2num_2seg['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1984, 324)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from column_models import FFColNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFColNet(utf8codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_global_norm(model, clip):\n",
    "    norms = []\n",
    "    total_norm = 0\n",
    "\n",
    "    for p in model.parameters():\n",
    "        norm = p.grad.data.norm()\n",
    "\n",
    "        if norm > clip:\n",
    "            p.grad.data.div_(max(norm, 1e-6) / clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, batch_gen):\n",
    "\n",
    "    for b,tok,x in batch_gen:\n",
    "#         x = x.unsqueeze(2).to(device)\n",
    "        x = x.long().to(device)\n",
    "        y = x\n",
    "#         print(x.shape, y.shape)\n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        ycode = model.embeds(y.long()[:,-128:])\n",
    "#         print(output.shape, ycode.shape, y.shape)\n",
    "        loss = criterion(output, ycode)\n",
    "        loss.backward()\n",
    "#         clip_global_norm(model, 0.25)\n",
    "        optimizer.step()\n",
    "        print('Loss: {:.5f}'.format(loss.item()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prime_factors(n):\n",
    "#     i = 2\n",
    "#     factors = []\n",
    "#     while i * i <= n:\n",
    "#         if n % i:\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             n //= i\n",
    "#             factors.append(i)\n",
    "#     if n > 1:\n",
    "#         factors.append(n)\n",
    "#     return factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prime_factors(324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload('column_models.MixedConvAttentionColumn')\n",
    "\n",
    "# model = FFColNet(utf8codes,1024).to(device)\n",
    "# model = NLPGatedConv1DColumnV1(utf8codes, 324).to(device)\n",
    "model = MixedConvAttentionColumn(utf8codes, c_in=[324, 64, 128, 128])  # .to(device)\n",
    "# model = ConvColumn(utf8codes, c_in=[324, 64, 128, 128]).to(device)\n",
    "# model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)  #The problem was the loss function... CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.1, lr_decay=1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = get_all_batches(rawdata, txt2num_2seg, 1024, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 conv1b torch.Size([256, 64, 1024]) torch.float32 True\n",
      "2 conv1b torch.Size([256, 128, 1024])\n",
      "3 conv1b torch.Size([256, 128, 1024])\n",
      "4 conv1b torch.Size([256, 128, 1024])\n",
      "1 conv1b torch.Size([256, 128, 512]) torch.float32 True\n",
      "2 conv1b torch.Size([256, 128, 512])\n",
      "3 conv1b torch.Size([256, 128, 512])\n",
      "4 conv1b torch.Size([256, 128, 512])\n",
      "1 conv1b torch.Size([256, 128, 256]) torch.float32 True\n",
      "2 conv1b torch.Size([256, 324, 256])\n",
      "3 conv1b torch.Size([256, 324, 256])\n",
      "4 conv1b torch.Size([256, 324, 256])\n",
      "Loss: 0.72243\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion, optimizer, batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b0 = list(batch_gen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (1, 256,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131072/1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_data, nll=False):\n",
    "#     pbar = tqdm.tqdm(zip(test_data['input'], test_data['label']))\n",
    "\n",
    "#     if nll:\n",
    "#         criterion = nn.NLLLoss(size_average=False)\n",
    "#     else:\n",
    "#         criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "#     nllloss = 0\n",
    "\n",
    "#     for X_batch, Y_batch in pbar:\n",
    "#         X_tensor = torch.from_numpy(X_batch).transpose(0,1).unsqueeze(0).cuda()\n",
    "#         Y_tensor = torch.from_numpy(Y_batch.astype(np.int)).transpose(0,1).unsqueeze(0).cuda()\n",
    "#         Y_tensor = Y_tensor.view(-1)\n",
    "#         output, hidden = model(X_tensor, Y_tensor, training=False)\n",
    "#         nllloss += criterion(output, Y_tensor).item()\n",
    "\n",
    "#     loss = nllloss / (len(test_data['input']) * 128 * 20)\n",
    "\n",
    "#     print('Perplexity:', np.exp(loss))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def run_epochs(data, test_data, model, criterion, optimizer, n_epochs=5, nll=False):\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         train(model, criterion, optimizer, data)\n",
    "#         test(model, test_data, nll)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     with open('/home/leo/projects/Datasets/text8.train.pkl', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     with open('/home/leo/projects/Datasets/text8.test.pkl', 'rb') as f:\n",
    "#         test_data = pickle.load(f)\n",
    "#     model = MixedConvAttentionColumn()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adagrad(model.parameters(), lr=0.1, lr_decay=1e-5, weight_decay=1e-5)\n",
    "#     n_epochs = 2\n",
    "#     nll = False\n",
    "#     run_epochs(data, test_data, model, criterion, optimizer, n_epochs, nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/leo/projects/Datasets/text8.train.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# with open('/home/leo/projects/Datasets/text8.test.pkl', 'rb') as f:\n",
    "#     test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
