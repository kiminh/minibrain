{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTF-8 Embedding and Decoder Test with Autoencoder\n",
    "\n",
    "This file is dedicated to test Overfitting Autoencoder to compress the size of the input embedding from the multi-hot to a dense vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langmodels.utf8codec as utf8codec\n",
    "from langmodels.utf8codec import *\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the codebook and all the dictionaries mapping the data\n",
    "code_matrix, txt2code, code2txt, txt2num, num2txt = utf8codec._load_codebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(num2txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.array(list(num2txt.keys()))\n",
    "all_data = all_data.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_overfit_batch(num2txt, batch_size):\n",
    "    \"\"\"\n",
    "    The idea is to prepare the list of all the numbers in batches, the batches are randomly mixed to avoid issues.\n",
    "    each batch contains:\n",
    "    (batch size, seq width, index)  ??\n",
    "    (batch size, index)  ??\n",
    "    :param num2txt: numeric index 2 string conversion dictionary containing the entire vocabulary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # assert type(num2txt) == 'dict'\n",
    "    all_data = np.array(list(num2txt.keys()))\n",
    "    all_data = all_data.reshape((-1,1))\n",
    "#     print(all_data.shape)\n",
    "    # assume that we can hold all in memory\n",
    "    arr = []\n",
    "    for i in range(batch_size):\n",
    "        data = np.copy(all_data)\n",
    "#         print(data.shape)\n",
    "        np.random.shuffle(data)\n",
    "#         print(data.shape)\n",
    "        arr.append(data.transpose())\n",
    "        \n",
    "    ret = np.stack(arr, axis=1)\n",
    "    ret = ret.reshape(batch_size,-1)\n",
    "#     print(ret.shape)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# btch = _prepare_overfit_batch(num2txt, 100)\n",
    "# btch = utf8codec._prepare_overfit_batch(num2txt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_overfit(model, optimizer, loss_function, batches, epoch, device, log_interval=10):\n",
    "#     model.train()\n",
    "    train_loss = 0\n",
    "    batch_loss = []\n",
    "    batch_idx = 0\n",
    "    for b in batches:\n",
    "        tensor_data = torch.from_numpy(b).to(device).long()  #.double()  #.float()\n",
    "        optimizer.zero_grad()\n",
    "        # emb is obtained from the the pre-computed utf8codebook\n",
    "        emb, res = model(tensor_data)\n",
    "        print(emb.shape,emb.dtype, res.shape, res.dtype)\n",
    "        loss = loss_function(emb, res)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data.item()  # [0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(tensor_data), len(batches),\n",
    "                100. * batch_idx / len(batches),\n",
    "                loss.data.item() / len(tensor_data)))\n",
    "            batch_loss.append(loss)\n",
    "        batch_idx += 1\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(batches)))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "def test(model, test_data, epoch, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for d in test_data:\n",
    "        tensor_data = torch.from_numpy(d).to(device)\n",
    "        res = model(data)\n",
    "        test_loss += loss_function(tensor_data, res).data.item()  # [0]\n",
    "\n",
    "    test_loss /= len(test_data)\n",
    "    print('epoch: {}====> Test set loss: {:.4f}'.format(epoch, test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return F.sigmoid\n",
    "    elif activation == \"tanh\":\n",
    "        return F.tanh\n",
    "    elif activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        return None\n",
    "        # raise RuntimeError(\"activation should be sigmoid/tanh/relu/gelu, not %s.\" % activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UTF8Autoencoder(code_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare many batches so I have everything ready to train\n",
    "nbatches = 2000\n",
    "batch_size = 64\n",
    "batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(nbatches):\n",
    "    btch = _prepare_overfit_batch(num2txt, batch_size)\n",
    "    batches.append(btch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = chunker(batches, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = F.mse_loss\n",
    "\n",
    "model = model.to(device)  #.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epoch_loss = []\n",
    "epoch_count = 1\n",
    "for e in epochs:\n",
    "    eloss = train_overfit(model, optimizer, loss_function, e, epoch_count, device, log_interval=10)\n",
    "    epoch_count+=1\n",
    "    epoch_loss.append(eloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"2segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
