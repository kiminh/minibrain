{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tests for Part Of Speech tagging\n",
    "\n",
    "This notebook is dedicated to start working with the PoS dataset already pre-processed and the column networks that I'm creating.\n",
    "\n",
    "The network will be constructed from small parts, each will be trained on top of the previous one, adding a new column and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n",
      "Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from langmodels.models import *\n",
    "import langmodels.utf8codec as utf8codec\n",
    "from langmodels.utils.tools import *\n",
    "from langmodels.utils.preprocess_conllu import UPOS, DEPREL\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the codebook and all the dictionaries mapping the data\n",
    "# utf8codes, txt2code, code2txt, txt2num, num2txt = utf8codec._load_codebook()\n",
    "utf8codes = np.load(\"./utf8-codes/utf8_codebook_overfit_matrix_2seg_dim64.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8codes = utf8codes.reshape(1987,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Conv1DPoS(utf8codes)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1949540"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the original network 11266152 parameters, I have cut the number of features and dimensions to make it smaller\n",
    "\n",
    "for nlayers = 5 of dim 5 is 6912424 and 6846888 trainable\n",
    "\n",
    "for the following Conv1DPartOfSpeech the number of parameters is: 2161960 where 2096424 are trainable\n",
    "\n",
    "    nchannels_in=[64, 128, 256, 512, 256],\n",
    "    nchannels_out=[128, 256, 512, 256, 96],\n",
    "    kernels=[3, 3, 3, 3, 3],\n",
    "    nlayers=[6, 6, 4, 4, 3],\n",
    "    groups=[1, 4, 8, 4, 1],\n",
    "    \n",
    "And LinearUposDeprelDecoder params are:\n",
    "\n",
    "    lin_in_dim=96, \n",
    "    lin_hidd_dim=768,\n",
    "    upos_dim=18, \n",
    "    deprel_dim=278,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1884004"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are the one that are heavy, so I'll just load them and check what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = \"/home/leo/projects/Datasets/text/UniversalDependencies/ud-treebanks-v2.4/traindev_np_batches_779000x3x1024_uint16.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(779000, 3, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint16')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_train_txt = data_train[:,0,:]\n",
    "dta_train_upos = data_train[:,1,:]\n",
    "dta_train_deprel = data_train[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(dta_train_txt[:50].astype(\"int64\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[113, 117, 111,  ...,   0,   0,   0],\n",
       "        [ 77, 111, 110,  ...,   0,   0,   0],\n",
       "        [ 65, 117, 223,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 68,  97, 103,  ...,   0,   0,   0],\n",
       "        [ 67, 111, 109,  ...,   0,   0,   0],\n",
       "        [113, 117, 105,  ...,   0,   0,   0]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "txtcode, positions, latent, dec = net(x)\n",
    "last_latent = latent[-1]\n",
    "upos, deprel = dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 64, 1024]),\n",
       " torch.Size([1, 1024]),\n",
       " torch.Size([50, 96, 1024]),\n",
       " torch.Size([50, 1024, 18]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtcode.shape, positions.shape, last_latent.shape, upos.shape, #  deprel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.cat([upos,deprel], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(18, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upos and deprel data are given by indices, this keeps memory as low as possible, but they need to be encoded\n",
    "upos_eye = torch.eye(len(UPOS))\n",
    "# deprel_eye = torch.eye(len(DEPREL))\n",
    "\n",
    "upos_emb = nn.Embedding(*upos_eye.shape)\n",
    "upos_emb.weight.data.copy_(upos_eye)\n",
    "upos_emb.to(device)\n",
    "\n",
    "# deprel_emb = nn.Embedding(*deprel_eye.shape)\n",
    "# deprel_emb.weight.data.copy_(deprel_eye)\n",
    "# deprel_emb.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(data, n, dim=0):\n",
    "    \"\"\"Yield successive n-sized chunks from data by the dimension dim\"\"\"\n",
    "    for i in range(0, data.shape[dim], n):\n",
    "        yield data_train[i:i + n,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(upos, deprel, target_upos, target_deprel):\n",
    "\n",
    "    # TODO check a more sofisticated loss function, for the moment only the sum to see if it runs\n",
    "    # the issue is that upos is easier than deprel (18 vs 278 classes)\n",
    "#     upos_loss = F.mse_loss(upos, target_upos)\n",
    "#     deprel_loss = F.mse_loss(deprel, target_deprel)\n",
    "    # issue with the size of target and tensors for cross_entropy ... I don't understand\n",
    "#     upos_loss = F.cross_entropy(upos, target_upos)\n",
    "#     deprel_loss = F.cross_entropy(deprel, target_deprel)\n",
    "#     print(upos.shape, target_upos.shape, deprel.shape, target_deprel.shape)\n",
    "#     upos_loss = F.nll_loss(upos, target_upos)\n",
    "#     deprel_loss = F.nll_loss(deprel, target_deprel)\n",
    "#     upos_loss = F.kl_div(upos, target_upos)\n",
    "#     deprel_loss = F.kl_div(deprel, target_deprel)\n",
    "#     loss = upos_loss + deprel_loss\n",
    "    loss = F.kl_div(torch.cat([upos, deprel], dim=-1).contiguous(), torch.cat([target_upos, target_deprel], dim=-1).contiguous())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, loss_function, batches, epoch, ndatapoints, device, log_interval=100):\n",
    "#     model.train()\n",
    "    train_loss = 0\n",
    "#     batch_loss = []\n",
    "    batch_idx = 0\n",
    "    for b_data in batches:\n",
    "        torch.cuda.empty_cache()  # make sure the cache is emptied to begin the nexxt batch\n",
    "        b_train = torch.from_numpy(b_data[:,0,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "        b_upos = torch.from_numpy(b_data[:,1,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "#         b_deprel = torch.from_numpy(b_data[:,2,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "#         tensor_data = torch.from_numpy(bdata).to(device).long()  #.double()  #.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        txtcode, positions, latent, dec = model(b_train)\n",
    "        last_latent = latent[-1]\n",
    "        upos, deprel = dec\n",
    "#         print(emb.shape,emb.dtype, res.shape, res.dtype)\n",
    "#         loss = loss_function(upos, deprel, upos_emb(b_upos), deprel_emb(b_deprel))\n",
    "        # Untill I make it work, work only with the UPOS PoS as it will be faster MUCH faster\n",
    "        loss = F.kl_div(upos, upos_emb(b_upos), reduction=\"batchmean\")\n",
    "#         loss = F.mse_loss(upos, upos_emb(b_upos))\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.data.item()  # [0]\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Timestamp {} Train Epoch: {} [{}/{} ]\\tLoss: {:.6f}'.format(\n",
    "                datetime.now(),\n",
    "                epoch, batch_idx , (ndatapoints//len(b_data)),\n",
    "                loss.data.item() / b_data.shape[0]))\n",
    "#             batch_loss.append(loss)\n",
    "        batch_idx += 1\n",
    "        del(b_train)\n",
    "        del(b_upos)\n",
    "#         del(b_deprel)\n",
    "    print('====> Timestamp {} Epoch: {} Average loss: {:.8f}'.format(datetime.now(), epoch, train_loss / ndatapoints))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, test_data, epoch, device):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     for d in test_data:\n",
    "#         tensor_data = torch.from_numpy(d).to(device)\n",
    "#         res = model(data)\n",
    "#         test_loss += loss_function(tensor_data, res).data.item()  # [0]\n",
    "\n",
    "#     test_loss /= len(test_data)\n",
    "#     print('epoch: {}====> Test set loss: {:.4f}'.format(epoch, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=0, amsgrad=False )\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((779000, 3, 1024), 15580)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_train.shape[0]//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "data = data_train[-1000*batch_size:,:,:]  # just for the trials, use the last 1000 batches only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = chunks(data, batch_size, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp 2019-11-26 23:43:53.870421 Train Epoch: 2 [0/1000 ]\tLoss: -18.555200\n",
      "Timestamp 2019-11-26 23:44:16.984003 Train Epoch: 2 [100/1000 ]\tLoss: -18.852000\n",
      "Timestamp 2019-11-26 23:44:40.097733 Train Epoch: 2 [200/1000 ]\tLoss: -18.846400\n",
      "Timestamp 2019-11-26 23:45:03.317705 Train Epoch: 2 [300/1000 ]\tLoss: -18.587999\n",
      "Timestamp 2019-11-26 23:45:26.615978 Train Epoch: 2 [400/1000 ]\tLoss: -18.891200\n",
      "Timestamp 2019-11-26 23:45:49.917358 Train Epoch: 2 [500/1000 ]\tLoss: -18.941599\n",
      "Timestamp 2019-11-26 23:46:13.255728 Train Epoch: 2 [600/1000 ]\tLoss: -18.807600\n",
      "Timestamp 2019-11-26 23:46:36.497815 Train Epoch: 2 [700/1000 ]\tLoss: -18.763600\n",
      "Timestamp 2019-11-26 23:46:59.805018 Train Epoch: 2 [800/1000 ]\tLoss: -18.970399\n",
      "Timestamp 2019-11-26 23:47:23.135896 Train Epoch: 2 [900/1000 ]\tLoss: -18.632000\n",
      "====> Timestamp 2019-11-26 23:47:46.196081 Epoch: 2 Average loss: -18.75240836\n",
      "CPU times: user 2min 34s, sys: 1min 17s, total: 3min 51s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch_count = 2\n",
    "# epochs = range(1)\n",
    "# for e in epochs:\n",
    "eloss = train(model, optimizer, loss_function, batches, epoch_count, len(data), device, log_interval=100)\n",
    "#     epoch_count+=1\n",
    "#     if epoch_count == 20:\n",
    "#         print(\"epoch {} decreasing learning_rate to {}\".format(epoch_count, 1e-5))\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "#     epoch_loss.append(eloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network.save_model(\"./trained_models/conv1dcol\", \"conv1dcol_kl-div+1000batches-mse-loss_epoch-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried different Nx50 sizes for batches but the only one that works is 50, it seems will be the maximum number of samples in each batch for the training in my GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that training does not seem to work correctly.\n",
    "\n",
    "All training losses (kl_div, mse_loss) seem to learn well only the first 100 batches and then nothing, it oscilates. After several different initializations with kl_div it worked better (the first loss was about initialized to -1 ... ) so initialization seems to take an important role here.\n",
    "\n",
    "I need to write a test function now to be able to measure with the test datasets and see the real accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2153514496"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4483710976"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2153514496"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304770048"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
