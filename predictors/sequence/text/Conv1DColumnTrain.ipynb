{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tests for Part Of Speech tagging\n",
    "\n",
    "This notebook is dedicated to start working with the PoS dataset already pre-processed and the column networks that I'm creating.\n",
    "\n",
    "The network will be constructed from small parts, each will be trained on top of the previous one, adding a new column and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leo/venv3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Loading faiss with AVX2 support.\n",
      "Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from langmodels.models import *\n",
    "import langmodels.utf8codec as utf8codec\n",
    "from langmodels.utils.tools import *\n",
    "from langmodels.utils.preprocess_conllu import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the codebook and all the dictionaries mapping the data\n",
    "# utf8codes, txt2code, code2txt, txt2num, num2txt = utf8codec._load_codebook()\n",
    "utf8codes = np.load(\"./utf8-codes/utf8_codebook_overfit_matrix_2seg_dim64.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8codes = utf8codes.reshape(1987,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv1DPoS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5007c43617f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1DPoS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutf8codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Conv1DPoS' is not defined"
     ]
    }
   ],
   "source": [
    "net = Conv1DPoS(utf8codes)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the original network 11266152 parameters, I have cut the number of features and dimensions to make it smaller\n",
    "\n",
    "for nlayers = 5 of dim 5 is 6912424 and 6846888 trainable\n",
    "\n",
    "for the following Conv1DPartOfSpeech the number of parameters is: 2161960 where 2096424 are trainable\n",
    "\n",
    "    nchannels_in=[64, 128, 256, 512, 256],\n",
    "    nchannels_out=[128, 256, 512, 256, 96],\n",
    "    kernels=[3, 3, 3, 3, 3],\n",
    "    nlayers=[6, 6, 4, 4, 3],\n",
    "    groups=[1, 4, 8, 4, 1],\n",
    "    \n",
    "And LinearUposDeprelDecoder params are:\n",
    "\n",
    "    lin_in_dim=96, \n",
    "    lin_hidd_dim=768,\n",
    "    upos_dim=18, \n",
    "    deprel_dim=278,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trainable_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are the one that are heavy, so I'll just load them and check what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = \"/home/leo/projects/Datasets/text/UniversalDependencies/ud-treebanks-v2.4/traindev_np_batches_779000x3x1024_uint16.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_train_txt = data_train[:,0,:]\n",
    "dta_train_upos = data_train[:,1,:]\n",
    "dta_train_deprel = data_train[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.from_numpy(dta_train_txt[:50].astype(\"int64\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtcode, positions, latent, dec = net(x)\n",
    "# last_latent = latent[-1]\n",
    "# upos, deprel = dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtcode.shape, positions.shape, last_latent.shape, upos.shape, #  deprel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.cat([upos,deprel], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upos and deprel data are given by indices, this keeps memory as low as possible, but they need to be encoded\n",
    "upos_eye = torch.eye(len(UPOS))\n",
    "deprel_eye = torch.eye(len(DEPREL))\n",
    "with torch.no_grad():\n",
    "    upos_emb = nn.Embedding(*upos_eye.shape)\n",
    "    upos_emb.weight.data.copy_(upos_eye)\n",
    "    upos_emb = upos_emb.to(device)\n",
    "\n",
    "    deprel_emb = nn.Embedding(*deprel_eye.shape)\n",
    "    deprel_emb.weight.data.copy_(deprel_eye)\n",
    "    deprel_emb.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(data, n, dim=0):\n",
    "    \"\"\"Yield successive n-sized chunks from data by the dimension dim\"\"\"\n",
    "    for i in range(0, data.shape[dim], n):\n",
    "        yield data[i:i + n,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(upos, deprel, target_upos, target_deprel):\n",
    "\n",
    "    # TODO check a more sofisticated loss function, for the moment only the sum to see if it runs\n",
    "    # the issue is that upos is easier than deprel (18 vs 278 classes)\n",
    "#     upos_loss = F.mse_loss(upos, target_upos)\n",
    "#     deprel_loss = F.mse_loss(deprel, target_deprel)\n",
    "    # issue with the size of target and tensors for cross_entropy ... I don't understand\n",
    "#     upos_loss = F.cross_entropy(upos, target_upos)\n",
    "#     deprel_loss = F.cross_entropy(deprel, target_deprel)\n",
    "#     print(upos.shape, target_upos.shape, deprel.shape, target_deprel.shape)\n",
    "    upos_loss = F.nll_loss(upos, target_upos)\n",
    "    deprel_loss = F.nll_loss(deprel, target_deprel)\n",
    "#     upos_loss = F.kl_div(upos, target_upos)\n",
    "#     deprel_loss = F.kl_div(deprel, target_deprel)\n",
    "    loss = upos_loss + deprel_loss\n",
    "#     loss = F.kl_div(torch.cat([upos, deprel], dim=-1).contiguous(), torch.cat([target_upos, target_deprel], dim=-1).contiguous())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata = torch.from_numpy(data_train[-2:,0,:].astype(\"int64\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# testing tensorboard add_graph to see if the network graph is drawn correctly ;)\n",
    "# indata = torch.from_numpy(data_train[-2:,0,:].astype(\"int64\")).to(device)\n",
    "# writer.add_graph(net, indata)\n",
    "# Kernel dies when I do this ... so ... :O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_function, batches, epoch, ndatapoints, device, log_interval=100):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "#     batch_loss = []\n",
    "    batch_idx = 1\n",
    "    for b_data in batches:\n",
    "        torch.cuda.empty_cache()  # make sure the cache is emptied to begin the nexxt batch\n",
    "        b_train = torch.from_numpy(b_data[:,0,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "        b_upos = torch.from_numpy(b_data[:,1,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "#         b_deprel = torch.from_numpy(b_data[:,2,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "#         tensor_data = torch.from_numpy(bdata).to(device).long()  #.double()  #.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        txtcode, positions, latent, dec = model(b_train)\n",
    "        last_latent = latent[-1]\n",
    "        upos, deprel = dec\n",
    "#         print(emb.shape,emb.dtype, res.shape, res.dtype)\n",
    "#         print(upos.shape, b_upos.shape)\n",
    "#         loss = loss_function(upos, deprel, upos_emb(b_upos), deprel_emb(b_deprel))\n",
    "#         loss = loss_function(upos, deprel, b_upos, b_deprel)\n",
    "        # Untill I make it work, work only with the UPOS PoS as it will be faster MUCH faster\n",
    "#         loss = F.kl_div(upos, upos_emb(b_upos), reduction=\"batchmean\")\n",
    "        loss = F.nll_loss(upos.view([-1,18]),b_upos.view([-1]))\n",
    "#         loss = F.cross_entropy(upos.view([-1,18]),b_upos.view([-1]))\n",
    "#         loss = F.cross_entropy(upos,b_upos)\n",
    "#         loss = F.mse_loss(upos, upos_emb(b_upos))\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.data.item()  # [0]\n",
    "        writer.add_scalar(\"Loss/train\", loss.data.item(), global_step=epoch*batch_idx)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Timestamp {} Train Epoch: {} [{}/{} ]\\tLoss: {:.6f}'.format(\n",
    "                datetime.now(),\n",
    "                epoch, batch_idx , (ndatapoints//len(b_data)),\n",
    "                loss.data.item() / b_data.shape[0]))\n",
    "#             batch_loss.append(loss)\n",
    "        batch_idx += 1\n",
    "        del(b_train)\n",
    "        del(b_upos)\n",
    "#         del(b_deprel)\n",
    "        torch.cuda.empty_cache()\n",
    "    writer.add_scalar(\"EpochLoss/train\", train_loss / batch_idx, epoch)\n",
    "    print('====> Timestamp {} Epoch: {} Average loss: {:.8f}'.format(datetime.now(), epoch, train_loss / ndatapoints))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data ALL the training data\n",
    "base_dir = \"/home/leo/projects/Datasets/text/UniversalDependencies/ud-treebanks-v2.5\"\n",
    "# get all file paths for testing\n",
    "all_fnames = get_all_files_recurse(base_dir)\n",
    "fnames = [f for f in all_fnames if \"test-charse\" in f and f.endswith(\".npy\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all test files \n",
    "test_data = []\n",
    "for f in fnames:\n",
    "    data = np.load(f)\n",
    "    lang_name = path_leaf(f).split(\"-ud\")[0]\n",
    "    test_data.append((lang_name, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_function, test_data, epoch, device, max_data=100):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for lang, d in test_data:\n",
    "        torch.cuda.empty_cache()  # make sure the cache is emptied to begin the nexxt batch\n",
    "        b_test = torch.from_numpy(d[:max_data,0,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "        b_upos = torch.from_numpy(d[:max_data,1,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "#         b_deprel = torch.from_numpy(d[:,2,:].astype(\"int64\")).squeeze().to(device).long()\n",
    "        _, _, _, dec = model(b_test)\n",
    "#         last_latent = latent[-1]\n",
    "        upos, _ = dec\n",
    "        loss = loss_function(upos.view([-1,18]),b_upos.view([-1]))\n",
    "#         loss =  loss_function(res, tensor_data).data.item()  # [0]\n",
    "        test_loss += loss.data.item()\n",
    "        writer.add_scalar(\"LangLoss/test/\"+lang, loss.data.item(), global_step=epoch)\n",
    "        del(b_test)\n",
    "        del(b_upos)\n",
    "        torch.cuda.empty_cache()\n",
    "    test_loss /= len(test_data)  # although this is not faire as different languages give different results\n",
    "    writer.add_scalar(\"EpochLangLoss/test/\", test_loss, global_step=epoch)\n",
    "    print('epoch: {}====> Test set loss: {:.8f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model from saved state:\n",
    "# net.network.load_model(\"./trained_models/conv1dcol\", \"conv1dcol_kl-div+1000batches-mse-loss_epoch-3_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=0, amsgrad=False )\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape, data_train.shape[0]//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 10000\n",
    "batch_size = 50\n",
    "# data = data_train[-1000*batch_size:,:,:]  # just for the trials, use the last 1000 batches only\n",
    "data = data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = chunks(data, epoch_size, dim=0)\n",
    "# batches = chunks(data, batch_size, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# epoch_count = 0\n",
    "# test(model, loss_function, test_data, epoch_count, device, max_data=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epoch_count = 1\n",
    "for e in epochs:\n",
    "    batches = chunks(e, batch_size, dim=0)\n",
    "    eloss = train(model, optimizer, loss_function, batches, epoch_count, epoch_size, device, log_interval=10)\n",
    "    test(model, loss_function, test_data, epoch_count, device, max_data=50)\n",
    "    epoch_count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# epoch_count = 2\n",
    "# eloss = train(model, optimizer, loss_function, batches, epoch_count, len(data), device, log_interval=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network.save_model(\"./trained_models/conv1dcol\", \"conv1dcol_nll-loss_pretrained_epoch-{}\".format(epoch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried different Nx50 sizes for batches but the only one that works is 50, it seems will be the maximum number of samples in each batch for the training in my GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that training does not seem to work correctly.\n",
    "\n",
    "All training losses (kl_div, mse_loss) seem to learn well only the first 100 batches and then nothing, it oscilates. After several different initializations with kl_div it worked better (the first loss was about initialized to -1 ... ) so initialization seems to take an important role here.\n",
    "\n",
    "I need to write a test function now to be able to measure with the test datasets and see the real accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
